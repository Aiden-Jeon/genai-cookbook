{"request_id":"id-1431","request":"\"How can I resolve the 'INVALID_SECRET_LOOKUP' error with a 'SECRET_FUNCTION_KEY_NOT_CONSTANT' cause in Databricks?\"","response":"The `INVALID_SECRET_LOOKUP` error class in Databricks is associated with issues related to secret lookup. The `SECRET_FUNCTION_KEY_NOT_CONSTANT` cause indicates that the key provided in the `secret` function is not a constant string. To resolve this issue, ensure that the key is a constant string expression passed in the second argument of the `secret` function. For more information about using secrets in Databricks, you can refer to the documentation on [secrets management](https:\/\/docs.databricks.com\/security\/secrets\/index.html).","expected_retrieved_context":[{"doc_uri":"https:\/\/docs.databricks.com\/error-messages\/invalid-secret-lookup-error-class.html","content":"INVALID_SECRET_LOOKUP error class  \nSQLSTATE: 22531  \nInvalid secret lookup:  \nLIST_SECRETS_FAILED\nLIST_SECRETS_FAILED\nThe LIST_SECRETS table function failed to return the list of available secrets <cause>\n\nSECRET_FUNCTION_KEY_NOT_CONSTANT\nSECRET_FUNCTION_KEY_NOT_CONSTANT\nThe SECRET function requires the secret key as a constant string expression passed in the second argument\n\nSECRET_FUNCTION_LOOKUP_FAILED\nSECRET_FUNCTION_LOOKUP_FAILED\nThe SECRET function failed to find a secret with scope <scope> and key <key> <cause>\n\nSECRET_FUNCTION_NOT_SUPPORTED\nSECRET_FUNCTION_NOT_SUPPORTED\nThe SECRET and LIST_SECRETS functions are not supported.\n\nSECRET_FUNCTION_SCOPE_NOT_CONSTANT\nSECRET_FUNCTION_SCOPE_NOT_CONSTANT\nThe SECRET function requires the secret scope as a constant string expression passed in the first argument."}],"expected_response":"To resolve the 'INVALID_SECRET_LOOKUP' error with a 'SECRET_FUNCTION_KEY_NOT_CONSTANT' cause in Databricks, you need to ensure that the secret key is passed as a constant string expression in the second argument of the SECRET function. This error occurs when the secret key is not a constant value.\n\nHere's an example of how to correctly use the SECRET function with a constant secret key:\n\n```python\nsecret_value = spark.sql(\"SELECT SECRET('my-scope', 'my-key') AS secret_value\")\n```\n\nIn this example, 'my-scope' and 'my-key' are constant string expressions. Make sure to replace them with the actual scope and key of your secret.\n\nIf you're still encountering the error after ensuring that the secret key is a constant value, there might be an issue with the secret scope or key itself. You can check the secret's availability by using the LIST_SECRETS table function:\n\n```python\nsecrets = spark.sql(\"SELECT * FROM LIST_SECRETS()\")\nsecrets.show()\n```\n\nThis will list all available secrets in your workspace. Verify that the scope and key of the secret you're trying to access are listed. If not, you may need to create or update the secret in your Databricks workspace.","retrieved_context":[{"doc_uri":"https:\/\/docs.databricks.com\/error-messages\/invalid-secret-lookup-error-class.html","content":"Updated May 28, 2024\n \n\n[Send us feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n\n* [Documentation](..\/index.html)\n* [Databricks reference documentation](..\/reference\/api.html)\n* [Error handling in Databricks](index.html)\n* [Error classes in Databricks](error-classes.html)\n* INVALID\\_SECRET\\_LOOKUP error class\n* \n\n# INVALID\\_SECRET\\_LOOKUP error class\n\n[SQLSTATE: 22531](sqlstates.html#class-22-data-exception)\n\nInvalid secret lookup:\n\n## LIST\\_SECRETS\\_FAILED\n\nThe LIST\\_SECRETS table function failed to return the list of available secrets `<cause>`\n\n## SECRET\\_FUNCTION\\_KEY\\_NOT\\_CONSTANT\n\nThe SECRET function requires the secret key as a constant string expression passed in the second argument\n\n## SECRET\\_FUNCTION\\_LOOKUP\\_FAILED\n\nThe SECRET function failed to find a secret with scope `<scope>` and key `<key>` `<cause>`\n\n## SECRET\\_FUNCTION\\_NOT\\_SUPPORTED\n\nThe SECRET and LIST\\_SECRETS functions are not supported.\n\n## SECRET\\_FUNCTION\\_SCOPE\\_NOT\\_CONSTANT\n\nThe SECRET function requires the secret scope as a constant string expression passed in the first argument.\n\n---\n\n \u00a9 Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the [Apache Software Foundation](http:\/\/www.apache.org\/).\n \n\n[Send us feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n \n | [Privacy Policy](https:\/\/databricks.com\/privacy-policy) | [Terms of Use](https:\/\/databricks.com\/terms-of-use)\n\n { # 's' variable stands for the search page here. TODO: Fix it. #}\n \n [![DATA+AU Summit 2024](..\/_static\/images\/dais\/docs-dais-promo-image.png)](https:\/\/www.databricks.com\/dataaisummit?utm_source=databricks&utm_medium=web&utm_campaign=7018y000001f7cvqai&utm_offer=docs \"DATA+AU Summit 2024\")"},{"doc_uri":"https:\/\/docs.databricks.com\/sql\/language-manual\/functions\/secret.html","content":"Updated May 28, 2024\n \n\n[Send us feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n\n* [Documentation](..\/..\/..\/index.html)\n* [Develop on Databricks](..\/..\/..\/languages\/index.html)\n* [SQL language reference](..\/index.html)\n* [Functions](..\/sql-ref-functions.html)\n* [Built-in functions](..\/sql-ref-functions-builtin.html)\n* [Alphabetical list of built-in functions](..\/sql-ref-functions-builtin-alpha.html)\n* `secret` function\n* \n\n# `secret` function\n\n**Applies to:** ![check marked yes](..\/..\/..\/_images\/check.png) Databricks SQL preview ![check marked yes](..\/..\/..\/_images\/check.png) Databricks Runtime 11.3 LTS and above\n\nExtracts a secret value with the given `scope` and `key` from [Databricks secret service](..\/..\/..\/security\/secrets\/index.html).\n\n## Syntax\n\n```\nsecret ( scope, key )\n\n```\n\n## Arguments\n\n* `scope`: A constant string expression containing the scope of the secret to be extracted. The scope is the namespace in which multiple keys might reside.\n* `key`: A constant string expression with the key of the secret to be extracted.\n\n## Returns\n\nA `STRING`.\n\nTo retrieve a list of all secrets the user is authorized to see use the [list\\_secrets](list_secrets.html) function.\n\n## Error conditions\n\n* `INVALID_SECRET_LOOKUP.SECRET_FUNCTION_KEY_NOT_CONSTANT`: The `key` is not a constant string.\n* `INVALID_SECRET_LOOKUP.SECRET_FUNCTION_SCOPE_NOT_CONSTANT`: The `scope` is not a constant string.\n* `INVALID_SECRET_LOOKUP.SECRET_FUNCTION_NOT_SUPPORTED`: Databricks secret service is not available.\n* `INVALID_SECRET_LOOKUP.SECRET_FUNCTION_LOOKUP_FAILED`: The `key` cannot be found within `scope`.\n\n## Examples\n\n```\n> SELECT * FROM list_secrets();\n  scope         key\n  ------------  ---------------\n  secrets.r.us  theAnswerToLife\n\n> SELECT secret('secrets.r.us', 'theAnswerToLife');\n  fourtyTwo\n\n```\n\n## Related functions\n\n* [list\\_secrets table function](list_secrets.html)\n\n---\n\n \u00a9 Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the [Apache Software Foundation](http:\/\/www.apache.org\/).\n \n\n[Send us feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n \n | [Privacy Policy](https:\/\/databricks.com\/privacy-policy) | [Terms of Use](https:\/\/databricks.com\/terms-of-use)\n\n { # 's' variable stands for the search page here. TODO: Fix it. #}\n \n [![DATA+AU Summit 2024](..\/..\/..\/_static\/images\/dais\/docs-dais-promo-image.png)](https:\/\/www.databricks.com\/dataaisummit?utm_source=databricks&utm_medium=web&utm_campaign=7018y000001f7cvqai&utm_offer=docs \"DATA+AU Summit 2024\")"},{"doc_uri":"https:\/\/docs.databricks.com\/error-messages\/invalid-secret-lookup-error-class.html","content":"INVALID\\_SECRET\\_LOOKUP error class | Databricks on AWS\n\n* [![Databricks](..\/_static\/small-scale-lockup-full-color-rgb.svg)](https:\/\/www.databricks.com\/)\n\n* [![Databricks](..\/_static\/small-scale-lockup-full-color-rgb.svg)](https:\/\/www.databricks.com\/)\n* [Help Center](https:\/\/help.databricks.com\/s\/)\n* [Documentation](https:\/\/docs.databricks.com\/en\/index.html)\n* [Knowledge Base](https:\/\/kb.databricks.com\/)\n\n* [Community](https:\/\/community.databricks.com)\n* [Support](https:\/\/help.databricks.com)\n* [Feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n* [Try Databricks](https:\/\/databricks.com\/try-databricks)\n\n[![](..\/_static\/icons\/globe.png)English](javascript:void(0))\n* [English](..\/..\/en\/error-messages\/invalid-secret-lookup-error-class.html)\n* [\u65e5\u672c\u8a9e](..\/..\/ja\/error-messages\/invalid-secret-lookup-error-class.html)\n* [Portugu\u00eas](..\/..\/pt\/error-messages\/invalid-secret-lookup-error-class.html)\n\n[![](..\/_static\/icons\/aws.svg)Amazon\n Web Services](javascript:void(0))\n* [![](..\/_static\/icons\/azure.svg)Microsoft Azure](https:\/\/learn.microsoft.com\/azure\/databricks\/error-messages\/invalid-secret-lookup-error-class)\n* [![](..\/_static\/icons\/gcp.svg)Google Cloud Platform](https:\/\/docs.gcp.databricks.com\/error-messages\/invalid-secret-lookup-error-class.html)\n\n[Databricks on AWS](..\/index.html)\nGet started\n\n* [Get started](..\/getting-started\/index.html)\n* [What is Databricks?](..\/introduction\/index.html)\n* [DatabricksIQ](..\/databricksiq\/index.html)\n* [Release notes](..\/release-notes\/index.html)\n\nLoad & manage data\n\n* [Connect to data sources](..\/connect\/index.html)\n* [Connect to compute](..\/compute\/index.html)\n* [Discover data](..\/discover\/index.html)\n* [Query data](..\/query\/index.html)\n* [Ingest data](..\/ingestion\/index.html)\n* [Transform data](..\/transform\/index.html)\n* [Monitor data and AI assets](..\/lakehouse-monitoring\/index.html)\n* [Share data (Delta Sharing)](..\/data-sharing\/index.html)\n* [Databricks Marketplace](..\/marketplace\/index.html)\n\nWork with data\n\n* [Data engineering](..\/workspace-index.html)\n* [Generative AI & LLMs](..\/generative-ai\/generative-ai.html)\n* [Machine learning](..\/machine-learning\/index.html)\n* [Model serving](..\/machine-learning\/model-serving\/index.html)\n* [Data warehousing](..\/sql\/index.html)\n* [Delta Lake](..\/delta\/index.html)\n* [Developers](..\/languages\/index.html)\n* [Technology partners](..\/integrations\/index.html)\n\nAdministration\n\n* [Account and workspace administration](..\/admin\/index.html)\n* [Security and compliance](..\/security\/index.html)\n* [Data governance (Unity Catalog)](..\/data-governance\/index.html)\n* [Lakehouse architecture](..\/lakehouse-architecture\/index.html)\n\nReference & resources"},{"doc_uri":"https:\/\/docs.databricks.com\/sql\/language-manual\/functions\/list_secrets.html","content":"Updated May 28, 2024\n \n\n[Send us feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n\n* [Documentation](..\/..\/..\/index.html)\n* [Develop on Databricks](..\/..\/..\/languages\/index.html)\n* [SQL language reference](..\/index.html)\n* [Functions](..\/sql-ref-functions.html)\n* [Built-in functions](..\/sql-ref-functions-builtin.html)\n* [Alphabetical list of built-in functions](..\/sql-ref-functions-builtin-alpha.html)\n* `list_secrets` table function\n* \n\n# `list_secrets` table function\n\n**Applies to:** ![check marked yes](..\/..\/..\/_images\/check.png) Databricks SQL preview ![check marked yes](..\/..\/..\/_images\/check.png) Databricks Runtime 11.3 LTS and above\n\nReturns the keys which the user is authorized to see from [Databricks secret service](..\/..\/..\/security\/secrets\/index.html).\n\n## Syntax\n\n```\nlist_secrets()\n\n```\n\n## Arguments\n\nThis function takes no arguments.\n\n## Returns\n\nA table with two columns:\n\n* `scope STRING NOT NULL`: The namespace of the key.\n* `key STYRING NOT NULL`: The key itself\n\nTo return the secret values for specific keys use [`secret(scope, key)`](secret.html).\n\n## Error conditions\n\n* `INVALID_SECRET_LOOKUP.LIST_SECRETS_FAILED`: Databricks secret service is not available.\n\n## Examples\n\n```\n> SELECT * FROM list_secrets();\n  scope         key\n  ------------  ---------------\n  secrets.r.us  theAnswerToLife\n\n> SELECT secret('secrets.r.us', 'theAnswerToLife');\n  fourtyTwo\n\n```\n\n## Related functions\n\n* [list\\_secrets table function](#)\n\n---\n\n \u00a9 Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the [Apache Software Foundation](http:\/\/www.apache.org\/).\n \n\n[Send us feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n \n | [Privacy Policy](https:\/\/databricks.com\/privacy-policy) | [Terms of Use](https:\/\/databricks.com\/terms-of-use)\n\n { # 's' variable stands for the search page here. TODO: Fix it. #}\n \n [![DATA+AU Summit 2024](..\/..\/..\/_static\/images\/dais\/docs-dais-promo-image.png)](https:\/\/www.databricks.com\/dataaisummit?utm_source=databricks&utm_medium=web&utm_campaign=7018y000001f7cvqai&utm_offer=docs \"DATA+AU Summit 2024\")"},{"doc_uri":"https:\/\/docs.databricks.com\/security\/secrets\/secrets.html","content":"Secrets | Databricks on AWS\n\n* [![Databricks](..\/..\/_static\/small-scale-lockup-full-color-rgb.svg)](https:\/\/www.databricks.com\/)\n\n* [![Databricks](..\/..\/_static\/small-scale-lockup-full-color-rgb.svg)](https:\/\/www.databricks.com\/)\n* [Help Center](https:\/\/help.databricks.com\/s\/)\n* [Documentation](https:\/\/docs.databricks.com\/en\/index.html)\n* [Knowledge Base](https:\/\/kb.databricks.com\/)\n\n* [Community](https:\/\/community.databricks.com)\n* [Support](https:\/\/help.databricks.com)\n* [Feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n* [Try Databricks](https:\/\/databricks.com\/try-databricks)\n\n[![](..\/..\/_static\/icons\/globe.png)English](javascript:void(0))\n* [English](..\/..\/..\/en\/security\/secrets\/secrets.html)\n* [\u65e5\u672c\u8a9e](..\/..\/..\/ja\/security\/secrets\/secrets.html)\n* [Portugu\u00eas](..\/..\/..\/pt\/security\/secrets\/secrets.html)\n\n[![](..\/..\/_static\/icons\/aws.svg)Amazon\n Web Services](javascript:void(0))\n* [![](..\/..\/_static\/icons\/azure.svg)Microsoft Azure](https:\/\/learn.microsoft.com\/azure\/databricks\/security\/secrets\/secrets)\n* [![](..\/..\/_static\/icons\/gcp.svg)Google Cloud Platform](https:\/\/docs.gcp.databricks.com\/security\/secrets\/secrets.html)\n\n[Databricks on AWS](..\/..\/index.html)\nGet started\n\n* [Get started](..\/..\/getting-started\/index.html)\n* [What is Databricks?](..\/..\/introduction\/index.html)\n* [DatabricksIQ](..\/..\/databricksiq\/index.html)\n* [Release notes](..\/..\/release-notes\/index.html)\n\nLoad & manage data\n\n* [Connect to data sources](..\/..\/connect\/index.html)\n* [Connect to compute](..\/..\/compute\/index.html)\n* [Discover data](..\/..\/discover\/index.html)\n* [Query data](..\/..\/query\/index.html)\n* [Ingest data](..\/..\/ingestion\/index.html)\n* [Transform data](..\/..\/transform\/index.html)\n* [Monitor data and AI assets](..\/..\/lakehouse-monitoring\/index.html)\n* [Share data (Delta Sharing)](..\/..\/data-sharing\/index.html)\n* [Databricks Marketplace](..\/..\/marketplace\/index.html)\n\nWork with data\n\n* [Data engineering](..\/..\/workspace-index.html)\n* [Generative AI & LLMs](..\/..\/generative-ai\/generative-ai.html)\n* [Machine learning](..\/..\/machine-learning\/index.html)\n* [Model serving](..\/..\/machine-learning\/model-serving\/index.html)\n* [Data warehousing](..\/..\/sql\/index.html)\n* [Delta Lake](..\/..\/delta\/index.html)\n* [Developers](..\/..\/languages\/index.html)\n* [Technology partners](..\/..\/integrations\/index.html)\n\nAdministration\n\n* [Account and workspace administration](..\/..\/admin\/index.html)\n* [Security and compliance](..\/index.html)\n\t+ [Authentication and access control](..\/auth-authz\/index.html)\n\t+ [Networking](..\/network\/index.html)\n\t+ [Data security and encryption](..\/keys\/index.html)\n\t+ [Secret management](index.html)\n\t\t- [Secret scopes](secret-scopes.html)\n\t\t- [Secrets](#)\n\t\t- [Secret redaction](redaction.html)\n\t\t- [Secret workflow example](example-secret-workflow.html)\n\t+ [Auditing, privacy, and compliance](..\/privacy\/index.html)\n* [Data governance (Unity Catalog)](..\/..\/data-governance\/index.html)\n* [Lakehouse architecture](..\/..\/lakehouse-architecture\/index.html)\n\nReference & resources\n\n* [Reference](..\/..\/reference\/api.html)\n* [Resources](..\/..\/resources\/index.html)\n* [What\u2019s coming?](..\/..\/whats-coming.html)\n* [Documentation archive](..\/..\/archive\/index.html)\n\n Updated May 28, 2024\n \n\n[Send us feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n\n* [Documentation](..\/..\/index.html)\n* [Security and compliance guide](..\/index.html)\n* [Secret management](index.html)\n* Secrets\n* \n\n# Secrets\n\nA secret is a key-value pair that stores secret material, with a key name unique within a [secret scope](secret-scopes.html). Each scope is limited to 1000 secrets. The maximum allowed secret value size is 128 KB.\n\nSee also the [Secrets API](https:\/\/docs.databricks.com\/api\/workspace\/secrets).\n\n## Create a secret\n\nSecret names are case insensitive.\n\n### Create a secret in a Databricks-backed scope\n\nTo create a secret in a Databricks-backed scope using the [Databricks CLI](..\/..\/dev-tools\/cli\/index.html) (version 0.205 and above):\n\n```\ndatabricks secrets put-secret --json '{\n  \"scope\": \"<scope-name>\",\n  \"key\": \"<key-name>\",\n  \"string_value\": \"<secret>\"\n}'\n\n```\n\nIf you are creating a multi-line secret, you can pass the secret using standard input. For example:\n\n```\n(cat << EOF\nthis\nis\na\nmulti\nline\nsecret\nEOF\n) | databricks secrets put-secret <secret_scope> <secret_key>\n\n```\n\nYou can also provide a secret from a file. For more information about writing secrets, see [What is the Databricks CLI?](..\/..\/dev-tools\/cli\/index.html).\n\n## List secrets\n\nTo list secrets in a given scope:\n\n```\ndatabricks secrets list-secrets <scope-name>\n\n```\n\nThe response displays metadata information about the secrets, such as the secrets\u2019 key names. You use the [Secrets utility (dbutils.secrets)](..\/..\/dev-tools\/databricks-utils.html#dbutils-secrets) in a notebook or job to list this metadata. For example:\n\n```\ndbutils.secrets.list('my-scope')\n\n```\n\n## Read a secret\n\nYou create secrets using the REST API or CLI, but you must use the [Secrets utility (dbutils.secrets)](..\/..\/dev-tools\/databricks-utils.html#dbutils-secrets) in a notebook or job to read a secret.\n\n## Delete a secret\n\nTo delete a secret from a scope with the Databricks CLI:\n\n```\ndatabricks secrets delete-secret <scope-name> <key-name>\n\n```\n\nYou can also use the [Secrets API](https:\/\/docs.databricks.com\/api\/workspace\/secrets).\n\n## Use a secret in a Spark configuration property or environment variable\n\nPreview\n\nThis feature is in [Public Preview](..\/..\/release-notes\/release-types.html).\n\nNote\n\nAvailable in Databricks Runtime 6.4 Extended Support and above.\n\nYou can reference a secret in a Spark configuration property or environment variable. Retrieved secrets are redacted from notebook output and Spark driver and executor logs.\n\nImportant\n\nKeep the following security implications in mind when referencing secrets in a Spark configuration property or environment variable:\n\n* If table access control is not enabled on a cluster, any user with Can Attach To permissions on a cluster or Run permissions on a notebook can read Spark configuration properties from within the notebook. This includes users who do not have direct permission to read a secret. Databricks recommends enabling [table access control](..\/..\/data-governance\/table-acls\/table-acl.html) on all clusters or managing access to secrets using [secret scopes](secret-scopes.html#create-a-databricks-backed-secret-scope).\n* Even when table access control is enabled, users with Can Attach To permissions on a cluster or Run permissions on a notebook can read cluster environment variables from within the notebook. Databricks does not recommend storing secrets in cluster environment variables if they must not be available to all users on the cluster.\n* Secrets *are not* redacted from the Spark driver log `stdout` and `stderr` streams. To protect sensitive data, by default, Spark driver logs are viewable only by users with CAN MANAGE permission on job, single user access mode, and shared access mode clusters. To allow users with CAN ATTACH TO or CAN RESTART permission to view the logs on these clusters, set the following Spark configuration property in the cluster configuration: `spark.databricks.acl.needAdminPermissionToViewLogs false`.\n\nOn No Isolation Shared access mode clusters, the Spark driver logs can be viewed by users with CAN ATTACH TO or CAN MANAGE permission. To limit who can read the logs to only users with the CAN MANAGE permission, set `spark.databricks.acl.needAdminPermissionToViewLogs` to `true`.\n\n### Requirements and limitations\n\nThe following requirements and limitations apply to referencing secrets in Spark configuration properties and environment variables:\n\n* Cluster owners must have CAN READ permission on the secret scope.\n* Only cluster owners can add a reference to a secret in a Spark configuration property or environment variable and edit the existing scope and name. Owners change a secret using the [Secrets API](https:\/\/docs.databricks.com\/api\/workspace\/secrets). You must restart your cluster to fetch the secret again.\n* Users with the CAN MANAGE permission on the cluster can delete a secret Spark configuration property or environment variable.\n\n### Syntax for referencing secrets in a Spark configuration property or environment variable\n\nYou can refer to a secret using any valid variable name or Spark configuration property. Databricks enables special behavior for variables referencing secrets based on the syntax of the value being set, not the variable name.\n\nThe syntax of the Spark configuration property or environment variable value must be `{{secrets\/<scope-name>\/<secret-name>}}`. The value must start with `{{secrets\/` and end with `}}`.\n\nThe variable portions of the Spark configuration property or environment variable are:\n\n* `<scope-name>`: The name of the scope in which the secret is associated.\n* `<secret-name>`: The unique name of the secret in the scope.\n\nFor example, `{{secrets\/scope1\/key1}}`.\n\nNote\n\n* There should be no spaces between the curly brackets. If there are spaces, they are treated as part of the scope or secret name.\n\n### Reference a secret with a Spark configuration property\n\nYou specify a reference to a secret in a [Spark configuration property](..\/..\/compute\/configure.html#spark-configuration) in the following format:\n\n```\nspark.<property-name> {{secrets\/<scope-name>\/<secret-name>}}\n\n```\n\nAny Spark configuration `<property-name>` can reference a secret. Each Spark configuration property can only reference one secret, but you can configure multiple Spark properties to reference secrets.\n\nFor example:\n\nYou set a Spark configuration to reference a secret:\n\n```\nspark.password {{secrets\/scope1\/key1}}\n\n```\n\nTo fetch the secret in the notebook and use it:\n\n```\nspark.conf.get(\"spark.password\")\n\n```\n\n```\nSELECT ${spark.password};\n\n```\n\n### Reference a secret in an environment variable\n\nYou specify a secret path in an [environment variable](..\/..\/compute\/configure.html#environment-variables) in the following format:\n\n```\n<variable-name>={{secrets\/<scope-name>\/<secret-name>}}\n\n```\n\nYou can use any valid variable name when you reference a secret. Access to secrets referenced in environment variables is determined by the permissions of the user who configured the cluster. Secrets stored in environmental variables are accessible by all users of the cluster, but are redacted from plaintext display like secrets referenced elsewhere.\n\nEnvironment variables that reference secrets are accessible from a cluster-scoped init script. See [Set and use environment variables with init scripts](..\/..\/init-scripts\/environment-variables.html).\n\nFor example:\n\nYou set an environment variable to reference a secret:\n\n```\nSPARKPASSWORD={{secrets\/scope1\/key1}}\n\n```\n\nTo fetch the secret in an init script, access `$SPARKPASSWORD` using the following pattern:\n\n```\nif [ -n \"$SPARKPASSWORD\" ]; then\n  # code to use ${SPARKPASSWORD}\nfi\n\n```\n\n## Manage secrets permissions\n\nThis section describes how to manage secret access control using the [What is the Databricks CLI?](..\/..\/dev-tools\/cli\/index.html) (version 0.205 and above). You can also use the [Secrets API](https:\/\/docs.databricks.com\/api\/workspace\/secrets) or [Databricks Terraform provider](..\/..\/dev-tools\/terraform\/index.html). For secret permission levels, see [Secret ACLs](..\/auth-authz\/access-control\/index.html#secrets)\n\n### Create a secret ACL\n\nTo create a secret ACL for a given secret scope using the [Databricks CLI (legacy)](..\/..\/archive\/dev-tools\/cli\/index.html)\n\n```\ndatabricks secrets put-acl <scope-name> <principal> <permission>\n\n```\n\nMaking a put request for a principal that already has an applied permission overwrites the existing permission level.\n\nThe `principal` field specifies an existing Databricks principal. A user is specified using their email address, a service principal using its `applicationId` value, and a group using its group name.\n\n### View secret ACLs\n\nTo view all secret ACLs for a given secret scope:\n\n```\ndatabricks secrets list-acls <scope-name>\n\n```\n\nTo get the secret ACL applied to a principal for a given secret scope:\n\n```\ndatabricks secrets get-acl <scope-name> <principal>\n\n```"}]}
{"request_id":"id-4032","request":"1. \"What version of Apache Spark is included in the Databricks Runtime 2.1.0-db1 cluster image?\"","response":"The version of Apache Spark included in the Databricks Runtime 2.1.0-db1 cluster image is Apache Spark 2.1.0. This version includes several new features and improvements, such as:\n\n* Structured Streaming: a new high-level API for building continuous applications.\n* Improved performance and stability for Spark SQL and DataFrames.\n* New machine learning algorithms and improvements to MLlib.\n* Support for Java 8 and Scala 2.11.\n* Improved integration with cloud storage services.\n\nFor more information about the changes and improvements in Apache Spark 2.1.0, you can refer to the official Apache Spark release notes.","expected_retrieved_context":[{"doc_uri":"https:\/\/docs.databricks.com\/archive\/runtime-release-notes\/cluster-images\/2.1.0-db1.html","content":"2.1.0-db1 cluster image  \nDatabricks released this image in late December, 2016.  \nImportant  \nThis release has been deprecated. For more information about the Databricks Runtime deprecation policy and schedule, see Databricks runtime support lifecycles.  \nThe following release notes provide information about the Spark 2.1.0-db1 cluster image powered by Apache Spark.  \nApache Spark\nApache Spark\n2.1.0-db1 cluster image includes the Apache Spark 2.1.0 release. You can consult JIRA for the detailed changes. 2.1.0-db1 cluster image also includes the following extra bug fixes and improvements:  \n[SPARK-18947][SQL]: SQLContext.tableNames should not call Catalog.listTables  \n[SPARK-18927][SS]: MemorySink for StructuredStreaming can\u2019t recover from checkpoint if location is provided in SessionConf  \n[SPARK-18281] [SQL] [PYSPARK]: Remove timeout for reading data through socket for local iterator  \n[SPARK-18761][CORE]: Introduce \u201ctask reaper\u201d to oversee task killing in executors  \n[SPARK-18928]: Check TaskContext.isInterrupted() in FileScanRDD, JDBCRDD & UnsafeSorter  \n[SPARK-18921][SQL]: check database existence with Hive.databaseExists instead of getDatabase  \n[SPARK-18700][SQL]: Add StripedLock for each table\u2019s relation in cache  \n[SPARK-18703] [SPARK-18675] [SQL]: CTAS for hive serde table should work for all hive versions AND Drop Staging Directories and Data Files  \n[SPARK-18827][CORE]: Fix cannot read broadcast on disk  \n[SPARK-18108] [SQL]: Fix a schema inconsistent bug that makes a parquet reader fail to read data  \n[SPARK-18850] [SS]: Make StreamExecution and progress classes serializable  \n[SPARK-18892] [SQL]: Alias percentile_approx approx_percentile  \n[SPARK-17949] [SQL]: Introduce a JVM object based aggregate operator  \n[SPARK-18186] [SQL]: Migrate HiveUDAFFunction to TypedImperativeAggregate for partial aggregation support\n\nChanges and Improvements\nChanges and Improvements\nSpark Session Isolation is enabled by default. With Spark Session Isolation, different notebooks attached to a cluster are in different sessions with isolated runtime configurations and current database setting. For details, see Spark Session Isolation. In order to share temporary views across notebooks when session isolated is enabled, users can use global temporary views. Users can still disable session isolation by setting spark.databricks.session.share to true.  \nPython 3 support (beta): users can now configure clusters to use Python 3 in notebooks and Spark tasks.  \nResults in notebook cells will appear in a streaming fashion instead of all at once the command finishes running, which help users with more feedback on what the state of a command is.  \nHive metastore stores all table partition metadata by default for Spark tables stored with Hive\u2019s storage formats as well as tables stored with Spark\u2019s native formats. This reduces first query latency over partitioned tables and allows for the use of DDL commands to manipulate partitions for tables stored with Spark\u2019s native formats. Users can migrate tables created by previous versions of Spark by using the MSCK command. For more details, see Scalable Partition Handling for Cloud-Native Architecture in Apache Spark 2.1 in the Databricks blog.  \nFixed bugs impacting Spark\u2019s monitoring REST API.  \nReduced spark.hadoop.parquet.memory.pool.ratio from 0.95 (the default) to 0.5 in order to avoid memory use issues when writing Parquet files.  \nFixed multiple issues related to Spark task cancellation, including issues where tasks did not respond to interrupts (SPARK-18928). If a task does not respond to cancellation, then Spark executors will now log warning messages that contain the stacktraces of the hanging tasks (SPARK-18761). This additional logging can be disabled by setting spark.task.reaper.enabled to false.  \nSpark may be configured to automatically terminate executors with uncancellable tasks after a timeout. This new feature is disabled by default. To opt-in to this behavior, set spark.task.reaper.killTimeout to a non-negative value (e.g. 60s for a one-minute timeout).\n\nKnown Issues\nKnown Issues\nLog links on the executor page are not set correctly. Please use the worker page to access stdout and stderr links of an executor for now.\n\nSystem Environment"}],"expected_response":"The Databricks Runtime 2.1.0-db1 cluster image includes Apache Spark version 2.1.0.","retrieved_context":[{"doc_uri":"https:\/\/docs.databricks.com\/archive\/runtime-release-notes\/cluster-images\/2.1.1-db6.html","content":"Updated May 28, 2024\n \n\n[Send us feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n\n* [Documentation](..\/..\/..\/index.html)\n* [Databricks documentation archive](..\/..\/index.html)\n* [Unsupported Databricks Runtime release notes](..\/index.html)\n* 2.1.1-db6 cluster image (unsupported)\n* \n\n# 2.1.1-db6 cluster image (unsupported)\n\nDatabricks released this image in early August, 2017.\n\nImportant\n\nThis release was deprecated on July 30, 2018. For more information about the Databricks Runtime deprecation policy and schedule, see [Databricks runtime support lifecycles](..\/..\/..\/release-notes\/runtime\/databricks-runtime-ver.html).\n\nThe following release notes provide information about the 2.1.1-db6 cluster image powered by Apache Spark.\n\n## Changes and Improvements\n\n* Upgraded io.netty:netty-all from 4.0.42.Final to 4.0.43.Final. This upgrade fixes the problem that Spark tasks may hang forever.\n* Fixed the issue that using an [External Apache Hive metastore (legacy)](..\/..\/external-metastores\/external-hive-metastore.html) may hit the error of `com.amazonaws.AmazonClientException: Couldn't initialize a SAX driver to create an XMLReader`.\n* Bug fixes for the Kinesis connector for Structured Streaming.\n\n## Apache Spark\n\nThe 2.1.1-db6 cluster image includes Apache Spark 2.1.1. There is no additional patch applied to Spark compared with 2.1.1-db5.\n\n## Known Issues\n\n* Log links on the executor page are not set correctly. Please use the worker page to access stdout and stderr links of an executor for now.\n\n## Maintenance Updates\n\nSee [2.1.1-db6 maintenance updates](..\/maintenance-updates-archive.html#211-db6).\n\n## System Environment\n\n* **Operating System**: Ubuntu 16.04.1 LTS\n* **Java**: 1.8.0\\_111\n* **Scala**: 2.10.6 (Scala 2.10 cluster version)\/2.11.8 (Scala 2.11 cluster version)\n* **Python**: 2.7.12 (or 3.5.2 if using Python 3)\n* **R**: R version 3.2.3 (2015-12-10)\n\n### Pre-installed Python Libraries\n\n| Library | Version | Library | Version | Library | Version |\n| --- | --- | --- | --- | --- | --- |\n| ansi2html | 1.1.1 | argparse | 1.2.1 | boto | 2.42.0 |\n| boto3 | 1.4.1 | botocore | 1.4.70 | brewer2mpl | 1.4.1 |\n| certifi | 2016.2.28 | cffi | 1.7.0 | chardet | 2.3.0 |\n| colorama | 0.3.7 | configobj | 5.0.6 | cryptography | 1.5 |\n| cycler | 0.10.0 | Cython | 0.24.1 | decorator | 4.0.10 |\n| docutils | 0.13.1 | enum34 | 1.1.6 | et-xmlfile | 1.0.1 |\n| freetype-py | 1.0.2 | funcsigs | 1.0.2 | fusepy | 2.0.4 |\n| futures | 3.0.5 | ggplot | 0.6.8 | html5lib | 0.999 |\n| idna | 2.1 | ipaddress | 1.0.16 | ipython | 2.2.0 |\n| ipython-genutils | 0.1.0 | jdcal | 1.2 | Jinja2 | 2.8 |\n| jmespath | 0.9.0 | llvmlite | 0.13.0 | lxml | 3.6.4 |\n| MarkupSafe | 0.23 | matplotlib | 1.5.3 | mpld3 | 0.2 |\n| msgpack-python | 0.4.7 | ndg-httpsclient | 0.3.3 | numba | 0.28.1 |\n| numpy | 1.11.1 | openpyxl | 2.3.2 | pandas | 0.18.1 |\n| pathlib2 | 2.1.0 | patsy | 0.4.1 | pexpect | 4.0.1 |\n| pickleshare | 0.7.4 | Pillow | 3.3.1 | pip | 9.0.1 |\n| ply | 3.9 | prompt-toolkit | 1.0.7 | psycopg2 | 2.6.2 |\n| ptyprocess | 0.5.1 | py4j | 0.10.3 | pyasn1 | 0.1.9 |\n| pycparser | 2.14 | Pygments | 2.1.3 | PyGObject | 3.20.0 |\n| pyOpenSSL | 16.0.0 | pyparsing | 2.1.4 | pypng | 0.0.18 |\n| Python | 2.7.12 | python-dateutil | 2.5.3 | python-geohash | 0.8.5 |\n| pytz | 2016.6.1 | requests | 2.11.1 | s3transfer | 0.1.9 |\n| scikit-learn | 0.17.1 | scipy | 0.18.1 | scour | 0.32 |\n| seaborn | 0.7.1 | setuptools | 32.3.1 | simplejson | 3.8.2 |\n| simples3 | 1.0 | singledispatch | 3.4.0.3 | six | 1.10.0 |\n| statsmodels | 0.6.1 | traitlets | 4.3.0 | urllib3 | 1.19.1 |\n| virtualenv | 15.0.1 | wcwidth | 0.1.7 | wheel | 0.30.0a0 |\n| wsgiref | 0.1.2 |  |  |  |  |\n\n### Pre-installed R Libraries\n\n| Library | Version | Library | Version | Library | Version |\n| --- | --- | --- | --- | --- | --- |\n| abind | 1.4-3 | assertthat | 0.1 | base | 3.2.3 |\n| BH | 1.60.0-2 | bitops | 1.0-6 | boot | 1.3-17 |\n| brew | 1.0-6 | car | 2.1-3 | caret | 6.0-71 |\n| chron | 2.3-47 | class | 7.3-14 | cluster | 2.0.5 |\n| codetools | 0.2-14 | colorspace | 1.2-4 | compiler | 3.2.3 |\n| crayon | 1.3.1 | curl | 2.2 | data.table | 1.9.6 |\n| datasets | 3.2.3 | DBI | 0.5-1 | devtools | 1.12.0 |\n| dichromat | 2.0-0 | digest | 0.6.9 | doMC | 1.3.4 |\n| dplyr | 0.5.0 | foreach | 1.4.3 | foreign | 0.8-66 |\n| gbm | 2.1.1 | ggplot2 | 2.1.0 | git2r | 0.15.0 |\n| glmnet | 2.0-5 | graphics | 3.2.3 | grDevices | 3.2.3 |\n| grid | 3.2.3 | gsubfn | 0.6-6 | gtable | 0.1.2 |\n| h2o | 3.10.0.8 | httr | 1.2.1 | hwriter | 1.3.2 |\n| hwriterPlus | 1.0-3 | iterators | 1.0.8 | jsonlite | 1.1 |\n| KernSmooth | 2.23-15 | labeling | 0.3 | lattice | 0.20-34 |\n| lazyeval | 0.2.0 | littler | 0.3.0 | lme4 | 1.1-12 |\n| lubridate | 1.6.0 | magrittr | 1.5 | mapproj | 1.2-4 |\n| maps | 3.0.2 | MASS | 7.3-45 | Matrix | 1.2-7.1 |\n| MatrixModels | 0.4-1 | memoise | 1.0.0 | methods | 3.2.3 |\n| mgcv | 1.8-11 | mime | 0.5 | minqa | 1.2.4 |\n| multicore | 0.2 | munsell | 0.4.2 | mvtnorm | 1.0-5 |\n| nlme | 3.1-124 | nloptr | 1.0.4 | nnet | 7.3-12 |\n| openssl | 0.9.4 | parallel | 3.2.3 | pbkrtest | 0.4-6 |\n| pkgKitten | 0.1.3 | plyr | 1.8.4 | praise | 1.0.0 |\n| pROC | 1.8 | proto | 0.3-10 | quantreg | 5.29 |\n| R.methodsS3 | 1.7.1 | R.oo | 1.20.0 | R.utils | 2.4.0 |\n| R6 | 2.2.0 | randomForest | 4.6-12 | RColorBrewer | 1.1-2 |\n| Rcpp | 0.12.7 | RcppEigen | 0.3.2.9.0 | RCurl | 1.95-4.8 |\n| reshape2 | 1.4.2 | RODBC | 1.3-12 | roxygen2 | 5.0.1 |\n| rpart | 4.1-10 | Rserve | 1.7-3 | RSQLite | 1.0.0 |\n| rstudioapi | 0.6 | scales | 0.3.0 | sp | 1.0-15 |\n| SparkR | 2.1.1 | SparseM | 1.72 | spatial | 7.3-11 |\n| splines | 3.2.3 | sqldf | 0.4-10 | statmod | 1.4.26 |\n| stats | 3.2.3 | stats4 | 3.2.3 | stringi | 1.0-1 |\n| stringr | 1.0.0 | survival | 2.38-3 | tcltk | 3.2.3 |\n| TeachingDemos | 2.10 | testthat | 1.0.2 | tibble | 1.2 |\n| tools | 3.2.3 | utils | 3.2.3 | whisker | 0.3-2 |\n| withr | 1.0.2 |  |  |  |  |\n\n### Pre-installed Java and Scala libraries (Scala 2.10 cluster version)"},{"doc_uri":"https:\/\/docs.databricks.com\/archive\/runtime-release-notes\/cluster-images\/2.1.1-db5.html","content":"Updated May 28, 2024\n \n\n[Send us feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n\n* [Documentation](..\/..\/..\/index.html)\n* [Databricks documentation archive](..\/..\/index.html)\n* [Unsupported Databricks Runtime release notes](..\/index.html)\n* 2.1.1-db5 cluster image (unsupported)\n* \n\n# 2.1.1-db5 cluster image (unsupported)\n\nDatabricks released this image in late May, 2017.\n\nImportant\n\nThis release was deprecated on August 3, 2017. For more information about the Databricks Runtime deprecation policy and schedule, see [Databricks runtime support lifecycles](..\/..\/..\/release-notes\/runtime\/databricks-runtime-ver.html).\n\nThe following release notes provide information about the 2.1.1-db5 cluster image powered by Apache Spark.\n\n## Changes and Improvements\n\n* Added [Higher-order functions](..\/..\/..\/optimizations\/higher-order-lambda-functions.html) support for manipulating array data.\n* Removed xerces:xercesImpl and xml-apis:xml-apis from Spark\u2019s classpath to avoid of introducing conflicts with xerces included in OpenJDK.\n* Bug fixes and stability improvements to Spark.\n\n* Structured Streaming connector for Amazon Kinesis. More details at [Connect to Amazon Kinesis](..\/..\/..\/connect\/streaming\/kinesis.html).\n\n## Apache Spark\n\nThe 2.1.1-db5 cluster image includes Apache Spark 2.1.1. In addition to 2.1.1-db4, the 2.1.1-db5 cluster image also includes the following extra bug fixes and improvements made to Spark:\n\n* [[SPARK-20705]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-20705)[WEB-UI] The sort function can not be used in the master page when you use Firefox or Google Chrome.\n* [[SPARK-17424]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-17424) Fix unsound substitution bug in ScalaReflection.\n* [[SPARK-20665]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-20665)[SQL] Bround\u201d and \u201cRound\u201d function return NULL\n* [[SPARK-20685]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-20685) Fix BatchPythonEvaluation bug in case of single UDF w\/ repeated arg.\n* [[SPARK-20688]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-20688)[SQL] correctly check analysis for scalar sub-queries\n* [[SPARK-20631]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-20631)[PYTHON][ML] LogisticRegression.\\_checkThresholdConsistency should use values not Params\n* [[SPARK-20686]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-20686)[SQL] PropagateEmptyRelation incorrectly handles aggregate without grouping\n* [[SPARK-17685]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-17685)[SQL] Make SortMergeJoinExec\u2019s currentVars is null when calling createJoinKey\n* [[SPARK-20627]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-20627)[PYSPARK] Drop the hadoop distirbution name from the Python version\n* [[SPARK-20616]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-20616) RuleExecutor logDebug of batch results should show diff to start of batch\n* [[SPARK-20613]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-20613) Remove excess quotes in Windows executable\n* [[SPARK-20546]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-20546)[DEPLOY] spark-class gets syntax error in posix mode\n* [[SPARK-20558]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-20558)[CORE] clear InheritableThreadLocal variables in SparkContext when stopping it\n* [[SPARK-20540]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-20540)[CORE] Fix unstable executor requests.\n* [[SPARK-20496]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-20496)[SS] Bug in KafkaWriter Looks at Unanalyzed Plans\n* [[SPARK-20439]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-20439)[SQL][BACKPORT-2.1] Fix Catalog API listTables and getTable when failed to fetch table metadata\n\n## Known Issues\n\n* Log links on the executor page are not set correctly. Please use the worker page to access stdout and stderr links of an executor for now.\n\n## System Environment\n\n* **Operating System**: Ubuntu 16.04.1 LTS\n* **Java**: 1.8.0\\_111\n* **Scala**: 2.10.6 (Scala 2.10 cluster version)\/2.11.8 (Scala 2.11 cluster version)\n* **Python**: 2.7.12 (or 3.5.2 if using Python 3)\n* **R**: R version 3.2.3 (2015-12-10)\n\n### Pre-installed Python Libraries\n\n| Library | Version | Library | Version | Library | Version |\n| --- | --- | --- | --- | --- | --- |\n| ansi2html | 1.1.1 | argparse | 1.2.1 | boto | 2.42.0 |\n| boto3 | 1.4.1 | botocore | 1.4.70 | brewer2mpl | 1.4.1 |\n| certifi | 2016.2.28 | cffi | 1.7.0 | chardet | 2.3.0 |\n| colorama | 0.3.7 | configobj | 5.0.6 | cryptography | 1.5 |\n| cycler | 0.10.0 | Cython | 0.24.1 | decorator | 4.0.10 |\n| docutils | 0.13.1 | enum34 | 1.1.6 | et-xmlfile | 1.0.1 |\n| freetype-py | 1.0.2 | funcsigs | 1.0.2 | fusepy | 2.0.4 |\n| futures | 3.0.5 | ggplot | 0.6.8 | html5lib | 0.999 |\n| idna | 2.1 | ipaddress | 1.0.16 | ipython | 2.2.0 |\n| ipython-genutils | 0.1.0 | jdcal | 1.2 | Jinja2 | 2.8 |\n| jmespath | 0.9.0 | llvmlite | 0.13.0 | lxml | 3.6.4 |\n| MarkupSafe | 0.23 | matplotlib | 1.5.3 | mpld3 | 0.2 |\n| msgpack-python | 0.4.7 | ndg-httpsclient | 0.3.3 | numba | 0.28.1 |\n| numpy | 1.11.1 | openpyxl | 2.3.2 | pandas | 0.18.1 |\n| pathlib2 | 2.1.0 | patsy | 0.4.1 | pexpect | 4.0.1 |\n| pickleshare | 0.7.4 | Pillow | 3.3.1 | pip | 9.0.1 |\n| ply | 3.9 | prompt-toolkit | 1.0.7 | psycopg2 | 2.6.2 |\n| ptyprocess | 0.5.1 | py4j | 0.10.3 | pyasn1 | 0.1.9 |\n| pycparser | 2.14 | Pygments | 2.1.3 | PyGObject | 3.20.0 |\n| pyOpenSSL | 16.0.0 | pyparsing | 2.1.4 | pypng | 0.0.18 |\n| Python | 2.7.12 | python-dateutil | 2.5.3 | python-geohash | 0.8.5 |\n| pytz | 2016.6.1 | requests | 2.11.1 | s3transfer | 0.1.9 |\n| scikit-learn | 0.17.1 | scipy | 0.18.1 | scour | 0.32 |\n| seaborn | 0.7.1 | setuptools | 32.3.1 | simplejson | 3.8.2 |\n| simples3 | 1.0 | singledispatch | 3.4.0.3 | six | 1.10.0 |\n| statsmodels | 0.6.1 | traitlets | 4.3.0 | urllib3 | 1.19.1 |\n| virtualenv | 15.0.1 | wcwidth | 0.1.7 | wheel | 0.30.0a0 |\n| wsgiref | 0.1.2 |  |  |  |  |\n\n### Pre-installed R Libraries\n\n| Library | Version | Library | Version | Library | Version |\n| --- | --- | --- | --- | --- | --- |\n| abind | 1.4-3 | assertthat | 0.1 | base | 3.2.3 |\n| BH | 1.60.0-2 | bitops | 1.0-6 | boot | 1.3-17 |\n| brew | 1.0-6 | car | 2.1-3 | caret | 6.0-71 |\n| chron | 2.3-47 | class | 7.3-14 | cluster | 2.0.5 |\n| codetools | 0.2-14 | colorspace | 1.2-4 | compiler | 3.2.3 |\n| crayon | 1.3.1 | curl | 2.2 | data.table | 1.9.6 |\n| datasets | 3.2.3 | DBI | 0.5-1 | devtools | 1.12.0 |\n| dichromat | 2.0-0 | digest | 0.6.9 | doMC | 1.3.4 |\n| dplyr | 0.5.0 | foreach | 1.4.3 | foreign | 0.8-66 |\n| gbm | 2.1.1 | ggplot2 | 2.1.0 | git2r | 0.15.0 |\n| glmnet | 2.0-5 | graphics | 3.2.3 | grDevices | 3.2.3 |\n| grid | 3.2.3 | gsubfn | 0.6-6 | gtable | 0.1.2 |\n| h2o | 3.10.0.8 | httr | 1.2.1 | hwriter | 1.3.2 |\n| hwriterPlus | 1.0-3 | iterators | 1.0.8 | jsonlite | 1.1 |\n| KernSmooth | 2.23-15 | labeling | 0.3 | lattice | 0.20-34 |\n| lazyeval | 0.2.0 | littler | 0.3.0 | lme4 | 1.1-12 |\n| lubridate | 1.6.0 | magrittr | 1.5 | mapproj | 1.2-4 |\n| maps | 3.0.2 | MASS | 7.3-45 | Matrix | 1.2-7.1 |\n| MatrixModels | 0.4-1 | memoise | 1.0.0 | methods | 3.2.3 |\n| mgcv | 1.8-11 | mime | 0.5 | minqa | 1.2.4 |\n| multicore | 0.2 | munsell | 0.4.2 | mvtnorm | 1.0-5 |\n| nlme | 3.1-124 | nloptr | 1.0.4 | nnet | 7.3-12 |\n| openssl | 0.9.4 | parallel | 3.2.3 | pbkrtest | 0.4-6 |\n| pkgKitten | 0.1.3 | plyr | 1.8.4 | praise | 1.0.0 |\n| pROC | 1.8 | proto | 0.3-10 | quantreg | 5.29 |\n| R.methodsS3 | 1.7.1 | R.oo | 1.20.0 | R.utils | 2.4.0 |\n| R6 | 2.2.0 | randomForest | 4.6-12 | RColorBrewer | 1.1-2 |\n| Rcpp | 0.12.7 | RcppEigen | 0.3.2.9.0 | RCurl | 1.95-4.8 |\n| reshape2 | 1.4.2 | RODBC | 1.3-12 | roxygen2 | 5.0.1 |\n| rpart | 4.1-10 | Rserve | 1.7-3 | RSQLite | 1.0.0 |\n| rstudioapi | 0.6 | scales | 0.3.0 | sp | 1.0-15 |\n| SparkR | 2.1.1 | SparseM | 1.72 | spatial | 7.3-11 |\n| splines | 3.2.3 | sqldf | 0.4-10 | statmod | 1.4.26 |\n| stats | 3.2.3 | stats4 | 3.2.3 | stringi | 1.0-1 |\n| stringr | 1.0.0 | survival | 2.38-3 | tcltk | 3.2.3 |\n| TeachingDemos | 2.10 | testthat | 1.0.2 | tibble | 1.2 |\n| tools | 3.2.3 | utils | 3.2.3 | whisker | 0.3-2 |\n| withr | 1.0.2 |  |  |  |  |\n\n### Pre-installed Java and Scala libraries (Scala 2.10 cluster version)"},{"doc_uri":"https:\/\/docs.databricks.com\/archive\/runtime-release-notes\/cluster-images\/2.1.0-db1.html","content":"2.1.0-db1 cluster image | Databricks on AWS\n\n* [![Databricks](..\/..\/..\/_static\/small-scale-lockup-full-color-rgb.svg)](https:\/\/www.databricks.com\/)\n\n* [![Databricks](..\/..\/..\/_static\/small-scale-lockup-full-color-rgb.svg)](https:\/\/www.databricks.com\/)\n* [Help Center](https:\/\/help.databricks.com\/s\/)\n* [Documentation](https:\/\/docs.databricks.com\/en\/index.html)\n* [Knowledge Base](https:\/\/kb.databricks.com\/)\n\n* [Community](https:\/\/community.databricks.com)\n* [Support](https:\/\/help.databricks.com)\n* [Feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n* [Try Databricks](https:\/\/databricks.com\/try-databricks)\n\n[![](..\/..\/..\/_static\/icons\/globe.png)English](javascript:void(0))\n* [English](..\/..\/..\/..\/en\/archive\/runtime-release-notes\/cluster-images\/2.1.0-db1.html)\n* [\u65e5\u672c\u8a9e](..\/..\/..\/..\/ja\/archive\/runtime-release-notes\/cluster-images\/2.1.0-db1.html)\n* [Portugu\u00eas](..\/..\/..\/..\/pt\/archive\/runtime-release-notes\/cluster-images\/2.1.0-db1.html)\n\n[![](..\/..\/..\/_static\/icons\/aws.svg)Amazon\n Web Services](javascript:void(0))\n* [![](..\/..\/..\/_static\/icons\/azure.svg)Microsoft Azure](https:\/\/learn.microsoft.com\/azure\/databricks\/archive\/runtime-release-notes\/cluster-images\/2.1.0-db1)\n* [![](..\/..\/..\/_static\/icons\/gcp.svg)Google Cloud Platform](https:\/\/docs.gcp.databricks.com\/archive\/runtime-release-notes\/cluster-images\/2.1.0-db1.html)\n\n[Databricks on AWS](..\/..\/..\/index.html)\nGet started\n\n* [Get started](..\/..\/..\/getting-started\/index.html)\n* [What is Databricks?](..\/..\/..\/introduction\/index.html)\n* [DatabricksIQ](..\/..\/..\/databricksiq\/index.html)\n* [Release notes](..\/..\/..\/release-notes\/index.html)\n\nLoad & manage data\n\n* [Connect to data sources](..\/..\/..\/connect\/index.html)\n* [Connect to compute](..\/..\/..\/compute\/index.html)\n* [Discover data](..\/..\/..\/discover\/index.html)\n* [Query data](..\/..\/..\/query\/index.html)\n* [Ingest data](..\/..\/..\/ingestion\/index.html)\n* [Transform data](..\/..\/..\/transform\/index.html)\n* [Monitor data and AI assets](..\/..\/..\/lakehouse-monitoring\/index.html)\n* [Share data (Delta Sharing)](..\/..\/..\/data-sharing\/index.html)\n* [Databricks Marketplace](..\/..\/..\/marketplace\/index.html)\n\nWork with data\n\n* [Data engineering](..\/..\/..\/workspace-index.html)\n* [Generative AI & LLMs](..\/..\/..\/generative-ai\/generative-ai.html)\n* [Machine learning](..\/..\/..\/machine-learning\/index.html)\n* [Model serving](..\/..\/..\/machine-learning\/model-serving\/index.html)\n* [Data warehousing](..\/..\/..\/sql\/index.html)\n* [Delta Lake](..\/..\/..\/delta\/index.html)\n* [Developers](..\/..\/..\/languages\/index.html)\n* [Technology partners](..\/..\/..\/integrations\/index.html)\n\nAdministration\n\n* [Account and workspace administration](..\/..\/..\/admin\/index.html)\n* [Security and compliance](..\/..\/..\/security\/index.html)\n* [Data governance (Unity Catalog)](..\/..\/..\/data-governance\/index.html)\n* [Lakehouse architecture](..\/..\/..\/lakehouse-architecture\/index.html)\n\nReference & resources\n\n* [Reference](..\/..\/..\/reference\/api.html)\n* [Resources](..\/..\/..\/resources\/index.html)\n* [What\u2019s coming?](..\/..\/..\/whats-coming.html)\n* [Documentation archive](..\/..\/index.html)\n\n Updated May 28, 2024\n \n\n[Send us feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n\n* [Documentation](..\/..\/..\/index.html)\n* 2.1.0-db1 cluster image\n* \n\n# 2.1.0-db1 cluster image\n\nDatabricks released this image in late December, 2016.\n\nImportant\n\nThis release has been deprecated. For more information about the Databricks Runtime deprecation policy and schedule, see [Databricks runtime support lifecycles](..\/..\/..\/release-notes\/runtime\/databricks-runtime-ver.html).\n\nThe following release notes provide information about the Spark 2.1.0-db1 cluster image powered by Apache Spark.\n\n## Apache Spark\n\n2.1.0-db1 cluster image includes the Apache Spark 2.1.0 release.\nYou can consult JIRA for the [detailed changes](https:\/\/issues.apache.org\/jira\/secure\/ReleaseNote.jspa?projectId=12315420&version=12335644). 2.1.0-db1 cluster image also includes the following extra bug fixes and improvements:\n\n* [[SPARK-18947][SQL]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-18947): SQLContext.tableNames should not call Catalog.listTables\n* [[SPARK-18927][SS]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-18927): MemorySink for StructuredStreaming can\u2019t recover from checkpoint if location is provided in SessionConf\n* [[SPARK-18281] [SQL] [PYSPARK]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-18281): Remove timeout for reading data through socket for local iterator\n* [[SPARK-18761][CORE]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-18761): Introduce \u201ctask reaper\u201d to oversee task killing in executors\n* [[SPARK-18928]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-18928): Check TaskContext.isInterrupted() in FileScanRDD, JDBCRDD & UnsafeSorter\n* [[SPARK-18921][SQL]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-18921): check database existence with Hive.databaseExists instead of getDatabase\n* [[SPARK-18700][SQL]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-18700): Add StripedLock for each table\u2019s relation in cache\n* [[SPARK-18703]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-18703) [[SPARK-18675]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-18675) [SQL]: CTAS for hive serde table should work for all hive versions AND Drop Staging Directories and Data Files\n* [[SPARK-18827][CORE]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-18827): Fix cannot read broadcast on disk\n* [[SPARK-18108] [SQL]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-18108): Fix a schema inconsistent bug that makes a parquet reader fail to read data\n* [[SPARK-18850] [SS]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-18850): Make StreamExecution and progress classes serializable\n* [[SPARK-18892] [SQL]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-18892): Alias percentile\\_approx approx\\_percentile\n* [[SPARK-17949] [SQL]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-17949): Introduce a JVM object based aggregate operator\n* [[SPARK-18186] [SQL]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-18186): Migrate HiveUDAFFunction to TypedImperativeAggregate for partial aggregation support\n\n## Changes and Improvements\n\n* [Spark Session Isolation](..\/..\/..\/notebooks\/notebook-isolation.html) is enabled by default. With Spark Session Isolation, different notebooks attached to a cluster are in different sessions with isolated runtime configurations and current database setting. For details, see [Spark Session Isolation](..\/..\/..\/notebooks\/notebook-isolation.html). In order to share temporary views across notebooks when session isolated is enabled, users can use [global temporary views](https:\/\/issues.apache.org\/jira\/browse\/SPARK-17338). Users can still disable session isolation by setting `spark.databricks.session.share` to `true`.\n* Python 3 support (beta): users can now configure clusters to use Python 3 in notebooks and Spark tasks.\n* Results in notebook cells will appear in a streaming fashion instead of all at once the command finishes running, which help users with more feedback on what the state of a command is.\n* Hive metastore stores all table partition metadata by default for Spark tables stored with Hive\u2019s storage formats as well as tables stored with Spark\u2019s native formats. This reduces first query latency over partitioned tables and allows for the use of DDL commands to manipulate partitions for tables stored with Spark\u2019s native formats. Users can migrate tables created by previous versions of Spark by using the `MSCK` command. For more details, see [Scalable Partition Handling for Cloud-Native Architecture in Apache Spark 2.1](https:\/\/databricks.com\/blog\/2016\/12\/15\/scalable-partition-handling-for-cloud-native-architecture-in-apache-spark-2-1.html) in the Databricks blog.\n* Fixed bugs impacting [Spark\u2019s monitoring REST API](https:\/\/spark.apache.org\/docs\/2.0.2\/monitoring.html#rest-api).\n* Reduced `spark.hadoop.parquet.memory.pool.ratio` from 0.95 (the default) to 0.5 in order to avoid memory use issues when writing Parquet files.\n* Fixed multiple issues related to Spark task cancellation, including issues where tasks did not respond to interrupts ([SPARK-18928](https:\/\/issues.apache.org\/jira\/browse\/SPARK-18928)). If a task does not respond to cancellation, then Spark executors will now log warning messages that contain the stacktraces of the hanging tasks ([SPARK-18761](https:\/\/issues.apache.org\/jira\/browse\/SPARK-18761)). This additional logging can be disabled by setting `spark.task.reaper.enabled` to `false`.\n\nSpark may be configured to automatically terminate executors with uncancellable tasks after a timeout. This new feature is disabled by default. To opt-in to this behavior, set `spark.task.reaper.killTimeout` to a non-negative value (e.g. `60s` for a one-minute timeout).\n\n## Known Issues\n\n* Log links on the executor page are not set correctly. Please use the worker page to access stdout and stderr links of an executor for now.\n\n## System Environment\n\n* **Operating System**: Ubuntu 16.04.1 LTS\n* **Java**: 1.8.0\\_111\n* **Scala**: 2.10.6 (Scala 2.10 cluster version)\/2.11.8 (Scala 2.11 cluster version)\n* **Python**: 2.7.12 (or 3.5.2 if using Python 3)\n* **R**: R version 3.2.3 (2015-12-10)\n\n### Pre-installed Python Libraries"},{"doc_uri":"https:\/\/docs.databricks.com\/archive\/runtime-release-notes\/10.0.html","content":"Updated May 28, 2024\n \n\n[Send us feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n\n* [Documentation](..\/..\/index.html)\n* [Databricks documentation archive](..\/index.html)\n* [Unsupported Databricks Runtime release notes](index.html)\n* Databricks Runtime 10.0 (unsupported)\n* \n\n# Databricks Runtime 10.0 (unsupported)\n\nThe following release notes provide information about Databricks Runtime 10.0 and Databricks Runtime 10.0 Photon, powered by Apache Spark 3.2.0. Databricks released these images in October 2021. [Photon](..\/..\/compute\/photon.html) is in [Public Preview](..\/..\/release-notes\/release-types.html#runtime-releases).\n\n## New features and improvements\n\n* [New version of Apache Spark](#new-version-of-apache-spark)\n* [SELECT statement now supports QUALIFY clause to filter window function results](#select-statement-now-supports-qualify-clause-to-filter-window-function-results)\n* [Cluster support for JDK 11 (Public Preview)](#cluster-support-for-jdk-11-public-preview)\n* [Auto Loader now treats schemas as nullable](#auto-loader-now-treats-schemas-as-nullable)\n\n### [New version of Apache Spark](#id5)\n\nDatabricks Runtime 10.0 and Databricks Runtime 10.0 Photon include Apache Spark 3.2.0. For details, see [Apache Spark](#spark).\n\n### [SELECT statement now supports QUALIFY clause to filter window function results](#id6)\n\nThe `SELECT` statement now supports the `QUALIFY` clause. `QUALIFY` can be used to filter the results of window functions. One or more window functions must be present in either the `SELECT` list or the `QUALIFY` condition. For example:\n\n```\nSELECT * FROM t QUALIFY SUM(c2) OVER (PARTITION BY c1) > 0;\n\n```\n\n### [Cluster support for JDK 11 (Public Preview)](#id7)\n\nDatabricks now provides cluster support for Java Development Kit (JDK) 11.\n\nWhen you create a cluster, you can specify that the cluster uses JDK 11 (for both the driver and executor). To do this, add the following environment variable to **Advanced Options > Spark > Environment Variables**:\n\n```\nJNAME=zulu11-ca-amd64\n\n```\n\n### [Auto Loader now treats schemas as nullable](#id8)\n\nAuto Loader now treats all inferred and user-provided schemas as nullable by default. This is to avoid potential data corruption in cases where the data contains null fields for non-nullable columns. A new configuration is introduced to track this behavior, `spark.databricks.cloudFiles.schema.forceNullable`. By default, this configuration contains the setting of `spark.sql.streaming.fileSource.schema.forceNullable`, which the FileStreamSource in Apache Spark uses and is set to `true` by default.\n\n## Breaking changes\n\n### Breaking changes for all Spark SQL users\n\n* New `spark.databricks.behaviorChange.SC78546CorrelatedPredicate.enabled` configuration: when set to `true`, allows a subset of correlated equality predicates when a subquery is aggregated. The default setting is `true`.\n* New `spark.databricks.behaviorChange.SC82201BlockAutoAlias.enabled` configuration: when set to `true`, blocks autogenerating aliases when views are created. The default setting is `true`.\n* New `spark.databricks.behaviorChange.SC81078CTASWithLocation.enabled` configuration: when set to `true`, disallows `CREATE TABLE AS SELECT` with a non-empty location. The default setting is `true`. Note that when `spark.sql.legacy.allowNonEmptyLocationInCTAS` is also set to `true`, this configuration has no effect, and `CREATE TABLE AS SELECT` with a non-empty location is always allowed.\n\n### Breaking changes for Spark SQL users who enable ANSI mode\n\nFor information about ANSI mode, see [ANSI compliance in Databricks Runtime](..\/..\/sql\/language-manual\/sql-ref-ansi-compliance.html).\n\n* New `spark.databricks.behaviorChange.SC83587NextDayDOW.enabled` configuration: when set to `true`, an invalid `dayOfWeek` argument to the `next_day` function throws an `IllegalArgumentException` in ANSI mode; otherwise it returns `null`. The default setting is `true`.\n* New `spark.databricks.behaviorChange.SC83796CanCast.enabled` configuration: when set to `true`, enables new explicit cast syntax rules in ANSI mode. The default setting is `true`.\n* New `spark.databricks.behaviorChange.SC79099CastStringToBoolean.enabled` configuration: when set to `true`, a parse error exception is thrown when casting a string to a Boolean; otherwise, returns `null`. The default setting is `true`.\n* New `spark.databricks.behaviorChange.SC79064AbsOutsideRange.enabled` configuration: when set to `true`, the `abs` function throws an exception if the input is out of range. The default setting is `true`.\n\n### Breaking changes for all Python users\n\n* The Python API `delta.tables.DeltaTable.convertToDelta` now returns the correct `DeltaTable` Python object which can be used to run Delta Lake commands. Previously it returned an internal object which could not be called directly.\n\n## Library upgrades\n\n### Apache Hadoop 3 upgrade\n\n* Databricks Runtime 10.0 upgrades the Hadoop dependency from Hadoop 2.7.4 to Hadoop 3.3.1.\n\n#### Behavior changes\n\n* Hadoop 3 uses the [hadoop-client-api](https:\/\/mvnrepository.com\/artifact\/org.apache.hadoop\/hadoop-client-api) and [hadoop-client-runtime](https:\/\/mvnrepository.com\/artifact\/org.apache.hadoop\/hadoop-client-runtime) libraries instead of the [hadoop-common](https:\/\/mvnrepository.com\/artifact\/org.apache.hadoop\/hadoop-common\/3.3.1) library, which shades certain third-party dependencies that could potentially be used in Hadoop public APIs or extensions.\n* Hadoop 3 configuration options have changed since Hadoop 2. For Hadoop 3.3.1 options, see [core-default.xml](https:\/\/hadoop.apache.org\/docs\/r3.3.1\/hadoop-project-dist\/hadoop-common\/core-default.xml).\n* Databricks has updated some of the default configurations for Hadoop 3 to be consistent with Hadoop 2, to make sure storage connectors have the same default authentication settings and levels of performance:\n\n\t+ `fs.azure.authorization.caching.enable` = `false`\n\t+ `fs.s3a.attempts.maximum` = `10`\n\t+ `fs.s3a.block.size` = `67108864`\n\t+ `fs.s3a.connection.timeout` = `50000`\n\t+ `fs.s3a.max.total.tasks` = `1000`\n\t+ `fs.s3a.retry.limit` = `20`\n\t+ `fs.s3a.retry.throttle.interval` = `500ms`\n\t+ `fs.s3a.assumed.role.credentials.provider` = `com.amazonaws.auth.InstanceProfileCredentialsProvider`\n\t+ `fs.s3a.aws.credentials.provider` = `BasicAWSCredentialsProvider, DatabricksInstanceProfileCredentialsProvider, EnvironmentVariableCredentialsProvider, AnonymousAWSCredentialsProvider`\n* The Google Cloud Storage (GCS) connector has been upgraded from 2.1.6 to 2.2.2.\n* The Amazon Redshift connector now uses the `s3a:\/\/` scheme. The `s3n:\/\/` scheme is deprecated.\n* Handles the OSS class names for the Amazon S3 MetadataStore API in addition to shaded classes. This enables processing OSS configurations without requiring the shaded class names.\n\n\t+ For example, you can specify the `org.apache.hadoop.fs.s3a.s3guard.NullMetadataStore` class in the Hadoop configuration.\n* Makes the `new Configuration()` and `sparkContext.hadoopConfiguration` consistent.\n\n\t+ Now every time a new Hadoop configuration is created, it will be consistent with the predefined Hadoop configuration in `sparkContext.hadoopConfiguration` in the Databricks Runtime, including file system schemes and their default configuration.\n* Although Hadoop storage connectors included in the Databricks Runtime are fully compatible with Hadoop 3.3.1, they are not guaranteed to be in sync with OSS Hadoop 3.3.1 connectors and might have different behavior.\n\n\t+ The Amazon S3 connector still allows (although with a warning) user:secret authentication in S3 URLs, compared to [HADOOP-14833](https:\/\/issues.apache.org\/jira\/browse\/HADOOP-14833) which removes it.\n* globStatus now always returns sorted results([HADOOP-10798](https:\/\/issues.apache.org\/jira\/browse\/HADOOP-10798))\n* Added fs.s3a.endpoint if unset and fs.s3a.endpoint region is null([SPARK-35878](https:\/\/issues.apache.org\/jira\/browse\/SPARK-35878))\n\n\t+ Databricks provided Amazon S3 region auto resolution might not be triggered in some cases, due to the global endpoint being set. This is not an issue, as the AWS SDK will resolve the region correctly.\n* Add fs.s3a.downgrade.syncable.exceptions if not set([SPARK-35868](https:\/\/issues.apache.org\/jira\/browse\/SPARK-35868))\n* LZ4 and Snappy codecs do not rely on the native Hadoop library([HADOOP-17125](https:\/\/issues.apache.org\/jira\/browse\/HADOOP-17125))\n\n#### Known issues\n\n* [SPARK-36681](https:\/\/issues.apache.org\/jira\/browse\/SPARK-36681) Using SnappyCodec to write sequence file will fail with UnsatisfiedLinkError due to known issue in Hadoop 3.3.1([HADOOP-17891](https:\/\/issues.apache.org\/jira\/browse\/HADOOP-17891))\n\n## Apache Spark\n\nDatabricks Runtime 10.0 includes Apache Spark 3.2.0.\n\nIn this section:\n\n* [Highlights](#highlights)\n* [Core and Spark SQL](#core-and-spark-sql)\n* [Structured Streaming](#structured-streaming)\n* [PySpark](#pyspark)\n* [MLlib](#mllib)\n* [Deprecations and removals](#deprecations-and-removals)\n\n### [Highlights](#id9)\n\n* Support Pandas API layer on PySpark([SPARK-34849](https:\/\/issues.apache.org\/jira\/browse\/SPARK-34849))\n* EventTime based sessionization (session window)([SPARK-10816](https:\/\/issues.apache.org\/jira\/browse\/SPARK-10816))\n* Support ANSI SQL INTERVAL types([SPARK-27790](https:\/\/issues.apache.org\/jira\/browse\/SPARK-27790))\n* ANSI mode GA([SPARK-35030](https:\/\/issues.apache.org\/jira\/browse\/SPARK-35030))\n* Standardize exception messages in Spark ([SPARK-33539](https:\/\/issues.apache.org\/jira\/browse\/SPARK-33539))\n\n### [Core and Spark SQL](#id10)\n\n#### ANSI SQL compatibility enhancements\n\n* Support ANSI SQL INTERVAL types([SPARK-27790](https:\/\/issues.apache.org\/jira\/browse\/SPARK-27790))\n* New type coercion syntax rules in ANSI mode([SPARK-34246](https:\/\/issues.apache.org\/jira\/browse\/SPARK-34246))\n\n#### Performance enhancements\n\n* Query optimization\n\n\t+ Remove redundant aggregates in the Optimizer([SPARK-33122](https:\/\/issues.apache.org\/jira\/browse\/SPARK-33122))\n\t+ Push down limit through Project with Join([SPARK-34622](https:\/\/issues.apache.org\/jira\/browse\/SPARK-34622))\n\t+ Cardinality estimation of union, sort and range operator ([SPARK-33411](https:\/\/issues.apache.org\/jira\/browse\/SPARK-33411))\n\t+ UnwrapCastInBinaryComparison support In\/InSet predicate([SPARK-35316](https:\/\/issues.apache.org\/jira\/browse\/SPARK-35316))\n\t+ Keep necessary stats after partition pruning([SPARK-34119](https:\/\/issues.apache.org\/jira\/browse\/SPARK-34119))\n* Query execution\n\n\t+ Enable Zstandard buffer pool by default([SPARK-34340](https:\/\/issues.apache.org\/jira\/browse\/SPARK-34340), [SPARK-34390](https:\/\/issues.apache.org\/jira\/browse\/SPARK-34390))\n\t+ Add code-gen for all join types of sort merge join([SPARK-34705](https:\/\/issues.apache.org\/jira\/browse\/SPARK-34705))\n\t+ Broadcast nested loop join improvement([SPARK-34706](https:\/\/issues.apache.org\/jira\/browse\/SPARK-34706))\n\t+ Support two levels of hash maps for final hash aggregation ([SPARK-35141](https:\/\/issues.apache.org\/jira\/browse\/SPARK-35141))\n\t+ Allow concurrent writers for writing dynamic partitions and bucket table([SPARK-26164](https:\/\/issues.apache.org\/jira\/browse\/SPARK-26164))\n\t+ Improve performance of processing FETCH\\_PRIOR in Thriftserver([SPARK-33655](https:\/\/issues.apache.org\/jira\/browse\/SPARK-33655))\n\n#### Connector enhancements\n\n* Parquet\n\n\t+ Upgrade Parquet to 1.12.1([SPARK-36726](https:\/\/issues.apache.org\/jira\/browse\/SPARK-36726))\n\t+ Read parquet unsigned types that are stored as int32 physical type in parquet([SPARK-34817](https:\/\/issues.apache.org\/jira\/browse\/SPARK-34817))\n\t+ Read Parquet unsigned int64 logical type that stored as signed int64 physical type to decimal(20, 0)([SPARK-34786](https:\/\/issues.apache.org\/jira\/browse\/SPARK-34786))\n\t+ Improve Parquet In filter pushdown([SPARK-32792](https:\/\/issues.apache.org\/jira\/browse\/SPARK-32792))\n* ORC"},{"doc_uri":"https:\/\/docs.databricks.com\/archive\/runtime-release-notes\/cluster-images\/2.0.2-db4.html","content":"Updated May 28, 2024\n \n\n[Send us feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n\n* [Documentation](..\/..\/..\/index.html)\n* [Databricks documentation archive](..\/..\/index.html)\n* [Unsupported Databricks Runtime release notes](..\/index.html)\n* 2.0.2-db4 cluster image (unsupported)\n* \n\n# 2.0.2-db4 cluster image (unsupported)\n\nDatabricks released this image in late March, 2017.\n\nImportant\n\nThis release was deprecated on April 30, 2018. For more information about the Databricks Runtime deprecation policy and schedule, see [Databricks runtime support lifecycles](..\/..\/..\/release-notes\/runtime\/databricks-runtime-ver.html).\n\nThe following release notes provide information about the 2.0.2-db4 cluster image powered by Apache Spark.\n\n## Changes and Improvements\n\n* Improve the speed of installing SparkR during cluster setup.\n* Disallow dbutils.fs.mv to move a directory to the sub-directory of this directory.\n* Fix DBFS FUSE mount in Python 3 clusters.\n* Fix regression when calling .toDF() and .toDS() on dbutils.fs responses.\n* Bug fixes and stability improvements to Spark.\n* `setuptools` upgraded from 28.8.0 to 32.3.1.\n* `docutils` upgraded from 0.12 to 0.13.1.\n\n## Apache Spark\n\nThe 2.0.2-db4 cluster image includes Apache Spark 2.0.2. In addition to 2.0.2-db3, the 2.0.2-db4 cluster image also includes the following extra bug fixes and improvements made to Spark:"}]}
{"request_id":"id-1233","request":"1. \"How can I permanently delete cluster logs in my Databricks account?\"\n\n(Note: This question is based on the \"Cluster log purge\" feature released in June 2018. The user might be a developer, data engineer, or devops professional looking to manage cluster logs more efficiently.)","response":"To permanently delete cluster logs in your Databricks account, you can use the `databricks clusters permanent-delete` command with the `--cluster-id` option. Here's an example:\n```python\ndatabricks clusters permanent-delete --cluster-id 1234-567890-batch123\n```\nIf successful, no output is displayed. Note that once purged, cluster logs are not recoverable.\n\nFor more information on managing clusters in Databricks, you can refer to the [Databricks Clusters CLI (legacy) documentation](https:\/\/docs.databricks.com\/dev-tools\/cli\/clusters-cli.html). However, please note that this documentation is for legacy Databricks CLI versions 0.18 and below, and Databricks recommends using newer Databricks CLI version 0.205 or above instead.","expected_retrieved_context":[{"doc_uri":"https:\/\/docs.databricks.com\/release-notes\/product\/2018\/june.html","content":"June 2018  \nThese features and Databricks platform improvements were released in June 2018.  \nNote  \nReleases are staged. Your Databricks account may not be updated until a week after the initial release date.  \nCluster log purge\nCluster log purge\nJune 19-26, 2018: Version 2.74  \nBy default, cluster logs are retained for 30 days. You can now delete them permanently and immediately by going to the Workspace Storage tab on the Admin Console. See Purge workspace storage.\n\nTrash folder\nTrash folder\nJune 7-14, 2018: Version 2.73  \nA new Trash folder contains all notebooks, libraries, and folders that you have deleted. The Trash folder is automatically purged after 30 days. You can restore a deleted object by dragging it out of the Trash folder into another folder.  \nFor details, see Delete an object.\n\nReduced log retention period\nReduced log retention period\nJune 7-14, 2018: Version 2.73  \nCluster logs for terminated clusters are now retained for 30 days. They used to be retained indefinitely.\n\nGzipped API responses\nGzipped API responses\nJune 7-14, 2018: Version 2.73  \nRequests sent with the Accept-Encoding: gzip header return gzipped responses.\n\nTable import UI\nTable import UI\nJune 7-14, 2018: Version 2.73  \nThe create table UI now supports an option to infer the schema of CSV files:"}],"expected_response":"1. To permanently delete cluster logs in your Databricks account, you can use the Cluster Log Purge feature. This feature allows you to delete cluster logs permanently and immediately. To do this, go to the Workspace Storage tab on the Admin Console and follow the instructions to purge workspace storage. This feature was released in June 2018, and by default, cluster logs are retained for 30 days.","retrieved_context":[{"doc_uri":"https:\/\/docs.databricks.com\/release-notes\/product\/2018\/june.html","content":"June 2018 | Databricks on AWS\n\n* [![Databricks](..\/..\/..\/_static\/small-scale-lockup-full-color-rgb.svg)](https:\/\/www.databricks.com\/)\n\n* [![Databricks](..\/..\/..\/_static\/small-scale-lockup-full-color-rgb.svg)](https:\/\/www.databricks.com\/)\n* [Help Center](https:\/\/help.databricks.com\/s\/)\n* [Documentation](https:\/\/docs.databricks.com\/en\/index.html)\n* [Knowledge Base](https:\/\/kb.databricks.com\/)\n\n* [Community](https:\/\/community.databricks.com)\n* [Support](https:\/\/help.databricks.com)\n* [Feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n* [Try Databricks](https:\/\/databricks.com\/try-databricks)\n\n[![](..\/..\/..\/_static\/icons\/globe.png)English](javascript:void(0))\n* [English](..\/..\/..\/..\/en\/release-notes\/product\/2018\/june.html)\n* [\u65e5\u672c\u8a9e](..\/..\/..\/..\/ja\/release-notes\/product\/2018\/june.html)\n* [Portugu\u00eas](..\/..\/..\/..\/pt\/release-notes\/product\/2018\/june.html)\n\n[![](..\/..\/..\/_static\/icons\/aws.svg)Amazon\n Web Services](javascript:void(0))\n* [![](..\/..\/..\/_static\/icons\/azure.svg)Microsoft Azure](https:\/\/learn.microsoft.com\/azure\/databricks\/release-notes\/product\/2018\/june)\n* [![](..\/..\/..\/_static\/icons\/gcp.svg)Google Cloud Platform](https:\/\/docs.gcp.databricks.com\/release-notes\/product\/2018\/june.html)\n\n[Databricks on AWS](..\/..\/..\/index.html)\nGet started\n\n* [Get started](..\/..\/..\/getting-started\/index.html)\n* [What is Databricks?](..\/..\/..\/introduction\/index.html)\n* [DatabricksIQ](..\/..\/..\/databricksiq\/index.html)\n* [Release notes](..\/..\/index.html)\n\t+ [Databricks platform release notes](..\/index.html)\n\t\t- [May 2024](..\/2024\/may.html)\n\t\t- [April 2024](..\/2024\/april.html)\n\t\t- [March 2024](..\/2024\/march.html)\n\t\t- [February 2024](..\/2024\/february.html)\n\t\t- [January 2024](..\/2024\/january.html)\n\t\t- [December 2023](..\/2023\/december.html)\n\t\t- [November 2023](..\/2023\/november.html)\n\t\t- [October 2023](..\/2023\/october.html)\n\t\t- [September 2023](..\/2023\/september.html)\n\t\t- [August 2023](..\/2023\/august.html)\n\t\t- [July 2023](..\/2023\/july.html)\n\t\t- [June 2023](..\/2023\/june.html)\n\t\t- [May 2023](..\/2023\/may.html)\n\t\t- [April 2023](..\/2023\/april.html)\n\t\t- [March 2023](..\/2023\/march.html)\n\t\t- [February 2023](..\/2023\/february.html)\n\t\t- [January 2023](..\/2023\/january.html)\n\t\t- [December 2022](..\/2022\/december.html)\n\t\t- [November 2022](..\/2022\/november.html)\n\t\t- [October 2022](..\/2022\/october.html)\n\t\t- [September 2022](..\/2022\/september.html)\n\t\t- [August 2022](..\/2022\/august.html)\n\t\t- [July 2022](..\/2022\/july.html)\n\t\t- [June 2022](..\/2022\/june.html)\n\t\t- [May 2022](..\/2022\/may.html)\n\t\t- [April 2022](..\/2022\/april.html)\n\t\t- [March 2022](..\/2022\/march.html)\n\t\t- [February 2022](..\/2022\/february.html)\n\t\t- [January 2022](..\/2022\/january.html)\n\t\t- [December 2021](..\/2021\/december.html)\n\t\t- [November 2021](..\/2021\/november.html)\n\t\t- [October 2021](..\/2021\/october.html)\n\t\t- [September 2021](..\/2021\/september.html)\n\t\t- [August 2021](..\/2021\/august.html)\n\t\t- [July 2021](..\/2021\/july.html)\n\t\t- [June 2021](..\/2021\/june.html)\n\t\t- [May 2021](..\/2021\/may.html)\n\t\t- [April 2021](..\/2021\/april.html)\n\t\t- [March 2021](..\/2021\/march.html)\n\t\t- [February 2021](..\/2021\/february.html)\n\t\t- [January 2021](..\/2021\/january.html)\n\t\t- [December 2020](..\/2020\/december.html)\n\t\t- [November 2020](..\/2020\/november.html)\n\t\t- [October 2020](..\/2020\/october.html)\n\t\t- [September 2020](..\/2020\/september.html)\n\t\t- [August 2020](..\/2020\/august.html)\n\t\t- [July 2020](..\/2020\/july.html)\n\t\t- [June 2020](..\/2020\/june.html)\n\t\t- [May 2020](..\/2020\/may.html)\n\t\t- [April 2020](..\/2020\/april.html)\n\t\t- [March 2020](..\/2020\/march.html)\n\t\t- [February 2020](..\/2020\/february.html)\n\t\t- [January 2020](..\/2020\/january.html)\n\t\t- [December 2019](..\/2019\/december.html)\n\t\t- [November 2019](..\/2019\/november.html)\n\t\t- [October 2019](..\/2019\/october.html)\n\t\t- [September 2019](..\/2019\/september.html)\n\t\t- [August 2019](..\/2019\/august.html)\n\t\t- [July 2019](..\/2019\/july.html)\n\t\t- [June 2019](..\/2019\/june.html)\n\t\t- [May 2019](..\/2019\/may.html)\n\t\t- [April 2019](..\/2019\/april.html)\n\t\t- [March 2019](..\/2019\/march.html)\n\t\t- [February 2019](..\/2019\/february.html)\n\t\t- [January 2019](..\/2019\/january.html)\n\t\t- [December 2018](december.html)\n\t\t- [November 2018](november.html)\n\t\t- [October 2018](october.html)\n\t\t- [September 2018](september.html)\n\t\t- [August 2018](august.html)\n\t\t- [July 2018](july.html)\n\t\t- [June 2018](#)\n\t\t- [May 2018](may.html)\n\t\t- [April 2018](april.html)\n\t\t- [March 2018](march.html)\n\t\t- [February 2018](february.html)\n\t\t- [January 2018](january.html)\n\t+ [Databricks Runtime release notes versions and compatibility](..\/..\/runtime\/index.html)\n\t+ [Databricks SQL release notes](..\/..\/..\/sql\/release-notes\/index.html)\n\t+ [Serverless compute release notes](..\/..\/serverless.html)\n\t+ [Databricks developer tools and SDKs release notes](..\/..\/dev-tools\/index.html)\n\t+ [Databricks Connect release notes](..\/..\/dbconnect\/index.html)\n\t+ [Delta Live Tables release notes](..\/..\/delta-live-tables\/index.html)\n\t+ [Databricks Previews support & details](..\/..\/release-types.html)\n\t+ [Migrate from legacy features](..\/..\/migrations.html)\n\nLoad & manage data\n\n* [Connect to data sources](..\/..\/..\/connect\/index.html)\n* [Connect to compute](..\/..\/..\/compute\/index.html)\n* [Discover data](..\/..\/..\/discover\/index.html)\n* [Query data](..\/..\/..\/query\/index.html)\n* [Ingest data](..\/..\/..\/ingestion\/index.html)\n* [Transform data](..\/..\/..\/transform\/index.html)\n* [Monitor data and AI assets](..\/..\/..\/lakehouse-monitoring\/index.html)\n* [Share data (Delta Sharing)](..\/..\/..\/data-sharing\/index.html)\n* [Databricks Marketplace](..\/..\/..\/marketplace\/index.html)\n\nWork with data\n\n* [Data engineering](..\/..\/..\/workspace-index.html)\n* [Generative AI & LLMs](..\/..\/..\/generative-ai\/generative-ai.html)\n* [Machine learning](..\/..\/..\/machine-learning\/index.html)\n* [Model serving](..\/..\/..\/machine-learning\/model-serving\/index.html)\n* [Data warehousing](..\/..\/..\/sql\/index.html)\n* [Delta Lake](..\/..\/..\/delta\/index.html)\n* [Developers](..\/..\/..\/languages\/index.html)\n* [Technology partners](..\/..\/..\/integrations\/index.html)\n\nAdministration\n\n* [Account and workspace administration](..\/..\/..\/admin\/index.html)\n* [Security and compliance](..\/..\/..\/security\/index.html)\n* [Data governance (Unity Catalog)](..\/..\/..\/data-governance\/index.html)\n* [Lakehouse architecture](..\/..\/..\/lakehouse-architecture\/index.html)\n\nReference & resources\n\n* [Reference](..\/..\/..\/reference\/api.html)\n* [Resources](..\/..\/..\/resources\/index.html)\n* [What\u2019s coming?](..\/..\/..\/whats-coming.html)\n* [Documentation archive](..\/..\/..\/archive\/index.html)\n\n Updated May 28, 2024\n \n\n[Send us feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n\n* [Documentation](..\/..\/..\/index.html)\n* [Databricks release notes](..\/..\/index.html)\n* [Databricks platform release notes](..\/index.html)\n* June 2018\n* \n\n# June 2018\n\nThese features and Databricks platform improvements were released in June 2018.\n\nNote\n\nReleases are staged. Your Databricks account may not be updated until a week after the initial release date.\n\n## Cluster log purge\n\n**June 19-26, 2018: Version 2.74**\n\nBy default, cluster logs are retained for 30 days. You can now delete them permanently and immediately by going to the Workspace Storage tab on the Admin Console. See [Purge workspace storage](..\/..\/..\/admin\/workspace-settings\/storage.html).\n\n## Trash folder\n\n**June 7-14, 2018: Version 2.73**\n\nA new ![Trash](..\/..\/..\/_images\/trash-icon1.png) **Trash** folder contains all notebooks, libraries, and folders that you have deleted. The Trash folder is automatically purged after 30 days. You can restore a deleted object by dragging it out of the Trash folder into another folder.\n\nFor details, see [Delete an object](..\/..\/..\/workspace\/workspace-objects.html#delete-object).\n\n## Reduced log retention period\n\n**June 7-14, 2018: Version 2.73**\n\nCluster logs for terminated clusters are now retained for 30 days. They used to be retained indefinitely.\n\n## Gzipped API responses\n\n**June 7-14, 2018: Version 2.73**\n\nRequests sent with the `Accept-Encoding: gzip` header return gzipped responses.\n\n## Table import UI\n\n**June 7-14, 2018: Version 2.73**\n\nThe [create table UI](..\/..\/..\/archive\/legacy\/data-tab.html) now supports an option to infer the schema of CSV files:\n\n![Infer table schema option](..\/..\/..\/_images\/infer-schema.png)\n\n---\n\n \u00a9 Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the [Apache Software Foundation](http:\/\/www.apache.org\/).\n \n\n[Send us feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n \n | [Privacy Policy](https:\/\/databricks.com\/privacy-policy) | [Terms of Use](https:\/\/databricks.com\/terms-of-use)\n\n { # 's' variable stands for the search page here. TODO: Fix it. #}\n \n [![DATA+AU Summit 2024](..\/..\/..\/_static\/images\/dais\/docs-dais-promo-image.png)](https:\/\/www.databricks.com\/dataaisummit?utm_source=databricks&utm_medium=web&utm_campaign=7018y000001f7cvqai&utm_offer=docs \"DATA+AU Summit 2024\")"},{"doc_uri":"https:\/\/docs.databricks.com\/release-notes\/product\/2018\/may.html","content":"May 2018 | Databricks on AWS\n\n* [![Databricks](..\/..\/..\/_static\/small-scale-lockup-full-color-rgb.svg)](https:\/\/www.databricks.com\/)\n\n* [![Databricks](..\/..\/..\/_static\/small-scale-lockup-full-color-rgb.svg)](https:\/\/www.databricks.com\/)\n* [Help Center](https:\/\/help.databricks.com\/s\/)\n* [Documentation](https:\/\/docs.databricks.com\/en\/index.html)\n* [Knowledge Base](https:\/\/kb.databricks.com\/)\n\n* [Community](https:\/\/community.databricks.com)\n* [Support](https:\/\/help.databricks.com)\n* [Feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n* [Try Databricks](https:\/\/databricks.com\/try-databricks)\n\n[![](..\/..\/..\/_static\/icons\/globe.png)English](javascript:void(0))\n* [English](..\/..\/..\/..\/en\/release-notes\/product\/2018\/may.html)\n* [\u65e5\u672c\u8a9e](..\/..\/..\/..\/ja\/release-notes\/product\/2018\/may.html)\n* [Portugu\u00eas](..\/..\/..\/..\/pt\/release-notes\/product\/2018\/may.html)\n\n[![](..\/..\/..\/_static\/icons\/aws.svg)Amazon\n Web Services](javascript:void(0))\n* [![](..\/..\/..\/_static\/icons\/azure.svg)Microsoft Azure](https:\/\/learn.microsoft.com\/azure\/databricks\/release-notes\/product\/2018\/may)\n* [![](..\/..\/..\/_static\/icons\/gcp.svg)Google Cloud Platform](https:\/\/docs.gcp.databricks.com\/release-notes\/product\/2018\/may.html)\n\n[Databricks on AWS](..\/..\/..\/index.html)\nGet started\n\n* [Get started](..\/..\/..\/getting-started\/index.html)\n* [What is Databricks?](..\/..\/..\/introduction\/index.html)\n* [DatabricksIQ](..\/..\/..\/databricksiq\/index.html)\n* [Release notes](..\/..\/index.html)\n\t+ [Databricks platform release notes](..\/index.html)\n\t\t- [May 2024](..\/2024\/may.html)\n\t\t- [April 2024](..\/2024\/april.html)\n\t\t- [March 2024](..\/2024\/march.html)\n\t\t- [February 2024](..\/2024\/february.html)\n\t\t- [January 2024](..\/2024\/january.html)\n\t\t- [December 2023](..\/2023\/december.html)\n\t\t- [November 2023](..\/2023\/november.html)\n\t\t- [October 2023](..\/2023\/october.html)\n\t\t- [September 2023](..\/2023\/september.html)\n\t\t- [August 2023](..\/2023\/august.html)\n\t\t- [July 2023](..\/2023\/july.html)\n\t\t- [June 2023](..\/2023\/june.html)\n\t\t- [May 2023](..\/2023\/may.html)\n\t\t- [April 2023](..\/2023\/april.html)\n\t\t- [March 2023](..\/2023\/march.html)\n\t\t- [February 2023](..\/2023\/february.html)\n\t\t- [January 2023](..\/2023\/january.html)\n\t\t- [December 2022](..\/2022\/december.html)\n\t\t- [November 2022](..\/2022\/november.html)\n\t\t- [October 2022](..\/2022\/october.html)\n\t\t- [September 2022](..\/2022\/september.html)\n\t\t- [August 2022](..\/2022\/august.html)\n\t\t- [July 2022](..\/2022\/july.html)\n\t\t- [June 2022](..\/2022\/june.html)\n\t\t- [May 2022](..\/2022\/may.html)\n\t\t- [April 2022](..\/2022\/april.html)\n\t\t- [March 2022](..\/2022\/march.html)\n\t\t- [February 2022](..\/2022\/february.html)\n\t\t- [January 2022](..\/2022\/january.html)\n\t\t- [December 2021](..\/2021\/december.html)\n\t\t- [November 2021](..\/2021\/november.html)\n\t\t- [October 2021](..\/2021\/october.html)\n\t\t- [September 2021](..\/2021\/september.html)\n\t\t- [August 2021](..\/2021\/august.html)\n\t\t- [July 2021](..\/2021\/july.html)\n\t\t- [June 2021](..\/2021\/june.html)\n\t\t- [May 2021](..\/2021\/may.html)\n\t\t- [April 2021](..\/2021\/april.html)\n\t\t- [March 2021](..\/2021\/march.html)\n\t\t- [February 2021](..\/2021\/february.html)\n\t\t- [January 2021](..\/2021\/january.html)\n\t\t- [December 2020](..\/2020\/december.html)\n\t\t- [November 2020](..\/2020\/november.html)\n\t\t- [October 2020](..\/2020\/october.html)\n\t\t- [September 2020](..\/2020\/september.html)\n\t\t- [August 2020](..\/2020\/august.html)\n\t\t- [July 2020](..\/2020\/july.html)\n\t\t- [June 2020](..\/2020\/june.html)\n\t\t- [May 2020](..\/2020\/may.html)\n\t\t- [April 2020](..\/2020\/april.html)\n\t\t- [March 2020](..\/2020\/march.html)\n\t\t- [February 2020](..\/2020\/february.html)\n\t\t- [January 2020](..\/2020\/january.html)\n\t\t- [December 2019](..\/2019\/december.html)\n\t\t- [November 2019](..\/2019\/november.html)\n\t\t- [October 2019](..\/2019\/october.html)\n\t\t- [September 2019](..\/2019\/september.html)\n\t\t- [August 2019](..\/2019\/august.html)\n\t\t- [July 2019](..\/2019\/july.html)\n\t\t- [June 2019](..\/2019\/june.html)\n\t\t- [May 2019](..\/2019\/may.html)\n\t\t- [April 2019](..\/2019\/april.html)\n\t\t- [March 2019](..\/2019\/march.html)\n\t\t- [February 2019](..\/2019\/february.html)\n\t\t- [January 2019](..\/2019\/january.html)\n\t\t- [December 2018](december.html)\n\t\t- [November 2018](november.html)\n\t\t- [October 2018](october.html)\n\t\t- [September 2018](september.html)\n\t\t- [August 2018](august.html)\n\t\t- [July 2018](july.html)\n\t\t- [June 2018](june.html)\n\t\t- [May 2018](#)\n\t\t- [April 2018](april.html)\n\t\t- [March 2018](march.html)\n\t\t- [February 2018](february.html)\n\t\t- [January 2018](january.html)\n\t+ [Databricks Runtime release notes versions and compatibility](..\/..\/runtime\/index.html)\n\t+ [Databricks SQL release notes](..\/..\/..\/sql\/release-notes\/index.html)\n\t+ [Serverless compute release notes](..\/..\/serverless.html)\n\t+ [Databricks developer tools and SDKs release notes](..\/..\/dev-tools\/index.html)\n\t+ [Databricks Connect release notes](..\/..\/dbconnect\/index.html)\n\t+ [Delta Live Tables release notes](..\/..\/delta-live-tables\/index.html)\n\t+ [Databricks Previews support & details](..\/..\/release-types.html)\n\t+ [Migrate from legacy features](..\/..\/migrations.html)\n\nLoad & manage data\n\n* [Connect to data sources](..\/..\/..\/connect\/index.html)\n* [Connect to compute](..\/..\/..\/compute\/index.html)\n* [Discover data](..\/..\/..\/discover\/index.html)\n* [Query data](..\/..\/..\/query\/index.html)\n* [Ingest data](..\/..\/..\/ingestion\/index.html)\n* [Transform data](..\/..\/..\/transform\/index.html)\n* [Monitor data and AI assets](..\/..\/..\/lakehouse-monitoring\/index.html)\n* [Share data (Delta Sharing)](..\/..\/..\/data-sharing\/index.html)\n* [Databricks Marketplace](..\/..\/..\/marketplace\/index.html)\n\nWork with data\n\n* [Data engineering](..\/..\/..\/workspace-index.html)\n* [Generative AI & LLMs](..\/..\/..\/generative-ai\/generative-ai.html)\n* [Machine learning](..\/..\/..\/machine-learning\/index.html)\n* [Model serving](..\/..\/..\/machine-learning\/model-serving\/index.html)\n* [Data warehousing](..\/..\/..\/sql\/index.html)\n* [Delta Lake](..\/..\/..\/delta\/index.html)\n* [Developers](..\/..\/..\/languages\/index.html)\n* [Technology partners](..\/..\/..\/integrations\/index.html)\n\nAdministration\n\n* [Account and workspace administration](..\/..\/..\/admin\/index.html)\n* [Security and compliance](..\/..\/..\/security\/index.html)\n* [Data governance (Unity Catalog)](..\/..\/..\/data-governance\/index.html)\n* [Lakehouse architecture](..\/..\/..\/lakehouse-architecture\/index.html)\n\nReference & resources\n\n* [Reference](..\/..\/..\/reference\/api.html)\n* [Resources](..\/..\/..\/resources\/index.html)\n* [What\u2019s coming?](..\/..\/..\/whats-coming.html)\n* [Documentation archive](..\/..\/..\/archive\/index.html)\n\n Updated May 28, 2024\n \n\n[Send us feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n\n* [Documentation](..\/..\/..\/index.html)\n* [Databricks release notes](..\/..\/index.html)\n* [Databricks platform release notes](..\/index.html)\n* May 2018\n* \n\n# May 2018\n\nReleases are staged. Your Databricks account may not be updated until a week after the initial\nrelease date.\n\n## General Data Protection Regulation (GDPR)\n\n**May 22-31, 2018: Version 2.72**\n\nTo meet the requirements of the European Union General Data Protection Regulation (GDPR), which goes into effect on May 25, 2018, we have made a number of modifications to the Databricks platform to provide you with more control of data retention at both the account and user level. Updates include:\n\n* Cluster delete: permanently delete a cluster configuration using the UI or the Clusters API. See [Delete a compute](..\/..\/..\/compute\/clusters-manage.html#cluster-delete).\n* Workspace purge (released in version 2.71): permanently delete workspace objects, such as entire notebooks, individual notebook cells, individual notebook comments, and notebook revision history. See [Purge workspace storage](..\/..\/..\/admin\/workspace-settings\/storage.html).\n* Notebook revision history purge:\n\n\t+ Permanently delete the revision history of all notebooks in a workspace for a defined time frame. See [Purge workspace storage](..\/..\/..\/admin\/workspace-settings\/storage.html).\n\t+ Permanently delete a single notebook revision or the entire revision history of a notebook. See [Version history](..\/..\/..\/notebooks\/notebooks-code.html#version-control).\n\nAccount management features allow you to cancel subscriptions and delete your account:\n\n* Cancel your Databricks subscription. By default, permanent data purge happens 30 days after you cancel a workspace subscription.\n* Cancel any Community Edition subscription associated with your account owner username, separately from canceling your free trial or paid standard Databricks subscription.\n* Delete your Databricks account, including your login credentials and billing information.\n\nFor details, see [Manage your subscription](..\/..\/..\/admin\/account-settings\/account.html).\n\n## HorovodEstimator\n\n**May 29, 2018: Version 2.72**\n\nAdded documentation and a notebook for HorovodEstimator, an MLlib-style estimator API that leverages Uber\u2019s [Horovod](https:\/\/github.com\/horovod\/horovod) framework. HorovodEstimator facilitates distributed, multi-GPU training of deep neural networks on Spark DataFrames, simplifying the integration of ETL in Spark with model training in TensorFlow.\n\n## MLeap ML Model Export\n\n**May 22-31, 2018: Version 2.72**\n\nAdded documentation and notebooks on using MLeap on Databricks. MLeap allows you to deploy machine learning pipelines from Apache Spark and scikit-learn to a portable format and execution engine. See [MLeap ML model export](..\/..\/..\/archive\/model-export\/mleap-model-export.html).\n\n## Notebook cells: hide and show\n\n**May 22-31, 2018: Version 2.72**\n\nNew indicators and messaging make it easier to show Notebook cell contents after they\u2019ve been hidden. See [Hide and show cell content](..\/..\/..\/notebooks\/notebook-ui.html#hide-show-cell).\n\n## Doc site search\n\n**May 22, 2018**\n\nWe have replaced our doc site search with a better search tool. You\u2019ll see even more search improvements over the coming weeks.\n\nNote\n\nSearch may look broken if you try it shortly after the new search is deployed. Just clear your browser cache to see the new search experience.\n\n## Databricks Runtime 4.1 for Machine Learning (Beta)\n\n**May 18, 2018**\n\nDatabricks Runtime ML (Beta) provides a ready-to-go environment for machine learning and data science. It contains multiple popular libraries, including TensorFlow, Keras, and XGBoost. It also supports distributed TensorFlow training using Horovod.\n\nDatabricks Runtime ML lets you start a Databricks cluster with all of the libraries required for distributed TensorFlow training. It ensures the compatibility of the libraries included on the cluster (between TensorFlow and CUDA \/ cuDNN, for example) and substantially decreases the cluster start-up time compared to using init scripts.\n\nNote\n\nUpon GA, Databricks Runtime 4.1 ML will require a Databricks plan that includes the Databricks Operational Security package. Databricks Runtime 4.1 ML is currently available on standard plans without the Operational Security package. It is not available on Community Edition accounts. Note also that the Operational Security requirement may be enforced at any point during the beta period, before GA. If so, we will communicate the change in advance."},{"doc_uri":"https:\/\/docs.databricks.com\/admin\/workspace-settings\/storage.html","content":"Purge workspace storage | Databricks on AWS\n\n* [![Databricks](..\/..\/_static\/small-scale-lockup-full-color-rgb.svg)](https:\/\/www.databricks.com\/)\n\n* [![Databricks](..\/..\/_static\/small-scale-lockup-full-color-rgb.svg)](https:\/\/www.databricks.com\/)\n* [Help Center](https:\/\/help.databricks.com\/s\/)\n* [Documentation](https:\/\/docs.databricks.com\/en\/index.html)\n* [Knowledge Base](https:\/\/kb.databricks.com\/)\n\n* [Community](https:\/\/community.databricks.com)\n* [Support](https:\/\/help.databricks.com)\n* [Feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n* [Try Databricks](https:\/\/databricks.com\/try-databricks)\n\n[![](..\/..\/_static\/icons\/globe.png)English](javascript:void(0))\n* [English](..\/..\/..\/en\/admin\/workspace-settings\/storage.html)\n* [\u65e5\u672c\u8a9e](..\/..\/..\/ja\/admin\/workspace-settings\/storage.html)\n* [Portugu\u00eas](..\/..\/..\/pt\/admin\/workspace-settings\/storage.html)\n\n[![](..\/..\/_static\/icons\/aws.svg)Amazon\n Web Services](javascript:void(0))\n* [![](..\/..\/_static\/icons\/azure.svg)Microsoft Azure](https:\/\/learn.microsoft.com\/azure\/databricks\/admin\/workspace-settings\/storage)\n* [![](..\/..\/_static\/icons\/gcp.svg)Google Cloud Platform](https:\/\/docs.gcp.databricks.com\/admin\/workspace-settings\/storage.html)\n\n[Databricks on AWS](..\/..\/index.html)\nGet started\n\n* [Get started](..\/..\/getting-started\/index.html)\n* [What is Databricks?](..\/..\/introduction\/index.html)\n* [DatabricksIQ](..\/..\/databricksiq\/index.html)\n* [Release notes](..\/..\/release-notes\/index.html)\n\nLoad & manage data\n\n* [Connect to data sources](..\/..\/connect\/index.html)\n* [Connect to compute](..\/..\/compute\/index.html)\n* [Discover data](..\/..\/discover\/index.html)\n* [Query data](..\/..\/query\/index.html)\n* [Ingest data](..\/..\/ingestion\/index.html)\n* [Transform data](..\/..\/transform\/index.html)\n* [Monitor data and AI assets](..\/..\/lakehouse-monitoring\/index.html)\n* [Share data (Delta Sharing)](..\/..\/data-sharing\/index.html)\n* [Databricks Marketplace](..\/..\/marketplace\/index.html)\n\nWork with data\n\n* [Data engineering](..\/..\/workspace-index.html)\n* [Generative AI & LLMs](..\/..\/generative-ai\/generative-ai.html)\n* [Machine learning](..\/..\/machine-learning\/index.html)\n* [Model serving](..\/..\/machine-learning\/model-serving\/index.html)\n* [Data warehousing](..\/..\/sql\/index.html)\n* [Delta Lake](..\/..\/delta\/index.html)\n* [Developers](..\/..\/languages\/index.html)\n* [Technology partners](..\/..\/integrations\/index.html)\n\nAdministration\n\n* [Account and workspace administration](..\/index.html)\n\t+ [Get started with Databricks administration](..\/..\/getting-started\/admin-get-started.html)\n\t+ [Account administration](..\/account-settings\/index.html)\n\t+ [Workspace deployment](..\/workspace\/index.html)\n\t+ [Manage a workspace](index.html)\n\t\t- [SQL warehouse admin settings](..\/sql\/index.html)\n\t\t- [Enable serverless compute public preview](serverless.html)\n\t\t- [Manage Databricks Previews](manage-previews.html)\n\t\t- [Manage workspace appearance settings](appearance.html)\n\t\t- [Manage workspace email settings](email.html)\n\t\t- [Manage notification destinations](notification-destinations.html)\n\t\t- [Auto-enable deletion vectors](deletion-vectors.html)\n\t\t- [Enable the web terminal](..\/clusters\/web-terminal.html)\n\t\t- [Purge workspace storage](#)\n\t\t- [Configure notebook result storage location](notebook-results.html)\n\t\t- [Manage third-party analytics tools](usage-analytics-tools.html)\n\t\t- [Manage access to notebook features](notebooks.html)\n\t\t- [Disable the upload data UI](disable-upload-data-ui.html)\n\t\t- [Manage the DBFS file browser](dbfs-browser.html)\n\t\t- [Enforce user isolation cluster types on a workspace](enforce-user-isolation.html)\n\t\t- [Manage SSD storage](..\/clusters\/manage-ssd.html)\n\t\t- [Workspace access for Databricks personnel](..\/workspace\/workspace-access.html)\n\t\t- [Enforce AWS Instance Metadata Service v2 on a workspace](..\/cloud-configurations\/aws\/imdsv2.html)\n\t\t- [Manage instance profiles in Databricks](manage-instance-profiles.html)\n\t\t- [Restrict workspace admins](restrict-workspace-admins.html)\n\t+ [Identity management](..\/users-groups\/index.html)\n\t+ [Compute policies](..\/clusters\/policies.html)\n\t+ [Audit logs](..\/account-settings\/audit-logs.html)\n\t+ [System tables](..\/system-tables\/index.html)\n* [Security and compliance](..\/..\/security\/index.html)\n* [Data governance (Unity Catalog)](..\/..\/data-governance\/index.html)\n* [Lakehouse architecture](..\/..\/lakehouse-architecture\/index.html)\n\nReference & resources\n\n* [Reference](..\/..\/reference\/api.html)\n* [Resources](..\/..\/resources\/index.html)\n* [What\u2019s coming?](..\/..\/whats-coming.html)\n* [Documentation archive](..\/..\/archive\/index.html)\n\n Updated May 28, 2024\n \n\n[Send us feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n\n* [Documentation](..\/..\/index.html)\n* [Databricks administration introduction](..\/index.html)\n* [Manage your workspace](index.html)\n* Purge workspace storage\n* \n\n# Purge workspace storage\n\nYour organization\u2019s privacy requirements may require that you occasionally purge deleted objects like notebook cells, entire notebooks, experiments, or cluster logs.\n\n## Purge workspace objects\n\nYou can delete workspace objects such as entire notebooks, individual notebook cells, individual\nnotebook comments, and experiments, but they are recoverable.\n\nTo permanently purge deleted workspace objects:\n\n1. Go to the [settings page](..\/index.html#admin-settings).\n2. Click the **Advanced** tab.\n3. In the **Storage** section, click the **Purge** button next to **Permanently purge workspace storage**.\n4. Click the **Purge** button.\n5. Click **Yes, purge** to confirm.\n\nWarning\n\nOnce purged, workspace objects are not recoverable.\n\n## Purge notebook revision history\n\nTo permanently purge notebook revision history:\n\n1. Go to the [settings page](..\/index.html#admin-settings).\n2. Click the **Advanced** tab.\n3. In the **Storage** section, next to **Permanently purge all revision history**, select the timeframe to purge. The default is **24 hours and older**.\n4. Next to the timeframe, click the **Purge** button.\n5. Click the **Purge** button.\n6. Click **Yes, purge** to confirm.\n\nWarning\n\nOnce purged, version history is not recoverable.\n\n## Purge cluster logs\n\nTo permanently purge Spark driver logs and historical metrics snapshots for all clusters in the workspace:\n\n1. Go to the [settings page](..\/index.html#admin-settings).\n2. Click the **Advanced** tab.\n3. Next to **Permanently purge cluster logs**, click the **Purge** button.\n4. Click **Yes, purge** to confirm.\n\nWarning\n\nOnce purged, cluster logs are not recoverable.\n\n---\n\n \u00a9 Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the [Apache Software Foundation](http:\/\/www.apache.org\/).\n \n\n[Send us feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n \n | [Privacy Policy](https:\/\/databricks.com\/privacy-policy) | [Terms of Use](https:\/\/databricks.com\/terms-of-use)\n\n { # 's' variable stands for the search page here. TODO: Fix it. #}\n \n [![DATA+AU Summit 2024](..\/..\/_static\/images\/dais\/docs-dais-promo-image.png)](https:\/\/www.databricks.com\/dataaisummit?utm_source=databricks&utm_medium=web&utm_campaign=7018y000001f7cvqai&utm_offer=docs \"DATA+AU Summit 2024\")"},{"doc_uri":"https:\/\/docs.databricks.com\/archive\/dev-tools\/cli\/clusters-cli.html","content":"Updated May 28, 2024\n \n\n[Send us feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n\n* [Documentation](..\/..\/..\/index.html)\n* [Databricks documentation archive](..\/..\/index.html)\n* [Databricks CLI (legacy)](index.html)\n* Clusters CLI (legacy)\n* \n\n# Clusters CLI (legacy)\n\nImportant\n\nThis documentation has been retired and might not be updated.\n\nThis information applies to legacy Databricks CLI versions 0.18 and below. Databricks recommends that you use newer Databricks CLI version 0.205 or above instead. See [What is the Databricks CLI?](..\/..\/..\/dev-tools\/cli\/index.html). To find your version of the Databricks CLI, run `databricks -v`.\n\nTo migrate from Databricks CLI version 0.18 or below to Databricks CLI version 0.205 or above, see [Databricks CLI migration](..\/..\/..\/dev-tools\/cli\/migrate.html).\n\nYou run Databricks clusters CLI subcommands by appending them to `databricks clusters`. These subcommands call the [Clusters API](https:\/\/docs.databricks.com\/api\/workspace\/clusters).\n\n```\ndatabricks clusters -h\n\n```\n\n```\nUsage: databricks clusters [OPTIONS] COMMAND [ARGS]...\n\n  Utility to interact with Databricks clusters.\n\nOptions:\n  -v, --version  [VERSION]\n  -h, --help     Show this message and exit.\n\nCommands:\n  create           Creates a Databricks cluster.\n    Options:\n      --json-file PATH  File containing JSON request to POST to \/api\/2.0\/clusters\/create.\n      --json JSON       JSON string to POST to \/api\/2.0\/clusters\/create.\n  delete           Removes a Databricks cluster.\n    Options:\n      --cluster-id CLUSTER_ID Can be found in the URL at https:\/\/<databricks-instance>\/?o=<16-digit-number>#\/setting\/clusters\/$CLUSTER_ID\/configuration.\n  edit             Edits a Databricks cluster.\n    Options:\n      --json-file PATH  File containing JSON request to POST to \/api\/2.0\/clusters\/edit.\n      --json JSON       JSON string to POST to \/api\/2.0\/clusters\/edit.\n  events Gets events for a Spark cluster.\n    Options:\n      --cluster-id CLUSTER_ID  Can be found in the URL at https:\/\/<databricks-instance>\/#\/setting\/clusters\/$CLUSTER_ID\/configuration.  [required]\n      --start-time TEXT        The start time in epoch milliseconds. If\n                               unprovided, returns events starting from the\n                               beginning of time.\n      --end-time TEXT          The end time in epoch milliseconds. If unprovided,\n                               returns events up to the current time\n      --order TEXT             The order to list events in; either ASC or DESC.\n                               Defaults to DESC (most recent first).\n      --event-type TEXT        An event types to filter on (specify multiple event\n                               types by passing the --event-type option multiple\n                               times). If empty, all event types are returned.\n      --offset TEXT            The offset in the result set. Defaults to 0 (no\n                               offset). When an offset is specified and the\n                               results are requested in descending order, the\n                               end_time field is required.\n      --limit TEXT             The maximum number of events to include in a page\n                               of events. Defaults to 50, and maximum allowed\n                               value is 500.\n      --output FORMAT          can be \"JSON\" or \"TABLE\". Set to TABLE by default.\n  get              Retrieves metadata about a cluster.\n    Options:\n      --cluster-id CLUSTER_ID Can be found in the URL at https:\/\/<databricks-instance>\/?o=<16-digit-number>#\/setting\/clusters\/$CLUSTER_ID\/configuration.\n  list             Lists active and recently terminated clusters.\n    Options:\n      --output FORMAT          JSON or TABLE. Set to TABLE by default.\n  list-node-types  Lists node types for a cluster.\n  list-zones       Lists zones where clusters can be created.\n  permanent-delete Permanently deletes a cluster.\n    Options:\n      --cluster-id CLUSTER_ID  Can be found in the URL at https:\/\/<databricks-instance>\/?o=<16-digit-number>#\/setting\/clusters\/$CLUSTER_ID\/configuration.\n  resize           Resizes a Databricks cluster given its ID.\n    Options:\n      --cluster-id CLUSTER_ID  Can be found in the URL at https:\/\/<databricks-instance>\/?o=<16-digit-number>#\/setting\/clusters\/$CLUSTER_ID\/configuration.\n      --num-workers INTEGER    Number of workers. [required]\n  restart          Restarts a Databricks cluster.\n    Options:\n      --cluster-id CLUSTER_ID  Can be found in the URL at https:\/\/<databricks-instance>\/?o=<16-digit-number>#\/setting\/clusters\/$CLUSTER_ID\/configuration.\n  spark-versions   Lists possible Databricks Runtime versions.\n  start            Starts a terminated Databricks cluster.\n    Options:\n      --cluster-id CLUSTER_ID  Can be found in the URL at https:\/\/<databricks-instance>\/?o=<16-digit-number>#\/setting\/clusters\/$CLUSTER_ID\/configuration.\n\n```\n\n## Create a cluster\n\nTo display usage documentation, run `databricks clusters create --help`.\n\n```\ndatabricks clusters create --json-file create-cluster.json\n\n```\n\n`create-cluster.json`:\n\n```\n{\n  \"cluster_name\": \"my-cluster\",\n  \"spark_version\": \"7.3.x-scala2.12\",\n  \"node_type_id\": \"i3.xlarge\",\n  \"spark_conf\": {\n    \"spark.speculation\": true\n  },\n  \"aws_attributes\": {\n    \"availability\": \"SPOT\",\n    \"zone_id\": \"us-west-2a\"\n  },\n  \"num_workers\": 25\n}\n\n```\n\n```\n{\n  \"cluster_id\": \"1234-567890-batch123\"\n}\n\n```\n\n## Delete a cluster\n\nTo display usage documentation, run `databricks clusters delete --help`.\n\n```\ndatabricks clusters delete --cluster-id 1234-567890-batch123\n\n```\n\nIf successful, no output is displayed.\n\n## Change a cluster\u2019s configuration\n\nTo display usage documentation, run `databricks clusters edit --help`.\n\n```\ndatabricks clusters edit --json-file edit-cluster.json\n\n```\n\n`edit-cluster.json`:\n\n```\n{\n  \"cluster_id\": \"1234-567890-batch123\",\n  \"num_workers\": 10,\n  \"spark_version\": \"7.3.x-scala2.12\",\n  \"node_type_id\": \"i3.xlarge\"\n}\n\n```\n\nIf successful, no output is displayed.\n\n## List events for a cluster\n\nTo display usage documentation, run `databricks clusters events --help`.\n\n```\ndatabricks clusters events \\\n--cluster-id 1234-567890-batch123 \\\n--start-time 1617238800000 \\\n--end-time 1619485200000 \\\n--order DESC \\\n--limit 5 \\\n--event-type RUNNING \\\n--output JSON \\\n| jq .\n\n```\n\n```\n{\n  \"events\": [\n    {\n      \"cluster_id\": \"1234-567890-batch123\",\n      \"timestamp\": 1619214150232,\n      \"type\": \"RUNNING\",\n      \"details\": {\n        \"current_num_workers\": 2,\n        \"target_num_workers\": 2\n      }\n    },\n    ...\n    {\n      \"cluster_id\": \"1234-567890-batch123\",\n      \"timestamp\": 1617895221986,\n      \"type\": \"RUNNING\",\n      \"details\": {\n        \"current_num_workers\": 2,\n        \"target_num_workers\": 2\n      }\n    }\n  ],\n  \"next_page\": {\n    \"cluster_id\": \"1234-567890-batch123\",\n    \"start_time\": 1617238800000,\n    \"end_time\": 1619485200000,\n    \"order\": \"DESC\",\n    \"event_types\": [\n      \"RUNNING\"\n    ],\n    \"offset\": 5,\n    \"limit\": 5\n  },\n  \"total_count\": 11\n}\n\n```\n\n## Get information about a cluster\n\nTo display usage documentation, run `databricks clusters get --help`.\n\n```\ndatabricks clusters get --cluster-id 1234-567890-batch123\n\n```\n\nOr:\n\n```\ndatabricks clusters get --cluster-name my-cluster\n\n```\n\n```\n{\n  \"cluster_id\": \"1234-567890-batch123\",\n  \"spark_context_id\": 8232037838300762810,\n  \"cluster_name\": \"my-cluster\",\n  \"spark_version\": \"8.1.x-scala2.12\",\n  \"aws_attributes\": {\n    \"zone_id\": \"us-west-2c\",\n    \"first_on_demand\": 1,\n    \"availability\": \"SPOT_WITH_FALLBACK\",\n    \"spot_bid_price_percent\": 100,\n    \"ebs_volume_count\": 0\n  },\n  \"node_type_id\": \"i3.xlarge\",\n  \"driver_node_type_id\": \"i3.xlarge\",\n  \"autotermination_minutes\": 120,\n  \"enable_elastic_disk\": false,\n  \"disk_spec\": {\n    \"disk_count\": 0\n  },\n  \"cluster_source\": \"UI\",\n  \"enable_local_disk_encryption\": false,\n  \"instance_source\": {\n    \"node_type_id\": \"i3.xlarge\"\n  },\n  \"driver_instance_source\": {\n    \"node_type_id\": \"i3.xlarge\"\n  },\n  \"state\": \"TERMINATED\",\n  \"state_message\": \"Inactive cluster terminated (inactive for 120 minutes).\",\n  \"start_time\": 1616773202562,\n  \"terminated_time\": 1619228528317,\n  \"last_state_loss_time\": 1619214150116,\n  \"autoscale\": {\n    \"min_workers\": 2,\n    \"max_workers\": 8\n  },\n  \"default_tags\": {\n    \"Vendor\": \"Databricks\",\n    \"Creator\": \"someone@example.com\",\n    \"ClusterName\": \"my-cluster\",\n    \"ClusterId\": \"1234-567890-batch123\"\n  },\n  \"creator_user_name\": \"somone@example.com\",\n  \"termination_reason\": {\n    \"code\": \"INACTIVITY\",\n    \"parameters\": {\n      \"inactivity_duration_min\": \"120\"\n    },\n    \"type\": \"SUCCESS\"\n  },\n  \"init_scripts_safe_mode\": false\n}\n\n```\n\n## List information about all available clusters\n\nTo display usage documentation, run `databricks clusters list --help`.\n\n```\ndatabricks clusters list --output JSON | jq .\n\n```\n\n```\n{\n  \"clusters\": [\n    {\n      \"cluster_id\": \"1234-567890-batch123\",\n      \"spark_context_id\": 8232037838300762810,\n      \"cluster_name\": \"my-cluster\",\n      \"spark_version\": \"8.1.x-scala2.12\",\n      \"aws_attributes\": {\n        \"zone_id\": \"us-west-2c\",\n        \"first_on_demand\": 1,\n        \"availability\": \"SPOT_WITH_FALLBACK\",\n        \"spot_bid_price_percent\": 100,\n        \"ebs_volume_count\": 0\n      },\n      \"node_type_id\": \"i3.xlarge\",\n      \"driver_node_type_id\": \"i3.xlarge\",\n      \"autotermination_minutes\": 120,\n      \"enable_elastic_disk\": false,\n      \"disk_spec\": {\n        \"disk_count\": 0\n      },\n      \"cluster_source\": \"UI\",\n      \"enable_local_disk_encryption\": false,\n      \"instance_source\": {\n        \"node_type_id\": \"i3.xlarge\"\n      },\n      \"driver_instance_source\": {\n        \"node_type_id\": \"i3.xlarge\"\n      },\n      \"state\": \"TERMINATED\",\n      \"state_message\": \"Inactive cluster terminated (inactive for 120 minutes).\",\n      \"start_time\": 1616773202562,\n      \"terminated_time\": 1619228528317,\n      \"last_state_loss_time\": 1619214150116,\n      \"autoscale\": {\n        \"min_workers\": 2,\n        \"max_workers\": 8\n      },\n      \"default_tags\": {\n        \"Vendor\": \"Databricks\",\n        \"Creator\": \"someone@example.com\",\n        \"ClusterName\": \"my-cluster\",\n        \"ClusterId\": \"1234-567890-batch123\"\n      },\n      \"creator_user_name\": \"somone@example.com\",\n      \"termination_reason\": {\n        \"code\": \"INACTIVITY\",\n        \"parameters\": {\n          \"inactivity_duration_min\": \"120\"\n        },\n        \"type\": \"SUCCESS\"\n      },\n      \"init_scripts_safe_mode\": false\n    },\n    ...\n  ]\n}\n\n```\n\n## List available cluster node types\n\nTo display usage documentation, run `databricks clusters list-node-types --help`.\n\n```\ndatabricks clusters list-node-types\n\n```\n\n```\n{\n  \"node_types\": [\n    {\n      \"node_type_id\": \"z1d.12xlarge\",\n      \"memory_mb\": 393216,\n      \"num_cores\": 48.0,\n      \"description\": \"z1d.12xlarge\",\n      \"instance_type_id\": \"z1d.12xlarge\",\n      \"is_deprecated\": false,\n      \"category\": \"Memory Optimized\",\n      \"support_ebs_volumes\": true,\n      \"support_cluster_tags\": true,\n      \"num_gpus\": 0,\n      \"node_instance_type\": {\n        \"instance_type_id\": \"z1d.12xlarge\",\n        \"local_disks\": 2,\n        \"local_disk_size_gb\": 900,\n        \"instance_family\": \"EC2 z1d Family vCPUs\",\n        \"swap_size\": \"10g\"\n      },\n      \"is_hidden\": false,\n      \"support_port_forwarding\": true,\n      \"display_order\": 0,\n      \"is_io_cache_enabled\": false\n    },\n    ...\n  ]\n}\n\n```\n\n## List available zones for creating clusters\n\nTo display usage documentation, run `databricks clusters list-zones --help`.\n\n```\ndatabricks clusters list-zones\n\n```\n\n```\n{\n  \"zones\": [\n    \"us-west-2c\",\n    \"us-west-2a\",\n    \"us-west-2b\"\n  ],\n  \"default_zone\": \"us-west-2c\"\n}\n\n```\n\n## Permanently delete a cluster\n\nTo display usage documentation, run `databricks clusters permanent-delete --help`.\n\n```\ndatabricks clusters permanent-delete --cluster-id 1234-567890-batch123\n\n```\n\nIf successful, no output is displayed.\n\n## Resize a cluster\n\nTo display usage documentation, run `databricks clusters resize --help`."},{"doc_uri":"https:\/\/docs.databricks.com\/release-notes\/product\/2018\/august.html","content":"August 2018 | Databricks on AWS\n\n* [![Databricks](..\/..\/..\/_static\/small-scale-lockup-full-color-rgb.svg)](https:\/\/www.databricks.com\/)\n\n* [![Databricks](..\/..\/..\/_static\/small-scale-lockup-full-color-rgb.svg)](https:\/\/www.databricks.com\/)\n* [Help Center](https:\/\/help.databricks.com\/s\/)\n* [Documentation](https:\/\/docs.databricks.com\/en\/index.html)\n* [Knowledge Base](https:\/\/kb.databricks.com\/)\n\n* [Community](https:\/\/community.databricks.com)\n* [Support](https:\/\/help.databricks.com)\n* [Feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n* [Try Databricks](https:\/\/databricks.com\/try-databricks)\n\n[![](..\/..\/..\/_static\/icons\/globe.png)English](javascript:void(0))\n* [English](..\/..\/..\/..\/en\/release-notes\/product\/2018\/august.html)\n* [\u65e5\u672c\u8a9e](..\/..\/..\/..\/ja\/release-notes\/product\/2018\/august.html)\n* [Portugu\u00eas](..\/..\/..\/..\/pt\/release-notes\/product\/2018\/august.html)\n\n[![](..\/..\/..\/_static\/icons\/aws.svg)Amazon\n Web Services](javascript:void(0))\n* [![](..\/..\/..\/_static\/icons\/azure.svg)Microsoft Azure](https:\/\/learn.microsoft.com\/azure\/databricks\/release-notes\/product\/2018\/august)\n* [![](..\/..\/..\/_static\/icons\/gcp.svg)Google Cloud Platform](https:\/\/docs.gcp.databricks.com\/release-notes\/product\/2018\/august.html)\n\n[Databricks on AWS](..\/..\/..\/index.html)\nGet started\n\n* [Get started](..\/..\/..\/getting-started\/index.html)\n* [What is Databricks?](..\/..\/..\/introduction\/index.html)\n* [DatabricksIQ](..\/..\/..\/databricksiq\/index.html)\n* [Release notes](..\/..\/index.html)\n\t+ [Databricks platform release notes](..\/index.html)\n\t\t- [May 2024](..\/2024\/may.html)\n\t\t- [April 2024](..\/2024\/april.html)\n\t\t- [March 2024](..\/2024\/march.html)\n\t\t- [February 2024](..\/2024\/february.html)\n\t\t- [January 2024](..\/2024\/january.html)\n\t\t- [December 2023](..\/2023\/december.html)\n\t\t- [November 2023](..\/2023\/november.html)\n\t\t- [October 2023](..\/2023\/october.html)\n\t\t- [September 2023](..\/2023\/september.html)\n\t\t- [August 2023](..\/2023\/august.html)\n\t\t- [July 2023](..\/2023\/july.html)\n\t\t- [June 2023](..\/2023\/june.html)\n\t\t- [May 2023](..\/2023\/may.html)\n\t\t- [April 2023](..\/2023\/april.html)\n\t\t- [March 2023](..\/2023\/march.html)\n\t\t- [February 2023](..\/2023\/february.html)\n\t\t- [January 2023](..\/2023\/january.html)\n\t\t- [December 2022](..\/2022\/december.html)\n\t\t- [November 2022](..\/2022\/november.html)\n\t\t- [October 2022](..\/2022\/october.html)\n\t\t- [September 2022](..\/2022\/september.html)\n\t\t- [August 2022](..\/2022\/august.html)\n\t\t- [July 2022](..\/2022\/july.html)\n\t\t- [June 2022](..\/2022\/june.html)\n\t\t- [May 2022](..\/2022\/may.html)\n\t\t- [April 2022](..\/2022\/april.html)\n\t\t- [March 2022](..\/2022\/march.html)\n\t\t- [February 2022](..\/2022\/february.html)\n\t\t- [January 2022](..\/2022\/january.html)\n\t\t- [December 2021](..\/2021\/december.html)\n\t\t- [November 2021](..\/2021\/november.html)\n\t\t- [October 2021](..\/2021\/october.html)\n\t\t- [September 2021](..\/2021\/september.html)\n\t\t- [August 2021](..\/2021\/august.html)\n\t\t- [July 2021](..\/2021\/july.html)\n\t\t- [June 2021](..\/2021\/june.html)\n\t\t- [May 2021](..\/2021\/may.html)\n\t\t- [April 2021](..\/2021\/april.html)\n\t\t- [March 2021](..\/2021\/march.html)\n\t\t- [February 2021](..\/2021\/february.html)\n\t\t- [January 2021](..\/2021\/january.html)\n\t\t- [December 2020](..\/2020\/december.html)\n\t\t- [November 2020](..\/2020\/november.html)\n\t\t- [October 2020](..\/2020\/october.html)\n\t\t- [September 2020](..\/2020\/september.html)\n\t\t- [August 2020](..\/2020\/august.html)\n\t\t- [July 2020](..\/2020\/july.html)\n\t\t- [June 2020](..\/2020\/june.html)\n\t\t- [May 2020](..\/2020\/may.html)\n\t\t- [April 2020](..\/2020\/april.html)\n\t\t- [March 2020](..\/2020\/march.html)\n\t\t- [February 2020](..\/2020\/february.html)\n\t\t- [January 2020](..\/2020\/january.html)\n\t\t- [December 2019](..\/2019\/december.html)\n\t\t- [November 2019](..\/2019\/november.html)\n\t\t- [October 2019](..\/2019\/october.html)\n\t\t- [September 2019](..\/2019\/september.html)\n\t\t- [August 2019](..\/2019\/august.html)\n\t\t- [July 2019](..\/2019\/july.html)\n\t\t- [June 2019](..\/2019\/june.html)\n\t\t- [May 2019](..\/2019\/may.html)\n\t\t- [April 2019](..\/2019\/april.html)\n\t\t- [March 2019](..\/2019\/march.html)\n\t\t- [February 2019](..\/2019\/february.html)\n\t\t- [January 2019](..\/2019\/january.html)\n\t\t- [December 2018](december.html)\n\t\t- [November 2018](november.html)\n\t\t- [October 2018](october.html)\n\t\t- [September 2018](september.html)\n\t\t- [August 2018](#)\n\t\t- [July 2018](july.html)\n\t\t- [June 2018](june.html)\n\t\t- [May 2018](may.html)\n\t\t- [April 2018](april.html)\n\t\t- [March 2018](march.html)\n\t\t- [February 2018](february.html)\n\t\t- [January 2018](january.html)\n\t+ [Databricks Runtime release notes versions and compatibility](..\/..\/runtime\/index.html)\n\t+ [Databricks SQL release notes](..\/..\/..\/sql\/release-notes\/index.html)\n\t+ [Serverless compute release notes](..\/..\/serverless.html)\n\t+ [Databricks developer tools and SDKs release notes](..\/..\/dev-tools\/index.html)\n\t+ [Databricks Connect release notes](..\/..\/dbconnect\/index.html)\n\t+ [Delta Live Tables release notes](..\/..\/delta-live-tables\/index.html)\n\t+ [Databricks Previews support & details](..\/..\/release-types.html)\n\t+ [Migrate from legacy features](..\/..\/migrations.html)\n\nLoad & manage data\n\n* [Connect to data sources](..\/..\/..\/connect\/index.html)\n* [Connect to compute](..\/..\/..\/compute\/index.html)\n* [Discover data](..\/..\/..\/discover\/index.html)\n* [Query data](..\/..\/..\/query\/index.html)\n* [Ingest data](..\/..\/..\/ingestion\/index.html)\n* [Transform data](..\/..\/..\/transform\/index.html)\n* [Monitor data and AI assets](..\/..\/..\/lakehouse-monitoring\/index.html)\n* [Share data (Delta Sharing)](..\/..\/..\/data-sharing\/index.html)\n* [Databricks Marketplace](..\/..\/..\/marketplace\/index.html)\n\nWork with data\n\n* [Data engineering](..\/..\/..\/workspace-index.html)\n* [Generative AI & LLMs](..\/..\/..\/generative-ai\/generative-ai.html)\n* [Machine learning](..\/..\/..\/machine-learning\/index.html)\n* [Model serving](..\/..\/..\/machine-learning\/model-serving\/index.html)\n* [Data warehousing](..\/..\/..\/sql\/index.html)\n* [Delta Lake](..\/..\/..\/delta\/index.html)\n* [Developers](..\/..\/..\/languages\/index.html)\n* [Technology partners](..\/..\/..\/integrations\/index.html)\n\nAdministration\n\n* [Account and workspace administration](..\/..\/..\/admin\/index.html)\n* [Security and compliance](..\/..\/..\/security\/index.html)\n* [Data governance (Unity Catalog)](..\/..\/..\/data-governance\/index.html)\n* [Lakehouse architecture](..\/..\/..\/lakehouse-architecture\/index.html)\n\nReference & resources\n\n* [Reference](..\/..\/..\/reference\/api.html)\n* [Resources](..\/..\/..\/resources\/index.html)\n* [What\u2019s coming?](..\/..\/..\/whats-coming.html)\n* [Documentation archive](..\/..\/..\/archive\/index.html)\n\n Updated May 28, 2024\n \n\n[Send us feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n\n* [Documentation](..\/..\/..\/index.html)\n* [Databricks release notes](..\/..\/index.html)\n* [Databricks platform release notes](..\/index.html)\n* August 2018\n* \n\n# August 2018\n\nThese features and Databricks platform improvements were released in August 2018.\n\nNote\n\nReleases are staged. Your Databricks account may not be updated until a week after the initial release date.\n\n## Workspace sidebar redesign\n\n**August 28 - September 4, 2018: Version 2.79**\n\nThe sidebar for navigating workspace files has been simplified and streamlined, showing only two swimlanes at a time, providing left and right arrows to scroll horizontally when there are more than two swimlanes to view, defaulting to your own user file view when you click the Workspace or Home button in the left navigation, and enabling you to pin the sidebar open while you work in the main pane, which remains unobstructed. We hope you enjoy the changes.\n\n## New environment variables in init scripts\n\n**August 28 - September 4, 2018: Version 2.79**\n\nCluster-scoped init scripts now support the following [environment variables](..\/..\/..\/init-scripts\/environment-variables.html):\n\n* `DB_DRIVER_IP` - the IP address of the driver node\n* `DB_INSTANCE_TYPE` - the type of Spark node\n* `DB_PYTHON_VERSION` - the version of Python used on the cluster\n* `DB_IS_JOB_CLUSTER` - indicates whether the cluster was created to run a job\n\nSee [Use cluster-scoped init scripts](..\/..\/..\/init-scripts\/cluster-scoped.html).\n\n## EBS leaked volumes logging and deletion\n\n**August 28 - September 4, 2018: Version 2.79**\n\nWhenever a Databricks cluster instance is terminated, AWS should automatically delete all EBS volumes associated with the cluster. Unfortunately, if an instance dies during startup, the EBS volumes never get attached to the instance, and AWS never cleans them up. The AWS support team has confirmed that they are working on fix for this bug, but they have not provided an ETA. Until AWS provides a fix, Databricks is providing a workaround that enables you or Databricks support to locate and delete the unattached EBS volumes.\n\nFor instructions, see the KnowledgeBase article [EBS Leaked Volumes](https:\/\/kb.databricks.com\/clusters\/ebs-leaked-volumes.html#ebs-leaked-volumes).\n\nImportant\n\nIn late September or October, Databricks will start deleting all leaked EBS volumes by default. We recommend that you identify any leaked EBS volumes by contacting Databricks support or by using the instructions in the KnowledgeBase article, and save any data on these volumes that you want to keep.\n\n## AWS r3 and c3 instance types now deprecated\n\n**August 28, 2018**\n\nDatabricks will remove support for AWS r3 and c3 instance types *in one year*. At that point they will no longer be available for selection when you create a cluster. API workloads will continue to run after support ceases, but we don\u2019t recommend using these instance types for production workloads at that point.\n\n## Audit logging for ACL changes\n\n**August 14-21, 2018: Version 2.78**\n\nAll changes to ACLs are now included in the usage and audit logs, including ACL changes for:\n\n* Notebooks and folders\n* Clusters\n* Jobs\n* IAM roles (instance profiles)\n\nSee [Audit log reference](..\/..\/..\/admin\/account-settings\/audit-logs.html).\n\n## Cluster-scoped init scripts\n\n**August 14-21, 2018: Version 2.78**\n\nCluster-scoped init scripts are init scripts defined in a cluster configuration. Cluster-scoped init scripts have the following advantages over other types of scripts:\n\n* Apply to both clusters you create and those created to run jobs.\n* Are protected by cluster ACLs.\n* Log start and finish events in the [cluster event log](..\/..\/..\/compute\/clusters-manage.html#event-log).\n* Have a configurable log location.\n* Support pre-defined environment variables, which enable conditional processing.\n\nFor details, see [Use cluster-scoped init scripts](..\/..\/..\/init-scripts\/cluster-scoped.html).\n\n## Collapsible headings\n\n**August 14-21, 2018: Version 2.78**\n\nCells that appear after cells containing Markdown headings can now be collapsed into the heading cell. The following image shows a Heading 1 with the following two cells collapsed into it.\n\n![Collapsed cells under Markdown heading](..\/..\/..\/_images\/headings.png)\n\nClick the **+** and **-** to expand and collapse headings.\n\n---"}]}
{"request_id":"id-2531","request":"How can I read data from a Parquet file using Databricks in a Python notebook?","response":"To read data from a Parquet file using Databricks in a Python notebook, you can use the following code:\n\n```python\n# Import the necessary libraries\nfrom pyspark.sql import SparkSession\n\n# Create a SparkSession\nspark = SparkSession.builder \\\n   .appName('Read Parquet File') \\\n   .getOrCreate()\n\n# Define the path to the Parquet file\nparquet_file_path = \"\/path\/to\/your\/file.parquet\"\n\n# Read the Parquet file\ndf = spark.read.parquet(parquet_file_path)\n\n# Show the data\ndf.show()\n```\n\nReplace `\/path\/to\/your\/file.parquet` with the actual path to your Parquet file. This code creates a SparkSession, defines the path to the Parquet file, reads the file into a DataFrame, and then displays the data.\n\nFor more information about reading Parquet files in Databricks, you can refer to the [Databricks documentation](https:\/\/docs.databricks.com\/data\/data-sources\/read-parquet.html).","expected_retrieved_context":[{"doc_uri":"https:\/\/docs.databricks.com\/query\/formats\/parquet.html","content":"Read Parquet files using Databricks  \nThis article shows you how to read data from Apache Parquet files using Databricks.  \nWhat is Parquet?\nWhat is Parquet?\nApache Parquet is a columnar file format with optimizations that speed up queries. It\u2019s a more efficient file format than CSV or JSON.  \nFor more information, see Parquet Files.\n\nOptions\nOptions\nSee the following Apache Spark reference articles for supported read and write options.  \nRead  \nPython  \nScala  \nWrite  \nPython  \nScala\n\nNotebook example: Read and write to Parquet files\nNotebook example: Read and write to Parquet files\nThe following notebook shows how to read and write data to Parquet files.  \nReading Parquet files notebook  \nOpen notebook in new tab Copy link for import"}],"expected_response":"To read data from a Parquet file using Databricks in a Python notebook, you can use the `spark.read.parquet()` function. Here's a simple example:\n\n```python\n# Import the SparkSession class\nfrom pyspark.sql import SparkSession\n\n# Create a SparkSession object\nspark = SparkSession.builder.appName('ParquetReader').getOrCreate()\n\n# Replace 'path_to_your_parquet_file' with the actual path to your Parquet file\nparquet_file_path = 'path_to_your_parquet_file'\n\n# Read the Parquet file\ndf = spark.read.parquet(parquet_file_path)\n\n# Show the data\ndf.show()\n```\n\nIn this example, replace `'path_to_your_parquet_file'` with the actual path to your Parquet file. This could be a local file path on your Databricks workspace, or a path to a file stored in a cloud storage service like AWS S3 or Azure Data Lake Storage.\n\nRemember to install the necessary libraries and set up your Databricks workspace before running this code. For more information on how to do this, refer to the Databricks documentation.","retrieved_context":[{"doc_uri":"https:\/\/docs.databricks.com\/query\/formats\/parquet.html","content":"Read Parquet files using Databricks | Databricks on AWS\n\n* [![Databricks](..\/..\/_static\/small-scale-lockup-full-color-rgb.svg)](https:\/\/www.databricks.com\/)\n\n* [![Databricks](..\/..\/_static\/small-scale-lockup-full-color-rgb.svg)](https:\/\/www.databricks.com\/)\n* [Help Center](https:\/\/help.databricks.com\/s\/)\n* [Documentation](https:\/\/docs.databricks.com\/en\/index.html)\n* [Knowledge Base](https:\/\/kb.databricks.com\/)\n\n* [Community](https:\/\/community.databricks.com)\n* [Support](https:\/\/help.databricks.com)\n* [Feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n* [Try Databricks](https:\/\/databricks.com\/try-databricks)\n\n[![](..\/..\/_static\/icons\/globe.png)English](javascript:void(0))\n* [English](..\/..\/..\/en\/query\/formats\/parquet.html)\n* [\u65e5\u672c\u8a9e](..\/..\/..\/ja\/query\/formats\/parquet.html)\n* [Portugu\u00eas](..\/..\/..\/pt\/query\/formats\/parquet.html)\n\n[![](..\/..\/_static\/icons\/aws.svg)Amazon\n Web Services](javascript:void(0))\n* [![](..\/..\/_static\/icons\/azure.svg)Microsoft Azure](https:\/\/learn.microsoft.com\/azure\/databricks\/query\/formats\/parquet)\n* [![](..\/..\/_static\/icons\/gcp.svg)Google Cloud Platform](https:\/\/docs.gcp.databricks.com\/query\/formats\/parquet.html)\n\n[Databricks on AWS](..\/..\/index.html)\nGet started\n\n* [Get started](..\/..\/getting-started\/index.html)\n* [What is Databricks?](..\/..\/introduction\/index.html)\n* [DatabricksIQ](..\/..\/databricksiq\/index.html)\n* [Release notes](..\/..\/release-notes\/index.html)\n\nLoad & manage data\n\n* [Connect to data sources](..\/..\/connect\/index.html)\n* [Connect to compute](..\/..\/compute\/index.html)\n* [Discover data](..\/..\/discover\/index.html)\n* [Query data](..\/index.html)\n\t+ [Data format options](index.html)\n\t\t- [Delta Sharing](deltasharing.html)\n\t\t- [Read Parquet files using Databricks](#)\n\t\t- [ORC file](orc.html)\n\t\t- [JSON file](json.html)\n\t\t- [Read and write to CSV files](csv.html)\n\t\t- [Avro file](avro.html)\n\t\t- [Text files](text.html)\n\t\t- [Image](image.html)\n\t\t- [Binary file](binary.html)\n\t\t- [Hive table](hive-tables.html)\n\t\t- [XML file](xml.html)\n\t\t- [MLflow experiment](mlflow-experiment.html)\n\t\t- [LZO compressed file](lzo.html)\n\t+ [Query streaming data](..\/streaming.html)\n* [Ingest data](..\/..\/ingestion\/index.html)\n* [Transform data](..\/..\/transform\/index.html)\n* [Monitor data and AI assets](..\/..\/lakehouse-monitoring\/index.html)\n* [Share data (Delta Sharing)](..\/..\/data-sharing\/index.html)\n* [Databricks Marketplace](..\/..\/marketplace\/index.html)\n\nWork with data\n\n* [Data engineering](..\/..\/workspace-index.html)\n* [Generative AI & LLMs](..\/..\/generative-ai\/generative-ai.html)\n* [Machine learning](..\/..\/machine-learning\/index.html)\n* [Model serving](..\/..\/machine-learning\/model-serving\/index.html)\n* [Data warehousing](..\/..\/sql\/index.html)\n* [Delta Lake](..\/..\/delta\/index.html)\n* [Developers](..\/..\/languages\/index.html)\n* [Technology partners](..\/..\/integrations\/index.html)\n\nAdministration\n\n* [Account and workspace administration](..\/..\/admin\/index.html)\n* [Security and compliance](..\/..\/security\/index.html)\n* [Data governance (Unity Catalog)](..\/..\/data-governance\/index.html)\n* [Lakehouse architecture](..\/..\/lakehouse-architecture\/index.html)\n\nReference & resources\n\n* [Reference](..\/..\/reference\/api.html)\n* [Resources](..\/..\/resources\/index.html)\n* [What\u2019s coming?](..\/..\/whats-coming.html)\n* [Documentation archive](..\/..\/archive\/index.html)\n\n Updated May 28, 2024\n \n\n[Send us feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n\n* [Documentation](..\/..\/index.html)\n* [Query data](..\/index.html)\n* [Data format options](index.html)\n* Read Parquet files using Databricks\n* \n\n# Read Parquet files using Databricks\n\nThis article shows you how to read data from Apache Parquet files using Databricks.\n\n## What is Parquet?\n\n[Apache Parquet](https:\/\/parquet.apache.org\/) is a columnar file format with optimizations that speed up queries. It\u2019s a more efficient file format than [CSV](csv.html) or [JSON](json.html).\n\nFor more information, see [Parquet Files](https:\/\/spark.apache.org\/docs\/latest\/sql-data-sources-parquet.html).\n\n## Options\n\nSee the following Apache Spark reference articles for supported read and write options.\n\n* Read\n\n\t+ [Python](https:\/\/api-docs.databricks.com\/python\/pyspark\/latest\/pyspark.sql\/api\/pyspark.sql.DataFrameReader.parquet.html?highlight=parquet#pyspark.sql.DataFrameReader.parquet)\n\t+ [Scala](https:\/\/api-docs.databricks.com\/scala\/spark\/latest\/org\/apache\/spark\/sql\/DataFrameReader.html#parquet(paths:String*):org.apache.spark.sql.DataFrame)\n* Write\n\n\t+ [Python](https:\/\/api-docs.databricks.com\/python\/pyspark\/latest\/pyspark.sql\/api\/pyspark.sql.DataFrameWriter.parquet.html?highlight=parquet#pyspark.sql.DataFrameWriter.parquet)\n\t+ [Scala](https:\/\/api-docs.databricks.com\/scala\/spark\/latest\/org\/apache\/spark\/sql\/DataFrameWriter.html#parquet(path:String):Unit)\n\n## Notebook example: Read and write to Parquet files\n\nThe following notebook shows how to read and write data to Parquet files.\n\n### Reading Parquet files notebook\n\n[Open notebook in new tab](\/_extras\/notebooks\/source\/read-parquet-files.html)\n ![Copy to clipboard](\/_static\/clippy.svg) Copy link for import     \n\n---\n\n \u00a9 Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the [Apache Software Foundation](http:\/\/www.apache.org\/).\n \n\n[Send us feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n \n | [Privacy Policy](https:\/\/databricks.com\/privacy-policy) | [Terms of Use](https:\/\/databricks.com\/terms-of-use)\n\n { # 's' variable stands for the search page here. TODO: Fix it. #}\n \n [![DATA+AU Summit 2024](..\/..\/_static\/images\/dais\/docs-dais-promo-image.png)](https:\/\/www.databricks.com\/dataaisummit?utm_source=databricks&utm_medium=web&utm_campaign=7018y000001f7cvqai&utm_offer=docs \"DATA+AU Summit 2024\")"},{"doc_uri":"https:\/\/docs.databricks.com\/getting-started\/lakehouse-e2e.html","content":"def __init__(self, source):\n        self.source = source\n\n    def get_date(self):\n        try:\n            df = spark.read.format(\"json\").load(source)\n        except:\n            return \"2016-01-01\"\n        batch_date = df.selectExpr(\"max(distinct(date(tpep_pickup_datetime))) + 1 day\").first()[0]\n        if batch_date.month == 3:\n            raise Exception(\"Source data exhausted\")\n        return batch_date\n\n    def get_batch(self, batch_date):\n        return (\n            spark.table(\"samples.nyctaxi.trips\")\n            .filter(col(\"tpep_pickup_datetime\").cast(\"date\") == batch_date)\n        )\n\n    def write_batch(self, batch):\n        batch.write.format(\"json\").mode(\"append\").save(self.source)\n\n    def land_batch(self):\n        batch_date = self.get_date()\n        batch = self.get_batch(batch_date)\n        self.write_batch(batch)\n\nRawData = LoadData(source)\n\n```\n\nYou can now land a batch of data by copying the following code into a cell and executing it. You can manually execute this cell up to 60 times to trigger new data arrival.\n\n```\nRawData.land_batch()\n\n```\n\n## Step 4: Configure Auto Loader to ingest data to Unity Catalog\n\nDatabricks recommends storing data with [Delta Lake](..\/delta\/index.html). Delta Lake is an open source storage layer that provides ACID transactions and enables the data lakehouse. Delta Lake is the default format for tables created in Databricks.\n\nTo configure Auto Loader to ingest data to a Unity Catalog table, copy and paste the following code into an empty cell in your notebook:\n\n```\n# Import functions\nfrom pyspark.sql.functions import col, current_timestamp\n\n# Configure Auto Loader to ingest JSON data to a Delta table\n(spark.readStream\n  .format(\"cloudFiles\")\n  .option(\"cloudFiles.format\", \"json\")\n  .option(\"cloudFiles.schemaLocation\", checkpoint_path)\n  .load(file_path)\n  .select(\"*\", col(\"_metadata.file_path\").alias(\"source_file\"), current_timestamp().alias(\"processing_time\"))\n  .writeStream\n  .option(\"checkpointLocation\", checkpoint_path)\n  .trigger(availableNow=True)\n  .option(\"mergeSchema\", \"true\")\n  .toTable(table))\n\n```\n\nTo learn more about Auto Loader, see [What is Auto Loader?](..\/ingestion\/auto-loader\/index.html).\n\nTo learn more about Structured Streaming with Unity Catalog, see [Using Unity Catalog with Structured Streaming](..\/structured-streaming\/unity-catalog.html).\n\n## Step 5: Process and interact with data\n\nNotebooks execute logic cell-by-cell. Use these steps to execute the logic in your cell:\n\n1. To run the cell you completed in the previous step, select the cell and press **SHIFT+ENTER**.\n2. To query the table you\u2019ve just created, copy and paste the following code into an empty cell, then press **SHIFT+ENTER** to run the cell.\n\n```\ndf = spark.read.table(table_name)\n\n```\n3. To preview the data in your DataFrame, copy and paste the following code into an empty cell, then press **SHIFT+ENTER** to run the cell.\n\n```\ndisplay(df)\n\n```\n\nTo learn more about interactive options for visualizing data, see [Visualizations in Databricks notebooks](..\/visualizations\/index.html).\n\n## Step 6: Schedule a job\n\nYou can run Databricks notebooks as production scripts by adding them as a task in a Databricks job. In this step, you will create a new job that you can trigger manually.\n\nTo schedule your notebook as a task:\n\n1. Click **Schedule** on the right side of the header bar.\n2. Enter a unique name for the **Job name**.\n3. Click **Manual**.\n4. In the **Cluster** drop-down, select the cluster you created in step 1.\n5. Click **Create**.\n6. In the window that appears, click **Run now**.\n7. To see the job run results, click the ![External Link](..\/_images\/external-link.png) icon next to the **Last run** timestamp.\n\nFor more information on jobs, see [What is Databricks Jobs?](..\/workflows\/index.html#what-is-jobs).\n\n## Step 7: Query table from Databricks SQL\n\nAnyone with the `USE CATALOG` permission on the current catalog, the `USE SCHEMA` permission on the current schema, and `SELECT` permissions on the table can query the contents of the table from their preferred Databricks API.\n\nYou need access to a running SQL warehouse to execute queries in Databricks SQL.\n\nThe table you created earlier in this tutorial has the name `target_table`. You can query it using the catalog you provided in the first cell and the database with the patern `e2e_lakehouse_<your-username>`. You can use [Catalog Explorer](..\/catalog-explorer\/index.html) to find the data objects that you created.\n\n## Additional Integrations\n\nLearn more about integrations and tools for data engineering with Databricks:\n\n* [Connect your favorite IDE](..\/dev-tools\/index.html)\n* [Use dbt with Databricks](..\/partners\/prep\/dbt.html)\n* [Learn about the Databricks Command Line Interface (CLI)](..\/dev-tools\/cli\/index.html)\n* [Learn about the Databricks Terraform Provider](..\/dev-tools\/terraform\/index.html)\n\n---\n\n \u00a9 Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the [Apache Software Foundation](http:\/\/www.apache.org\/).\n \n\n[Send us feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n \n | [Privacy Policy](https:\/\/databricks.com\/privacy-policy) | [Terms of Use](https:\/\/databricks.com\/terms-of-use)\n\n { # 's' variable stands for the search page here. TODO: Fix it. #}\n \n [![DATA+AU Summit 2024](..\/_static\/images\/dais\/docs-dais-promo-image.png)](https:\/\/www.databricks.com\/dataaisummit?utm_source=databricks&utm_medium=web&utm_campaign=7018y000001f7cvqai&utm_offer=docs \"DATA+AU Summit 2024\")"},{"doc_uri":"https:\/\/docs.databricks.com\/notebooks\/notebook-workflows.html","content":"To view the run details, click the **Start time** link in the table. If the run is complete, you can also view the run details by clicking the **End time** link.\n\n![Result of ephemeral notebook run](..\/_images\/notebook-run-results.png)\n\n## Pass structured data\n\nThis section illustrates how to pass structured data between notebooks.\n\n```\n# Example 1 - returning data through temporary views.\n# You can only return one string using dbutils.notebook.exit(), but since called notebooks reside in the same JVM, you can\n# return a name referencing data stored in a temporary view.\n\n## In callee notebook\nspark.range(5).toDF(\"value\").createOrReplaceGlobalTempView(\"my_data\")\ndbutils.notebook.exit(\"my_data\")\n\n## In caller notebook\nreturned_table = dbutils.notebook.run(\"LOCATION_OF_CALLEE_NOTEBOOK\", 60)\nglobal_temp_db = spark.conf.get(\"spark.sql.globalTempDatabase\")\ndisplay(table(global_temp_db + \".\" + returned_table))\n\n# Example 2 - returning data through DBFS.\n# For larger datasets, you can write the results to DBFS and then return the DBFS path of the stored data.\n\n## In callee notebook\ndbutils.fs.rm(\"\/tmp\/results\/my_data\", recurse=True)\nspark.range(5).toDF(\"value\").write.format(\"parquet\").save(\"dbfs:\/tmp\/results\/my_data\")\ndbutils.notebook.exit(\"dbfs:\/tmp\/results\/my_data\")\n\n## In caller notebook\nreturned_table = dbutils.notebook.run(\"LOCATION_OF_CALLEE_NOTEBOOK\", 60)\ndisplay(spark.read.format(\"parquet\").load(returned_table))\n\n# Example 3 - returning JSON data.\n# To return multiple values, you can use standard JSON libraries to serialize and deserialize results.\n\n## In callee notebook\nimport json\ndbutils.notebook.exit(json.dumps({\n  \"status\": \"OK\",\n  \"table\": \"my_data\"\n}))\n\n## In caller notebook\nimport json\n\nresult = dbutils.notebook.run(\"LOCATION_OF_CALLEE_NOTEBOOK\", 60)\nprint(json.loads(result))\n\n```\n\n```\n\/\/ Example 1 - returning data through temporary views.\n\/\/ You can only return one string using dbutils.notebook.exit(), but since called notebooks reside in the same JVM, you can\n\/\/ return a name referencing data stored in a temporary view.\n\n\/** In callee notebook *\/\nsc.parallelize(1 to 5).toDF().createOrReplaceGlobalTempView(\"my_data\")\ndbutils.notebook.exit(\"my_data\")\n\n\/** In caller notebook *\/\nval returned_table = dbutils.notebook.run(\"LOCATION_OF_CALLEE_NOTEBOOK\", 60)\nval global_temp_db = spark.conf.get(\"spark.sql.globalTempDatabase\")\ndisplay(table(global_temp_db + \".\" + returned_table))\n\n\/\/ Example 2 - returning data through DBFS.\n\/\/ For larger datasets, you can write the results to DBFS and then return the DBFS path of the stored data.\n\n\/** In callee notebook *\/\ndbutils.fs.rm(\"\/tmp\/results\/my_data\", recurse=true)\nsc.parallelize(1 to 5).toDF().write.format(\"parquet\").save(\"dbfs:\/tmp\/results\/my_data\")\ndbutils.notebook.exit(\"dbfs:\/tmp\/results\/my_data\")\n\n\/** In caller notebook *\/\nval returned_table = dbutils.notebook.run(\"LOCATION_OF_CALLEE_NOTEBOOK\", 60)\ndisplay(sqlContext.read.format(\"parquet\").load(returned_table))\n\n\/\/ Example 3 - returning JSON data.\n\/\/ To return multiple values, you can use standard JSON libraries to serialize and deserialize results.\n\n\/** In callee notebook *\/\n\n\/\/ Import jackson json libraries\nimport com.fasterxml.jackson.module.scala.DefaultScalaModule\nimport com.fasterxml.jackson.module.scala.experimental.ScalaObjectMapper\nimport com.fasterxml.jackson.databind.ObjectMapper\n\n\/\/ Create a json serializer\nval jsonMapper = new ObjectMapper with ScalaObjectMapper\njsonMapper.registerModule(DefaultScalaModule)\n\n\/\/ Exit with json\ndbutils.notebook.exit(jsonMapper.writeValueAsString(Map(\"status\" -> \"OK\", \"table\" -> \"my_data\")))\n\n\/** In caller notebook *\/\n\n\/\/ Import jackson json libraries\nimport com.fasterxml.jackson.module.scala.DefaultScalaModule\nimport com.fasterxml.jackson.module.scala.experimental.ScalaObjectMapper\nimport com.fasterxml.jackson.databind.ObjectMapper\n\n\/\/ Create a json serializer\nval jsonMapper = new ObjectMapper with ScalaObjectMapper\njsonMapper.registerModule(DefaultScalaModule)\n\nval result = dbutils.notebook.run(\"LOCATION_OF_CALLEE_NOTEBOOK\", 60)\nprintln(jsonMapper.readValue[Map[String, String]](result))\n\n```\n\n## Handle errors\n\nThis section illustrates how to handle errors.\n\n```\n# Errors throw a WorkflowException.\n\ndef run_with_retry(notebook, timeout, args = {}, max_retries = 3):\n  num_retries = 0\n  while True:\n    try:\n      return dbutils.notebook.run(notebook, timeout, args)\n    except Exception as e:\n      if num_retries > max_retries:\n        raise e\n      else:\n        print(\"Retrying error\", e)\n        num_retries += 1\n\nrun_with_retry(\"LOCATION_OF_CALLEE_NOTEBOOK\", 60, max_retries = 5)\n\n```\n\n```\n\/\/ Errors throw a WorkflowException.\n\nimport com.databricks.WorkflowException\n\n\/\/ Since dbutils.notebook.run() is just a function call, you can retry failures using standard Scala try-catch\n\/\/ control flow. Here we show an example of retrying a notebook a number of times.\ndef runRetry(notebook: String, timeout: Int, args: Map[String, String] = Map.empty, maxTries: Int = 3): String = {\n  var numTries = 0\n  while (true) {\n    try {\n      return dbutils.notebook.run(notebook, timeout, args)\n    } catch {\n      case e: WorkflowException if numTries < maxTries =>\n        println(\"Error, retrying: \" + e)\n    }\n    numTries += 1\n  }\n  \"\" \/\/ not reached\n}\n\nrunRetry(\"LOCATION_OF_CALLEE_NOTEBOOK\", timeout = 60, maxTries = 5)\n\n```\n\n## Run multiple notebooks concurrently\n\nYou can run multiple notebooks at the same time by using standard Scala and Python constructs such as Threads ([Scala](https:\/\/docs.oracle.com\/javase\/7\/docs\/api\/java\/lang\/Thread.html), [Python](https:\/\/docs.python.org\/3\/library\/threading.html)) and Futures ([Scala](https:\/\/docs.scala-lang.org\/overviews\/core\/futures.html), [Python](https:\/\/docs.python.org\/3\/library\/multiprocessing.html)). The example notebooks demonstrate how to use these constructs.\n\n1. Download the following 4 notebooks. The notebooks are written in Scala.\n2. Import the notebooks into a single folder in the workspace.\n3. Run the **Run concurrently** notebook.\n\n### Run concurrently notebook\n\n[Open notebook in new tab](\/_extras\/notebooks\/source\/concurrent-notebooks.html)\n ![Copy to clipboard](\/_static\/clippy.svg) Copy link for import     \n\n### Run in parallel notebook\n\n[Open notebook in new tab](\/_extras\/notebooks\/source\/parallel-notebooks.html)\n ![Copy to clipboard](\/_static\/clippy.svg) Copy link for import     \n\n### Testing notebook\n\n[Open notebook in new tab](\/_extras\/notebooks\/source\/testing.html)\n ![Copy to clipboard](\/_static\/clippy.svg) Copy link for import     \n\n### Testing-2 notebook\n\n[Open notebook in new tab](\/_extras\/notebooks\/source\/testing-2.html)\n ![Copy to clipboard](\/_static\/clippy.svg) Copy link for import     \n\n---\n\n \u00a9 Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the [Apache Software Foundation](http:\/\/www.apache.org\/).\n \n\n[Send us feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n \n | [Privacy Policy](https:\/\/databricks.com\/privacy-policy) | [Terms of Use](https:\/\/databricks.com\/terms-of-use)\n\n { # 's' variable stands for the search page here. TODO: Fix it. #}\n \n [![DATA+AU Summit 2024](..\/_static\/images\/dais\/docs-dais-promo-image.png)](https:\/\/www.databricks.com\/dataaisummit?utm_source=databricks&utm_medium=web&utm_campaign=7018y000001f7cvqai&utm_offer=docs \"DATA+AU Summit 2024\")"},{"doc_uri":"https:\/\/docs.databricks.com\/query\/formats\/csv.html","content":"Read and write to CSV files | Databricks on AWS\n\n* [![Databricks](..\/..\/_static\/small-scale-lockup-full-color-rgb.svg)](https:\/\/www.databricks.com\/)\n\n* [![Databricks](..\/..\/_static\/small-scale-lockup-full-color-rgb.svg)](https:\/\/www.databricks.com\/)\n* [Help Center](https:\/\/help.databricks.com\/s\/)\n* [Documentation](https:\/\/docs.databricks.com\/en\/index.html)\n* [Knowledge Base](https:\/\/kb.databricks.com\/)\n\n* [Community](https:\/\/community.databricks.com)\n* [Support](https:\/\/help.databricks.com)\n* [Feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n* [Try Databricks](https:\/\/databricks.com\/try-databricks)\n\n[![](..\/..\/_static\/icons\/globe.png)English](javascript:void(0))\n* [English](..\/..\/..\/en\/query\/formats\/csv.html)\n* [\u65e5\u672c\u8a9e](..\/..\/..\/ja\/query\/formats\/csv.html)\n* [Portugu\u00eas](..\/..\/..\/pt\/query\/formats\/csv.html)\n\n[![](..\/..\/_static\/icons\/aws.svg)Amazon\n Web Services](javascript:void(0))\n* [![](..\/..\/_static\/icons\/azure.svg)Microsoft Azure](https:\/\/learn.microsoft.com\/azure\/databricks\/query\/formats\/csv)\n* [![](..\/..\/_static\/icons\/gcp.svg)Google Cloud Platform](https:\/\/docs.gcp.databricks.com\/query\/formats\/csv.html)\n\n[Databricks on AWS](..\/..\/index.html)\nGet started\n\n* [Get started](..\/..\/getting-started\/index.html)\n* [What is Databricks?](..\/..\/introduction\/index.html)\n* [DatabricksIQ](..\/..\/databricksiq\/index.html)\n* [Release notes](..\/..\/release-notes\/index.html)\n\nLoad & manage data\n\n* [Connect to data sources](..\/..\/connect\/index.html)\n* [Connect to compute](..\/..\/compute\/index.html)\n* [Discover data](..\/..\/discover\/index.html)\n* [Query data](..\/index.html)\n\t+ [Data format options](index.html)\n\t\t- [Delta Sharing](deltasharing.html)\n\t\t- [Read Parquet files using Databricks](parquet.html)\n\t\t- [ORC file](orc.html)\n\t\t- [JSON file](json.html)\n\t\t- [Read and write to CSV files](#)\n\t\t- [Avro file](avro.html)\n\t\t- [Text files](text.html)\n\t\t- [Image](image.html)\n\t\t- [Binary file](binary.html)\n\t\t- [Hive table](hive-tables.html)\n\t\t- [XML file](xml.html)\n\t\t- [MLflow experiment](mlflow-experiment.html)\n\t\t- [LZO compressed file](lzo.html)\n\t+ [Query streaming data](..\/streaming.html)\n* [Ingest data](..\/..\/ingestion\/index.html)\n* [Transform data](..\/..\/transform\/index.html)\n* [Monitor data and AI assets](..\/..\/lakehouse-monitoring\/index.html)\n* [Share data (Delta Sharing)](..\/..\/data-sharing\/index.html)\n* [Databricks Marketplace](..\/..\/marketplace\/index.html)\n\nWork with data\n\n* [Data engineering](..\/..\/workspace-index.html)\n* [Generative AI & LLMs](..\/..\/generative-ai\/generative-ai.html)\n* [Machine learning](..\/..\/machine-learning\/index.html)\n* [Model serving](..\/..\/machine-learning\/model-serving\/index.html)\n* [Data warehousing](..\/..\/sql\/index.html)\n* [Delta Lake](..\/..\/delta\/index.html)\n* [Developers](..\/..\/languages\/index.html)\n* [Technology partners](..\/..\/integrations\/index.html)\n\nAdministration\n\n* [Account and workspace administration](..\/..\/admin\/index.html)\n* [Security and compliance](..\/..\/security\/index.html)\n* [Data governance (Unity Catalog)](..\/..\/data-governance\/index.html)\n* [Lakehouse architecture](..\/..\/lakehouse-architecture\/index.html)\n\nReference & resources\n\n* [Reference](..\/..\/reference\/api.html)\n* [Resources](..\/..\/resources\/index.html)\n* [What\u2019s coming?](..\/..\/whats-coming.html)\n* [Documentation archive](..\/..\/archive\/index.html)\n\n Updated May 28, 2024\n \n\n[Send us feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n\n* [Documentation](..\/..\/index.html)\n* [Query data](..\/index.html)\n* [Data format options](index.html)\n* Read and write to CSV files\n* \n\n# Read and write to CSV files\n\nThis article provides examples for reading and writing to CSV files with Databricks using Python, Scala, R, and SQL.\n\nNote\n\nDatabricks recommends the [read\\_files table-valued function](..\/..\/sql\/language-manual\/functions\/read_files.html) for SQL users to read CSV files. `read_files` is available in Databricks Runtime 13.3 LTS and above.\n\nYou can also use a temporary view. If you use SQL to read CSV data directly without using temporary views or `read_files`, the following limitations apply:\n\n* You can\u2019t [specify data source options](#options).\n\n\t+ You can\u2019t [specify the schema](#specify-schema) for the data.\n\n## Options\n\nYou can configure several options for CSV file data sources. See the following Apache Spark reference articles for supported read and write options.\n\n* Read\n\n\t+ [Python](https:\/\/api-docs.databricks.com\/python\/pyspark\/latest\/pyspark.sql\/api\/pyspark.sql.DataFrameReader.csv.html#pyspark.sql.DataFrameReader.csv)\n\t+ [Scala](https:\/\/api-docs.databricks.com\/scala\/spark\/latest\/org\/apache\/spark\/sql\/DataFrameReader.html#csv(path:String):Unit)\n* Write\n\n\t+ [Python](https:\/\/api-docs.databricks.com\/python\/pyspark\/latest\/pyspark.sql\/api\/pyspark.sql.DataFrameWriter.csv.html#pyspark.sql.DataFrameWriter.csv)\n\t+ [Scala](https:\/\/api-docs.databricks.com\/scala\/spark\/latest\/org\/apache\/spark\/sql\/DataFrameWriter.html#csv(path:String):Unit)\n\n## Work with malformed CSV records\n\nWhen reading CSV files with a specified schema, it is possible that the data in the files does not match the schema. For example, a field containing name of the city will not parse as an integer. The consequences depend on the mode that the parser runs in:\n\n* `PERMISSIVE` (default): nulls are inserted for fields that could not be parsed correctly\n* `DROPMALFORMED`: drops lines that contain fields that could not be parsed\n* `FAILFAST`: aborts the reading if any malformed data is found\n\nTo set the mode, use the `mode` option.\n\n```\ndiamonds_df = (spark.read\n  .format(\"csv\")\n  .option(\"mode\", \"PERMISSIVE\")\n  .load(\"\/databricks-datasets\/Rdatasets\/data-001\/csv\/ggplot2\/diamonds.csv\")\n)\n\n```\n\nIn the `PERMISSIVE` mode it is possible to inspect the rows that could not be parsed correctly using one of the following methods:\n\n* You can provide a custom path to the option `badRecordsPath` to record corrupt records to a file.\n* You can add the column `_corrupt_record` to the schema provided to the DataFrameReader to review corrupt records in the resultant DataFrame.\n\nNote\n\nThe `badRecordsPath` option takes precedence over `_corrupt_record`, meaning that malformed rows written to the provided path do not appear in the resultant DataFrame.\n\nDefault behavior for malformed records changes when using the [rescued data column](#rescued-data).\n\n### Find malformed rows notebook\n\n[Open notebook in new tab](\/_extras\/notebooks\/source\/read-csv-corrupt-record.html)\n ![Copy to clipboard](\/_static\/clippy.svg) Copy link for import     \n\n## Rescued data column\n\nNote\n\nThis feature is supported in [Databricks Runtime 8.3 (unsupported)](..\/..\/archive\/runtime-release-notes\/8.3.html) and above.\n\nWhen using the `PERMISSIVE` mode, you can enable the rescued data column to capture any data that wasn\u2019t parsed because one or more fields in a record have one of the following issues:\n\n* Absent from the provided schema.\n* Does not match the data type of the provided schema.\n* Has a case mismatch with the field names in the provided schema.\n\nThe rescued data column is returned as a JSON document containing the columns that were rescued, and the source file path of the record. To remove the source file path from the rescued data column, you can set the SQL configuration `spark.conf.set(\"spark.databricks.sql.rescuedDataColumn.filePath.enabled\", \"false\")`. You can enable the rescued data column by setting the option `rescuedDataColumn` to a column name when reading data, such as `_rescued_data` with `spark.read.option(\"rescuedDataColumn\", \"_rescued_data\").format(\"csv\").load(<path>)`.\n\nThe CSV parser supports three modes when parsing records: `PERMISSIVE`, `DROPMALFORMED`, and `FAILFAST`. When used together with `rescuedDataColumn`, data type mismatches do not cause records to be dropped in `DROPMALFORMED` mode or throw an error in `FAILFAST` mode. Only corrupt records\u2014that is, incomplete or malformed CSV\u2014are dropped or throw errors.\n\nWhen `rescuedDataColumn` is used in `PERMISSIVE` mode, the following rules apply to [corrupt records](#corrupt-records):\n\n* The first row of the file (either a header row or a data row) sets the expected row length.\n* A row with a different number of columns is considered incomplete.\n* Data type mismatches are not considered corrupt records.\n* Only incomplete and malformed CSV records are considered corrupt and recorded to the `_corrupt_record` column or `badRecordsPath`.\n\n## SQL example: Read CSV file\n\nThe following SQL example reads a CSV file using `read_files`.\n\n```\n-- mode \"FAILFAST\" aborts file parsing with a RuntimeException if malformed lines are encountered\nSELECT * FROM read_files(\n  's3:\/\/<bucket>\/<path>\/<file>.csv',\n  format => 'csv',\n  header => true,\n  mode => 'FAILFAST')\n\n```\n\n## Scala, R, and Python examples: Read CSV file\n\nThe following notebook shows how to read a file, display sample data, and print the data schema using Scala, R, and Python. The examples in this section use the [diamonds dataset](..\/..\/discover\/databricks-datasets.html). Specify the path to the dataset as well as any options that you would like.\n\n### Read CSV files notebook\n\n[Open notebook in new tab](\/_extras\/notebooks\/source\/read-csv-files.html)\n ![Copy to clipboard](\/_static\/clippy.svg) Copy link for import     \n\n## Example: Specify schema\n\nWhen the schema of the CSV file is known, you can specify the desired schema to the CSV reader with the `schema` option.\n\n### Read CSV files with schema notebook\n\n[Open notebook in new tab](\/_extras\/notebooks\/source\/read-csv-schema.html)\n ![Copy to clipboard](\/_static\/clippy.svg) Copy link for import     \n\nSQL example using `read_files`:\n\n```\nSELECT * FROM read_files(\n  's3:\/\/<bucket>\/<path>\/<file>.csv',\n  format => 'csv',\n  header => false,\n  schema => 'id string, date date, event_time timestamp')\n\n```\n\n## Example: Pitfalls of reading a subset of columns\n\nThe behavior of the CSV parser depends on the set of columns that are read. If the specified schema is incorrect, the results might differ considerably depending on the subset of columns that is accessed. The following notebook presents the most common pitfalls.\n\n### Caveats of reading a subset of columns of a CSV file notebook\n\n[Open notebook in new tab](\/_extras\/notebooks\/source\/read-csv-column-subset.html)\n ![Copy to clipboard](\/_static\/clippy.svg) Copy link for import     \n\n---\n\n \u00a9 Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the [Apache Software Foundation](http:\/\/www.apache.org\/).\n \n\n[Send us feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n \n | [Privacy Policy](https:\/\/databricks.com\/privacy-policy) | [Terms of Use](https:\/\/databricks.com\/terms-of-use)\n\n { # 's' variable stands for the search page here. TODO: Fix it. #}\n \n [![DATA+AU Summit 2024](..\/..\/_static\/images\/dais\/docs-dais-promo-image.png)](https:\/\/www.databricks.com\/dataaisummit?utm_source=databricks&utm_medium=web&utm_campaign=7018y000001f7cvqai&utm_offer=docs \"DATA+AU Summit 2024\")"},{"doc_uri":"https:\/\/docs.databricks.com\/notebooks\/notebook-workflows.html","content":"Run a Databricks notebook from another notebook | Databricks on AWS\n\n* [![Databricks](..\/_static\/small-scale-lockup-full-color-rgb.svg)](https:\/\/www.databricks.com\/)\n\n* [![Databricks](..\/_static\/small-scale-lockup-full-color-rgb.svg)](https:\/\/www.databricks.com\/)\n* [Help Center](https:\/\/help.databricks.com\/s\/)\n* [Documentation](https:\/\/docs.databricks.com\/en\/index.html)\n* [Knowledge Base](https:\/\/kb.databricks.com\/)\n\n* [Community](https:\/\/community.databricks.com)\n* [Support](https:\/\/help.databricks.com)\n* [Feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n* [Try Databricks](https:\/\/databricks.com\/try-databricks)\n\n[![](..\/_static\/icons\/globe.png)English](javascript:void(0))\n* [English](..\/..\/en\/notebooks\/notebook-workflows.html)\n* [\u65e5\u672c\u8a9e](..\/..\/ja\/notebooks\/notebook-workflows.html)\n* [Portugu\u00eas](..\/..\/pt\/notebooks\/notebook-workflows.html)\n\n[![](..\/_static\/icons\/aws.svg)Amazon\n Web Services](javascript:void(0))\n* [![](..\/_static\/icons\/azure.svg)Microsoft Azure](https:\/\/learn.microsoft.com\/azure\/databricks\/notebooks\/notebook-workflows)\n* [![](..\/_static\/icons\/gcp.svg)Google Cloud Platform](https:\/\/docs.gcp.databricks.com\/notebooks\/notebook-workflows.html)\n\n[Databricks on AWS](..\/index.html)\nGet started\n\n* [Get started](..\/getting-started\/index.html)\n* [What is Databricks?](..\/introduction\/index.html)\n* [DatabricksIQ](..\/databricksiq\/index.html)\n* [Release notes](..\/release-notes\/index.html)\n\nLoad & manage data\n\n* [Connect to data sources](..\/connect\/index.html)\n* [Connect to compute](..\/compute\/index.html)\n* [Discover data](..\/discover\/index.html)\n* [Query data](..\/query\/index.html)\n* [Ingest data](..\/ingestion\/index.html)\n* [Transform data](..\/transform\/index.html)\n* [Monitor data and AI assets](..\/lakehouse-monitoring\/index.html)\n* [Share data (Delta Sharing)](..\/data-sharing\/index.html)\n* [Databricks Marketplace](..\/marketplace\/index.html)\n\nWork with data\n\n* [Data engineering](..\/workspace-index.html)\n\t+ [Delta Live Tables](..\/delta-live-tables\/index.html)\n\t+ [Structured Streaming](..\/structured-streaming\/index.html)\n\t+ [Apache Spark](..\/spark\/index.html)\n\t+ [Notebooks](index.html)\n\t\t- [Manage notebooks](notebooks-manage.html)\n\t\t- [Develop code in notebooks](notebooks-code.html)\n\t\t- [Schedule notebooks](schedule-notebook-jobs.html)\n\t\t- [Export and import notebooks](notebook-export-import.html)\n\t\t- [Notebook outputs and results](notebook-outputs.html)\n\t\t- [Notebook controls and appearance](notebook-ui.html)\n\t\t- [Use the interactive debugger](debugger.html)\n\t\t- [Collaborate using notebooks](notebooks-collaborate.html)\n\t\t- [Run notebooks](run-notebook.html)\n\t\t- [Share code between notebooks](share-code.html)\n\t\t- [Open or run a Delta Live Tables pipeline from a notebook](notebooks-dlt-pipeline.html)\n\t\t- [Dashboards in notebooks](dashboards.html)\n\t\t- [ipywidgets](ipywidgets.html)\n\t\t- [Databricks widgets](widgets.html)\n\t\t- [Use the Databricks notebook and file editor](notebook-editor.html)\n\t\t- [Run a notebook from another notebook](#)\n\t\t- [Package cells](package-cells.html)\n\t\t- [IPython kernel](ipython-kernel.html)\n\t\t- [Best practices](best-practices.html)\n\t\t- [Unit testing](testing.html)\n\t+ [Workflows](..\/workflows\/index.html)\n\t+ [Libraries](..\/libraries\/index.html)\n\t+ [Init scripts](..\/init-scripts\/index.html)\n\t+ [Git folders](..\/repos\/index.html)\n\t+ [DBFS](..\/dbfs\/index.html)\n\t+ [Files](..\/files\/index.html)\n\t+ [Migration](..\/migration\/index.html)\n\t+ [Optimization & performance](..\/optimizations\/index.html)\n* [Generative AI & LLMs](..\/generative-ai\/generative-ai.html)\n* [Machine learning](..\/machine-learning\/index.html)\n* [Model serving](..\/machine-learning\/model-serving\/index.html)\n* [Data warehousing](..\/sql\/index.html)\n* [Delta Lake](..\/delta\/index.html)\n* [Developers](..\/languages\/index.html)\n* [Technology partners](..\/integrations\/index.html)\n\nAdministration\n\n* [Account and workspace administration](..\/admin\/index.html)\n* [Security and compliance](..\/security\/index.html)\n* [Data governance (Unity Catalog)](..\/data-governance\/index.html)\n* [Lakehouse architecture](..\/lakehouse-architecture\/index.html)\n\nReference & resources\n\n* [Reference](..\/reference\/api.html)\n* [Resources](..\/resources\/index.html)\n* [What\u2019s coming?](..\/whats-coming.html)\n* [Documentation archive](..\/archive\/index.html)\n\n Updated May 28, 2024\n \n\n[Send us feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n\n* [Documentation](..\/index.html)\n* [Databricks data engineering](..\/workspace-index.html)\n* [Introduction to Databricks notebooks](index.html)\n* Run a Databricks notebook from another notebook\n* \n\n# Run a Databricks notebook from another notebook\n\nImportant\n\nFor notebook orchestration, use Databricks Jobs. For code modularization scenarios, use workspace files. You should only use the techniques described in this article when your use case cannot be implemented using a Databricks job, such as for looping notebooks over a dynamic set of parameters, or if you do not have access to [workspace files](..\/files\/workspace.html). For more information, see [Databricks Jobs](..\/workflows\/jobs\/create-run-jobs.html) and [share code](share-code.html).\n\n## Comparison of `%run` and `dbutils.notebook.run()`\n\nThe `%run` command allows you to include another notebook within a notebook. You can use `%run` to modularize your code, for example by putting supporting functions in a separate notebook. You can also use it to concatenate notebooks that implement the steps in an analysis. When you use `%run`, the called notebook is immediately executed and the functions and variables defined in it become available in the calling notebook.\n\nThe `dbutils.notebook` API is a complement to `%run` because it lets you pass parameters to and return values from a notebook. This allows you to build complex workflows and pipelines with dependencies. For example, you can get a list of files in a directory and pass the names to another notebook, which is not possible with `%run`. You can also create if-then-else workflows based on return values or call other notebooks using relative paths.\n\nUnlike `%run`, the `dbutils.notebook.run()` method starts a new job to run the notebook.\n\nThese methods, like all of the `dbutils` APIs, are available only in Python and Scala. However, you can use `dbutils.notebook.run()` to invoke an R notebook.\n\n## Use `%run` to import a notebook\n\nIn this example, the first notebook defines a function, `reverse`, which is available in the second notebook after you use the `%run` magic to execute `shared-code-notebook`.\n\n![Shared code notebook](..\/_images\/shared-code-notebook.png)\n\n![Notebook import example](..\/_images\/notebook-import-example.png)\n\nBecause both of these notebooks are in the same directory in the workspace, use the prefix `.\/` in `.\/shared-code-notebook` to indicate that the path should be resolved relative to the currently running notebook. You can organize notebooks into directories, such as `%run .\/dir\/notebook`, or use an absolute path like `%run \/Users\/username@organization.com\/directory\/notebook`.\n\nNote\n\n* `%run` must be in a cell *by itself*, because it runs the entire notebook inline.\n* You *cannot* use `%run` to run a Python file and `import` the entities defined in that file into a notebook. To import from a Python file, see [Modularize your code using files](share-code.html#reference-source-code-files-using-git). Or, package the file into a Python library, create a Databricks [library](..\/libraries\/index.html) from that Python library, and [install the library into the cluster](..\/libraries\/cluster-libraries.html#install-libraries) you use to run your notebook.\n* When you use `%run` to run a notebook that contains widgets, by default the specified notebook runs with the widget\u2019s default values. You can also pass in values to widgets; see [Use Databricks widgets with %run](widgets.html#widgets-and-percent-run).\n\n## `dbutils.notebook` API\n\nThe methods available in the `dbutils.notebook` API are `run` and `exit`. Both parameters and return values must be strings.\n\n**`run(path: String,\u00a0 timeout_seconds: int, arguments: Map): String`**\n\nRun a notebook and return its exit value. The method starts an ephemeral job that runs immediately.\n\nThe `timeout_seconds` parameter controls the timeout of the run (0 means no timeout): the call to\n`run` throws an exception if it doesn\u2019t finish within the specified time. If Databricks is down for more than 10 minutes,\nthe notebook run fails regardless of `timeout_seconds`.\n\nThe `arguments` parameter sets widget values of the target notebook. Specifically, if the notebook you are running has a widget\nnamed `A`, and you pass a key-value pair `(\"A\": \"B\")` as part of the arguments parameter to the `run()` call,\nthen retrieving the value of widget `A` will return `\"B\"`. You can find the instructions for creating and\nworking with widgets in the [Databricks widgets](widgets.html) article.\n\nNote\n\n* The `arguments` parameter accepts only Latin characters (ASCII character set). Using non-ASCII characters returns an error.\n* Jobs created using the `dbutils.notebook` API must complete in 30 days or less.\n\n### `run` Usage\n\n```\ndbutils.notebook.run(\"notebook-name\", 60, {\"argument\": \"data\", \"argument2\": \"data2\", ...})\n\n```\n\n```\ndbutils.notebook.run(\"notebook-name\", 60, Map(\"argument\" -> \"data\", \"argument2\" -> \"data2\", ...))\n\n```\n\n### `run` Example\n\nSuppose you have a notebook named `workflows` with a widget named `foo` that prints the widget\u2019s value:\n\n```\ndbutils.widgets.text(\"foo\", \"fooDefault\", \"fooEmptyLabel\")\nprint(dbutils.widgets.get(\"foo\"))\n\n```\n\nRunning `dbutils.notebook.run(\"workflows\", 60, {\"foo\": \"bar\"})` produces the following result:\n\n![Notebook with widget](..\/_images\/notebook-workflow-widget-example.png)\n\nThe widget had the value you passed in using `dbutils.notebook.run()`, `\"bar\"`, rather than the default.\n\n`exit(value: String): void`\nExit a notebook with a value. If you call a notebook using the `run` method, this is the value returned.\n\n```\ndbutils.notebook.exit(\"returnValue\")\n\n```\n\nCalling `dbutils.notebook.exit` in a job causes the notebook to complete successfully. If you want to cause the job to fail, throw an exception.\n\n## Example\n\nIn the following example, you pass arguments to `DataImportNotebook` and run different notebooks (`DataCleaningNotebook` or `ErrorHandlingNotebook`) based on the result from `DataImportNotebook`.\n\n![if-else example](..\/_images\/notebook-workflow-example.png)\n\nWhen the code runs, a table appears containing a link to the running notebook:\n\n![Link to running notebook](..\/_images\/dbutils.run.png)\n\nTo view the run details, click the **Start time** link in the table. If the run is complete, you can also view the run details by clicking the **End time** link.\n\n![Result of ephemeral notebook run](..\/_images\/notebook-run-results.png)\n\n## Pass structured data\n\nThis section illustrates how to pass structured data between notebooks.\n\n```\n# Example 1 - returning data through temporary views.\n# You can only return one string using dbutils.notebook.exit(), but since called notebooks reside in the same JVM, you can\n# return a name referencing data stored in a temporary view.\n\n## In callee notebook\nspark.range(5).toDF(\"value\").createOrReplaceGlobalTempView(\"my_data\")\ndbutils.notebook.exit(\"my_data\")\n\n## In caller notebook\nreturned_table = dbutils.notebook.run(\"LOCATION_OF_CALLEE_NOTEBOOK\", 60)\nglobal_temp_db = spark.conf.get(\"spark.sql.globalTempDatabase\")\ndisplay(table(global_temp_db + \".\" + returned_table))\n\n# Example 2 - returning data through DBFS.\n# For larger datasets, you can write the results to DBFS and then return the DBFS path of the stored data.\n\n## In callee notebook\ndbutils.fs.rm(\"\/tmp\/results\/my_data\", recurse=True)\nspark.range(5).toDF(\"value\").write.format(\"parquet\").save(\"dbfs:\/tmp\/results\/my_data\")\ndbutils.notebook.exit(\"dbfs:\/tmp\/results\/my_data\")\n\n## In caller notebook\nreturned_table = dbutils.notebook.run(\"LOCATION_OF_CALLEE_NOTEBOOK\", 60)\ndisplay(spark.read.format(\"parquet\").load(returned_table))"}]}
{"request_id":"id-3131","request":"\"What is the syntax for using the `list_secrets` table function in Databricks SQL?\"","response":"The `list_secrets` table function in Databricks SQL is used to return the keys which the user is authorized to see from the Databricks secret service. The syntax for using this function is as follows:\n\n```\nlist_secrets()\n```\n\nThis function takes no arguments and returns a table with two columns:\n\n* `scope`: The namespace of the key.\n* `key`: The key itself.\n\nTo return the secret values for specific keys, you can use the `secret(scope, key)` function.\n\nHere is an example of how to use the `list_secrets` function:\n\n```\nSELECT * FROM list_secrets();\n```\n\nThis will return a table with all the keys that the user is authorized to see.\n\nNote: The `list_secrets` function is only available in Databricks SQL and Databricks Runtime 11.3 LTS and above. It is not available in earlier versions of Databricks Runtime.","expected_retrieved_context":[{"doc_uri":"https:\/\/docs.databricks.com\/sql\/language-manual\/functions\/list_secrets.html","content":"list_secrets table function  \nApplies to: Databricks SQL preview Databricks Runtime 11.3 LTS and above  \nReturns the keys which the user is authorized to see from Databricks secret service.  \nSyntax\nSyntax\nlist_secrets()\n\nArguments\nArguments\nThis function takes no arguments.\n\nReturns\nReturns\nA table with two columns:  \nscope STRING NOT NULL: The namespace of the key.  \nkey STYRING NOT NULL: The key itself  \nTo return the secret values for specific keys use `secret(scope, key)`.\n\nError conditions\nError conditions\nINVALID_SECRET_LOOKUP.LIST_SECRETS_FAILED: Databricks secret service is not available.\n\nExamples\nExamples\n> SELECT * FROM list_secrets(); scope key ------------ --------------- secrets.r.us theAnswerToLife > SELECT secret('secrets.r.us', 'theAnswerToLife'); fourtyTwo\n\nRelated functions\nRelated functions\nlist_secrets table function"}],"expected_response":"The syntax for using the `list_secrets` table function in Databricks SQL is as follows:\n\n```sql\nlist_secrets()\n```\n\nThis function takes no arguments and returns a table with two columns: `scope` and `key`, which represent the namespace and the key of the secrets that the user is authorized to see from the Databricks secret service.","retrieved_context":[{"doc_uri":"https:\/\/docs.databricks.com\/sql\/language-manual\/functions\/list_secrets.html","content":"Updated May 28, 2024\n \n\n[Send us feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n\n* [Documentation](..\/..\/..\/index.html)\n* [Develop on Databricks](..\/..\/..\/languages\/index.html)\n* [SQL language reference](..\/index.html)\n* [Functions](..\/sql-ref-functions.html)\n* [Built-in functions](..\/sql-ref-functions-builtin.html)\n* [Alphabetical list of built-in functions](..\/sql-ref-functions-builtin-alpha.html)\n* `list_secrets` table function\n* \n\n# `list_secrets` table function\n\n**Applies to:** ![check marked yes](..\/..\/..\/_images\/check.png) Databricks SQL preview ![check marked yes](..\/..\/..\/_images\/check.png) Databricks Runtime 11.3 LTS and above\n\nReturns the keys which the user is authorized to see from [Databricks secret service](..\/..\/..\/security\/secrets\/index.html).\n\n## Syntax\n\n```\nlist_secrets()\n\n```\n\n## Arguments\n\nThis function takes no arguments.\n\n## Returns\n\nA table with two columns:\n\n* `scope STRING NOT NULL`: The namespace of the key.\n* `key STYRING NOT NULL`: The key itself\n\nTo return the secret values for specific keys use [`secret(scope, key)`](secret.html).\n\n## Error conditions\n\n* `INVALID_SECRET_LOOKUP.LIST_SECRETS_FAILED`: Databricks secret service is not available.\n\n## Examples\n\n```\n> SELECT * FROM list_secrets();\n  scope         key\n  ------------  ---------------\n  secrets.r.us  theAnswerToLife\n\n> SELECT secret('secrets.r.us', 'theAnswerToLife');\n  fourtyTwo\n\n```\n\n## Related functions\n\n* [list\\_secrets table function](#)\n\n---\n\n \u00a9 Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the [Apache Software Foundation](http:\/\/www.apache.org\/).\n \n\n[Send us feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n \n | [Privacy Policy](https:\/\/databricks.com\/privacy-policy) | [Terms of Use](https:\/\/databricks.com\/terms-of-use)\n\n { # 's' variable stands for the search page here. TODO: Fix it. #}\n \n [![DATA+AU Summit 2024](..\/..\/..\/_static\/images\/dais\/docs-dais-promo-image.png)](https:\/\/www.databricks.com\/dataaisummit?utm_source=databricks&utm_medium=web&utm_campaign=7018y000001f7cvqai&utm_offer=docs \"DATA+AU Summit 2024\")"},{"doc_uri":"https:\/\/docs.databricks.com\/error-messages\/invalid-secret-lookup-error-class.html","content":"Updated May 28, 2024\n \n\n[Send us feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n\n* [Documentation](..\/index.html)\n* [Databricks reference documentation](..\/reference\/api.html)\n* [Error handling in Databricks](index.html)\n* [Error classes in Databricks](error-classes.html)\n* INVALID\\_SECRET\\_LOOKUP error class\n* \n\n# INVALID\\_SECRET\\_LOOKUP error class\n\n[SQLSTATE: 22531](sqlstates.html#class-22-data-exception)\n\nInvalid secret lookup:\n\n## LIST\\_SECRETS\\_FAILED\n\nThe LIST\\_SECRETS table function failed to return the list of available secrets `<cause>`\n\n## SECRET\\_FUNCTION\\_KEY\\_NOT\\_CONSTANT\n\nThe SECRET function requires the secret key as a constant string expression passed in the second argument\n\n## SECRET\\_FUNCTION\\_LOOKUP\\_FAILED\n\nThe SECRET function failed to find a secret with scope `<scope>` and key `<key>` `<cause>`\n\n## SECRET\\_FUNCTION\\_NOT\\_SUPPORTED\n\nThe SECRET and LIST\\_SECRETS functions are not supported.\n\n## SECRET\\_FUNCTION\\_SCOPE\\_NOT\\_CONSTANT\n\nThe SECRET function requires the secret scope as a constant string expression passed in the first argument.\n\n---\n\n \u00a9 Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the [Apache Software Foundation](http:\/\/www.apache.org\/).\n \n\n[Send us feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n \n | [Privacy Policy](https:\/\/databricks.com\/privacy-policy) | [Terms of Use](https:\/\/databricks.com\/terms-of-use)\n\n { # 's' variable stands for the search page here. TODO: Fix it. #}\n \n [![DATA+AU Summit 2024](..\/_static\/images\/dais\/docs-dais-promo-image.png)](https:\/\/www.databricks.com\/dataaisummit?utm_source=databricks&utm_medium=web&utm_campaign=7018y000001f7cvqai&utm_offer=docs \"DATA+AU Summit 2024\")"},{"doc_uri":"https:\/\/docs.databricks.com\/sql\/language-manual\/functions\/secret.html","content":"Updated May 28, 2024\n \n\n[Send us feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n\n* [Documentation](..\/..\/..\/index.html)\n* [Develop on Databricks](..\/..\/..\/languages\/index.html)\n* [SQL language reference](..\/index.html)\n* [Functions](..\/sql-ref-functions.html)\n* [Built-in functions](..\/sql-ref-functions-builtin.html)\n* [Alphabetical list of built-in functions](..\/sql-ref-functions-builtin-alpha.html)\n* `secret` function\n* \n\n# `secret` function\n\n**Applies to:** ![check marked yes](..\/..\/..\/_images\/check.png) Databricks SQL preview ![check marked yes](..\/..\/..\/_images\/check.png) Databricks Runtime 11.3 LTS and above\n\nExtracts a secret value with the given `scope` and `key` from [Databricks secret service](..\/..\/..\/security\/secrets\/index.html).\n\n## Syntax\n\n```\nsecret ( scope, key )\n\n```\n\n## Arguments\n\n* `scope`: A constant string expression containing the scope of the secret to be extracted. The scope is the namespace in which multiple keys might reside.\n* `key`: A constant string expression with the key of the secret to be extracted.\n\n## Returns\n\nA `STRING`.\n\nTo retrieve a list of all secrets the user is authorized to see use the [list\\_secrets](list_secrets.html) function.\n\n## Error conditions\n\n* `INVALID_SECRET_LOOKUP.SECRET_FUNCTION_KEY_NOT_CONSTANT`: The `key` is not a constant string.\n* `INVALID_SECRET_LOOKUP.SECRET_FUNCTION_SCOPE_NOT_CONSTANT`: The `scope` is not a constant string.\n* `INVALID_SECRET_LOOKUP.SECRET_FUNCTION_NOT_SUPPORTED`: Databricks secret service is not available.\n* `INVALID_SECRET_LOOKUP.SECRET_FUNCTION_LOOKUP_FAILED`: The `key` cannot be found within `scope`.\n\n## Examples\n\n```\n> SELECT * FROM list_secrets();\n  scope         key\n  ------------  ---------------\n  secrets.r.us  theAnswerToLife\n\n> SELECT secret('secrets.r.us', 'theAnswerToLife');\n  fourtyTwo\n\n```\n\n## Related functions\n\n* [list\\_secrets table function](list_secrets.html)\n\n---\n\n \u00a9 Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the [Apache Software Foundation](http:\/\/www.apache.org\/).\n \n\n[Send us feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n \n | [Privacy Policy](https:\/\/databricks.com\/privacy-policy) | [Terms of Use](https:\/\/databricks.com\/terms-of-use)\n\n { # 's' variable stands for the search page here. TODO: Fix it. #}\n \n [![DATA+AU Summit 2024](..\/..\/..\/_static\/images\/dais\/docs-dais-promo-image.png)](https:\/\/www.databricks.com\/dataaisummit?utm_source=databricks&utm_medium=web&utm_campaign=7018y000001f7cvqai&utm_offer=docs \"DATA+AU Summit 2024\")"},{"doc_uri":"https:\/\/docs.databricks.com\/sql\/language-manual\/sql-ref-syntax-ddl-create-table-using.html","content":"Updated May 28, 2024\n \n\n[Send us feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n\n* [Documentation](..\/..\/index.html)\n* [Develop on Databricks](..\/..\/languages\/index.html)\n* [SQL language reference](index.html)\n* [CREATE TABLE](sql-ref-syntax-ddl-create-table.html)\n* CREATE TABLE [USING]\n* \n\n# CREATE TABLE [USING]\n\n**Applies to:** ![check marked yes](..\/..\/_images\/check.png) Databricks SQL ![check marked yes](..\/..\/_images\/check.png) Databricks Runtime\n\nDefines a managed or [external table](sql-ref-external-tables.html), optionally using a data source.\n\n## Syntax\n\n```\n{ { [CREATE OR] REPLACE TABLE | CREATE [EXTERNAL] TABLE [ IF NOT EXISTS ] }\n  table_name\n  [ table_specification ]\n  [ USING data_source ]\n  [ table_clauses ]\n  [ AS query ] }\n\ntable_specification\n  ( { column_identifier column_type [ column_properties ] } [, ...]\n    [ , table_constraint ] [...] )\n\ncolumn_properties\n  { NOT NULL |\n    GENERATED ALWAYS AS ( expr ) |\n    GENERATED { ALWAYS | BY DEFAULT } AS IDENTITY [ ( [ START WITH start ] [ INCREMENT BY step ] ) ] |\n    DEFAULT default_expression |\n    COMMENT column_comment |\n    column_constraint |\n    MASK clause } [ ... ]\n\ntable_clauses\n  { OPTIONS clause |\n    PARTITIONED BY clause |\n    CLUSTER BY clause |\n    clustered_by_clause |\n    LOCATION path [ WITH ( CREDENTIAL credential_name ) ] |\n    COMMENT table_comment |\n    TBLPROPERTIES clause |\n    WITH { ROW FILTER clause } } [...]\n\nclustered_by_clause\n  { CLUSTERED BY ( cluster_column [, ...] )\n    [ SORTED BY ( { sort_column [ ASC | DESC ] } [, ...] ) ]\n    INTO num_buckets BUCKETS }\n\n```\n\n## Parameters\n\n* **REPLACE**\n\nIf specified replaces the table and its content if it already exists.\nThis clause is only supported for Delta Lake tables.\n\n`REPLACE` preserves the [table history](..\/..\/delta\/history.html).\n\nNote\n\nDatabricks strongly recommends using `REPLACE` instead of dropping and re-creating Delta Lake tables.\n* **EXTERNAL**\n\nIf specified, creates an [external table](..\/..\/data-governance\/unity-catalog\/create-tables.html#external-tables).\nWhen creating an external table you must also provide a `LOCATION` clause.\nWhen an external table is dropped the files at the `LOCATION` will not be dropped.\n* **IF NOT EXISTS**\n\nIf specified and a table with the same name already exists, the statement is ignored.\n\n`IF NOT EXISTS` cannot coexist with `REPLACE`, which means `CREATE OR REPLACE TABLE IF NOT EXISTS` is not allowed.\n* **[table\\_name](sql-ref-names.html#table-name)**\n\nThe name of the table to be created. The name must not include a [temporal specification](sql-ref-names.html#table-name).\nIf the name is not qualified the table is created in the current schema.\n\nTables created in `hive_metastore` can only contain alphanumeric ASCII characters and underscores ([INVALID\\_SCHEMA\\_OR\\_RELATION\\_NAME](..\/..\/error-messages\/error-classes.html#invalid_schema_or_relation_name)).\n* **table\\_specification**\n\nThis optional clause defines the list of columns, their types, properties, descriptions, and column constraints.\n\nIf you do not define columns the table schema you must specify either `AS query` or `LOCATION`.\n\n\t+ **[column\\_identifier](sql-ref-identifiers.html)**\n\t\n\t\n\tA unique name for the column.\n\t\n\t\n\tColumn identifiers of Delta Lake tables without column mapping property (`'delta.columnMapping.mode' = 'name'`) must not contain the characters \u2018 \u2018 (space), \u2018,\u2019, \u2018;\u2019, \u2018{\u2018, \u2018}\u2019, \u2018(\u2018, \u2018)\u2019. \u2018n\u2019, \u2018t\u2019, and \u2018=\u2019.\n\t\n\t\n\tColumn identifiers of `AVRO` table must start with \u2018*\u2019 or a Unicode letter (including non-ASCII letters) and be followed by a combination of \u2018*\u2019, Unicode letters and digits.\n\t+ **[column\\_type](sql-ref-datatypes.html)**\n\t\n\t\n\tSpecifies the data type of the column.\n\tNot all [data types](sql-ref-datatypes.html) supported by Databricks are supported by all data sources.\n\t+ **NOT NULL**\n\t\n\t\n\tIf specified the column will not accept `NULL` values. This clause is only supported for Delta Lake tables.\n\t+ **GENERATED ALWAYS AS ( [expr](sql-ref-expression.html) )**\n\t\n\t\n\tWhen you specify this clause the value of this column is determined by the specified `expr`.\n\t\n\t\n\t`expr` may be composed of literals, column identifiers within the table, and deterministic, built-in SQL functions or operators except:\n\t\n\t\n\t\t- [Aggregate functions](sql-ref-functions-builtin.html#aggregate-functions)\n\t\t- [Analytic window functions](sql-ref-functions-builtin.html#analytic-window-functions)\n\t\t- [Ranking window functions](sql-ref-functions-builtin.html#ranking-window-functions)\n\t\t- Table valued generator functionsAlso `expr` must not contain any [subquery](sql-ref-syntax-qry-query.html).\n\t+ **GENERATED { ALWAYS | BY DEFAULT } AS IDENTITY [ ( [ START WITH start ] [ INCREMENT BY step ] ) ]**\n\t\n\t\n\t**Applies to:** ![check marked yes](..\/..\/_images\/check.png) Databricks SQL ![check marked yes](..\/..\/_images\/check.png) Databricks Runtime 10.4 LTS and above\n\t\n\t\n\tDefines an identity column. When you write to the table, and do not provide values for the identity column, it will be automatically assigned a unique and statistically increasing (or decreasing if `step` is negative) value. This clause is only supported for Delta Lake tables. This clause can only be used for columns with BIGINT data type.\n\t\n\t\n\tThe automatically assigned values start with `start` and increment by `step`. Assigned values are unique but are not guaranteed to be contiguous. Both parameters are optional, and the default value is 1. `step` cannot be `0`.\n\t\n\t\n\tIf the automatically assigned values are beyond the range of the identity column type, the query will fail.\n\t\n\t\n\tWhen `ALWAYS` is used, you cannot provide your own values for the identity column.\n\t\n\t\n\tThe following operations are not supported:\n\t\n\t\n\t\t- `PARTITIONED BY` an identity column\n\t\t- `UPDATE` an identity column\n\tNote\n\t\n\t\n\tDeclaring an identity column on a Delta table disables concurrent transactions. Only use identity columns in use cases where concurrent writes to the target table are not required.\n\t+ **DEFAULT default\\_expression**\n\t\n\t\n\t**Applies to:** ![check marked yes](..\/..\/_images\/check.png) Databricks SQL ![check marked yes](..\/..\/_images\/check.png) Databricks Runtime 11.3 LTS and above\n\t\n\t\n\tDefines a `DEFAULT` value for the column which is used on `INSERT`, `UPDATE`, and `MERGE ... INSERT` when the column is not specified.\n\t\n\t\n\tIf no default is specified `DEFAULT NULL` is applied for nullable columns.\n\t\n\t\n\t`default_expression` may be composed of literals, and built-in SQL functions or operators except:\n\t\n\t\n\t\t- [Aggregate functions](sql-ref-functions-builtin.html#aggregate-functions)\n\t\t- [Analytic window functions](sql-ref-functions-builtin.html#analytic-window-functions)\n\t\t- [Ranking window functions](sql-ref-functions-builtin.html#ranking-window-functions)\n\t\t- Table valued generator functionsAlso `default_expression` must not contain any [subquery](sql-ref-syntax-qry-query.html).\n\t\n\t\n\t`DEFAULT` is supported for `CSV`, `JSON`, `PARQUET`, and `ORC` sources.\n\t+ **COMMENT column\\_comment**\n\t\n\t\n\tA string literal to describe the column.\n\t+ **[column\\_constraint](sql-ref-syntax-ddl-create-table-constraint.html)**\n\t\n\t\n\tAdds a primary key or foreign key constraint to the column in a Delta Lake table.\n\t\n\t\n\tConstraints are not supported for tables in the `hive_metastore` catalog.\n\t\n\t\n\tTo add a check constraint to a Delta Lake table use [ALTER TABLE](sql-ref-syntax-ddl-alter-table.html).\n\t+ **[MASK clause](sql-ref-syntax-ddl-column-mask.html)**\n\t\n\t\n\t**Applies to:** ![check marked yes](..\/..\/_images\/check.png) Databricks SQL ![check marked yes](..\/..\/_images\/check.png) Databricks Runtime 12.2 LTS and above ![check marked yes](..\/..\/_images\/check.png) Unity Catalog only\n\t\n\t\n\t\n\tPreview\n\t\n\t\n\tThis feature is in [Public Preview](..\/..\/release-notes\/release-types.html).\n\t\n\t\n\t\n\tAdds a column mask function to anonymize sensitive data.\n\tAll future queries from that column will receive the result of evaluating\n\tthat function over the column in place of the column\u2019s original value.\n\tThis can be useful for fine-grained access control purposes wherein the function\n\tcan inspect the identity and\/or group memberships of the invoking user in order\n\tto decide whether to redact the value.\n\t+ **[table\\_constraint](sql-ref-syntax-ddl-create-table-constraint.html)**\n\t\n\t\n\tAdds an informational primary key or informational foreign key constraints to the Delta Lake table.\n\t\n\t\n\tKey constraints are not supported for tables in the `hive_metastore` catalog.\n\t\n\t\n\tTo add a check constraint to a Delta Lake table use [ALTER TABLE](sql-ref-syntax-ddl-alter-table.html).\n* **USING data\\_source**\n\nThe file format to use for the table. `data_source` must be one of:\n\n\t+ `AVRO`\n\t+ `BINARYFILE`\n\t+ `CSV`\n\t+ `DELTA`\n\t+ `JSON`\n\t+ `ORC`\n\t+ `PARQUET`\n\t+ `TEXT`The following additional file formats to use for the table are supported in Databricks Runtime:\n\n\t+ `JDBC`\n\t+ `LIBSVM`\n\t+ a fully-qualified class name of a custom implementation of `org.apache.spark.sql.sources.DataSourceRegister`.If `USING` is omitted, the default is `DELTA`.\n\nFor any `data_source` other than `DELTA` you must also specify a `LOCATION` unless the table catalog is `hive_metastore`.\n\n**The following applies to:** Databricks Runtime\n\n`HIVE` is supported to create a Hive SerDe table in Databricks Runtime.\nYou can specify the Hive-specific `file_format` and `row_format` using the `OPTIONS` clause, which is a case-insensitive string map.\nThe `option_keys` are:\n\n\t+ `FILEFORMAT`\n\t+ `INPUTFORMAT`\n\t+ `OUTPUTFORMAT`\n\t+ `SERDE`\n\t+ `FIELDDELIM`\n\t+ `ESCAPEDELIM`\n\t+ `MAPKEYDELIM`\n\t+ `LINEDELIM`\n* **table\\_clauses**\n\nOptionally specify location, partitioning, clustering, options, comments, and user defined properties for the new table.\nEach sub clause may only be specified once.\n\n\t+ **[PARTITIONED BY](sql-ref-partition.html#partitioned-by)**\n\t\n\t\n\tAn optional clause to partition the table by a subset of columns.\n\t\n\t\n\t\n\tNote\n\t\n\t\n\tIf you don\u2019t define a Delta table, partitioning columns are placed at the end of the table, even if they are defined earlier in the column specification. Consider using `CLUSTER BY` instead of `PARTITIONED BY` for Delta tables.\n\t+ **[CLUSTER BY](sql-ref-syntax-ddl-cluster-by.html)**\n\t\n\t\n\tApplies to: ![check marked yes](..\/..\/_images\/check.png) Databricks SQL ![check marked yes](..\/..\/_images\/check.png) Databricks Runtime 13.3 and later\n\t\n\t\n\tAn optional clause to cluster a Delta table by a subset of columns.\n\tTo cluster other tables use `clustered_by_clause`.\n\t\n\t\n\tDelta Lake liquid clustering cannot be combined with `PARTITIONED BY`.\n\t+ **clustered\\_by\\_clause**\n\t\n\t\n\tOptionally cluster the table or each partition into a fixed number of hash buckets using a subset of the columns.\n\t\n\t\n\tClustering is not supported for Delta Lake tables.\n\t\n\t\n\t\t- **CLUSTERED BY**\n\t\t\n\t\t\n\t\tSpecifies the set of columns by which to cluster each partition, or the table if no partitioning is specified.\n\t\t\n\t\t\n\t\t\t* **[cluster\\_column](sql-ref-identifiers.html)**\n\t\t\t\n\t\t\t\n\t\t\tAn identifier referencing a `column_identifier` in the table. If you specify more than one column there must be no duplicates.\n\t\t\tSince a clustering operates on the partition level you must not name a partition column also as a cluster column.\n\t\t- **SORTED BY**\n\t\t\n\t\t\n\t\tOptionally maintains a sort order for rows in a bucket.\n\t\t\n\t\t\n\t\t\t* **sort\\_column**\n\t\t\t\n\t\t\t\n\t\t\tA column to sort the bucket by. The column must not be partition column.\n\t\t\tSort columns must be unique.\n\t\t\t* **ASC** or **DESC**\n\t\t\t\n\t\t\t\n\t\t\tOptionally specifies whether `sort_column` is sorted in ascending (`ASC`) or descending (`DESC`) order.\n\t\t\tThe default values is `ASC`.\n\t\t- **INTO num\\_buckets BUCKETS**\n\t\t\n\t\t\n\t\tAn INTEGER literal specifying the number of buckets into which each partition (or the table if no partitioning is specified) is divided.\n\t+ **LOCATION path [ WITH ( CREDENTIAL [credential\\_name](sql-ref-names.html#credential-name) ) ]**\n\t\n\t\n\tAn optional path to the directory where table data is stored, which could be a path on distributed storage.\n\t`path` must be a STRING literal.\n\tIf you specify no location the table is considered a `managed table` and Databricks creates a default table location.\n\t\n\t\n\tSpecifying a location makes the table an [external table](..\/..\/data-governance\/unity-catalog\/create-tables.html#external-tables).\n\t\n\t\n\tFor tables that do not reside in the `hive_metastore` catalog, the table `path` must be protected by an [external location](sql-ref-external-locations.html) unless a valid [storage credential](sql-ref-storage-credentials.html) is specified.\n\t\n\t\n\tYou cannot create external tables in locations that overlap with the location of [managed tables](..\/..\/data-governance\/unity-catalog\/create-tables.html#managed-table).\n\t\n\t\n\tFor a Delta Lake table the table configuration is inherited from the `LOCATION` if data is present.\n\tTherefore, if any `TBLPROPERTIES`, `table_specification`, or `PARTITIONED BY` clauses are specified for Delta Lake tables they must exactly match the Delta Lake location data.\n\t+ **[OPTIONS](sql-ref-syntax-ddl-tblproperties.html#options)**\n\t\n\t\n\tSets or resets one or more user defined table options.\n\t+ **COMMENT table\\_comment**\n\t\n\t\n\tA string literal to describe the table.\n\t+ **[TBLPROPERTIES](sql-ref-syntax-ddl-tblproperties.html#tblproperties)**\n\t\n\t\n\tOptionally sets one or more user defined properties.\n\t+ **WITH [ROW FILTER clause](sql-ref-syntax-ddl-row-filter.html)**\n\t\n\t\n\t**Applies to:** ![check marked yes](..\/..\/_images\/check.png) Databricks SQL ![check marked yes](..\/..\/_images\/check.png) Databricks Runtime 12.2 LTS and above ![check marked yes](..\/..\/_images\/check.png) Unity Catalog only\n\t\n\t\n\t\n\tPreview\n\t\n\t\n\tThis feature is in [Public Preview](..\/..\/release-notes\/release-types.html).\n\t\n\t\n\t\n\tAdds a row filter function to the table.\n\tAll future queries from that table will receive subset of its rows\n\tfor which the function evaluates to boolean TRUE.\n\tThis can be useful for fine-grained access control purposes wherein\n\tthe function can inspect the identity and\/or group memberships of\n\tthe invoking user in order to decide whether to filter certain rows.\n* **AS [query](sql-ref-syntax-qry-query.html)**\n\nThis optional clause populates the table using the data from `query`.\nWhen you specify a `query` you must not also specify a `table_specification`. The table schema is derived from the query."},{"doc_uri":"https:\/\/docs.databricks.com\/sql\/language-manual\/functions\/list_secrets.html","content":"list\\_secrets table function | Databricks on AWS\n\n* [![Databricks](..\/..\/..\/_static\/small-scale-lockup-full-color-rgb.svg)](https:\/\/www.databricks.com\/)\n\n* [![Databricks](..\/..\/..\/_static\/small-scale-lockup-full-color-rgb.svg)](https:\/\/www.databricks.com\/)\n* [Help Center](https:\/\/help.databricks.com\/s\/)\n* [Documentation](https:\/\/docs.databricks.com\/en\/index.html)\n* [Knowledge Base](https:\/\/kb.databricks.com\/)\n\n* [Community](https:\/\/community.databricks.com)\n* [Support](https:\/\/help.databricks.com)\n* [Feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n* [Try Databricks](https:\/\/databricks.com\/try-databricks)\n\n[![](..\/..\/..\/_static\/icons\/globe.png)English](javascript:void(0))\n* [English](..\/..\/..\/..\/en\/sql\/language-manual\/functions\/list_secrets.html)\n* [\u65e5\u672c\u8a9e](..\/..\/..\/..\/ja\/sql\/language-manual\/functions\/list_secrets.html)\n* [Portugu\u00eas](..\/..\/..\/..\/pt\/sql\/language-manual\/functions\/list_secrets.html)\n\n[![](..\/..\/..\/_static\/icons\/aws.svg)Amazon\n Web Services](javascript:void(0))\n* [![](..\/..\/..\/_static\/icons\/azure.svg)Microsoft Azure](https:\/\/learn.microsoft.com\/azure\/databricks\/sql\/language-manual\/functions\/list_secrets)\n* [![](..\/..\/..\/_static\/icons\/gcp.svg)Google Cloud Platform](https:\/\/docs.gcp.databricks.com\/sql\/language-manual\/functions\/list_secrets.html)\n\n[Databricks on AWS](..\/..\/..\/index.html)\nGet started\n\n* [Get started](..\/..\/..\/getting-started\/index.html)\n* [What is Databricks?](..\/..\/..\/introduction\/index.html)\n* [DatabricksIQ](..\/..\/..\/databricksiq\/index.html)\n* [Release notes](..\/..\/..\/release-notes\/index.html)\n\nLoad & manage data\n\n* [Connect to data sources](..\/..\/..\/connect\/index.html)\n* [Connect to compute](..\/..\/..\/compute\/index.html)\n* [Discover data](..\/..\/..\/discover\/index.html)\n* [Query data](..\/..\/..\/query\/index.html)\n* [Ingest data](..\/..\/..\/ingestion\/index.html)\n* [Transform data](..\/..\/..\/transform\/index.html)\n* [Monitor data and AI assets](..\/..\/..\/lakehouse-monitoring\/index.html)\n* [Share data (Delta Sharing)](..\/..\/..\/data-sharing\/index.html)\n* [Databricks Marketplace](..\/..\/..\/marketplace\/index.html)\n\nWork with data\n\n* [Data engineering](..\/..\/..\/workspace-index.html)\n* [Generative AI & LLMs](..\/..\/..\/generative-ai\/generative-ai.html)\n* [Machine learning](..\/..\/..\/machine-learning\/index.html)\n* [Model serving](..\/..\/..\/machine-learning\/model-serving\/index.html)\n* [Data warehousing](..\/..\/index.html)\n* [Delta Lake](..\/..\/..\/delta\/index.html)\n* [Developers](..\/..\/..\/languages\/index.html)\n* [Technology partners](..\/..\/..\/integrations\/index.html)\n\nAdministration\n\n* [Account and workspace administration](..\/..\/..\/admin\/index.html)\n* [Security and compliance](..\/..\/..\/security\/index.html)\n* [Data governance (Unity Catalog)](..\/..\/..\/data-governance\/index.html)\n* [Lakehouse architecture](..\/..\/..\/lakehouse-architecture\/index.html)\n\nReference & resources"}]}
{"request_id":"id-4533","request":"How can I migrate init scripts from DBFS to a supported location like Unity Catalog Volumes, workspace files, or cloud object storage?","response":"To migrate init scripts from DBFS to a supported location like Unity Catalog Volumes, you can follow these steps:\n\n1. Create a new Unity Catalog and a volume within it.\n2. Copy the init scripts from the DBFS location to the new Unity Catalog volume. You can use the Databricks CLI or the DBFS API to copy the files.\n3. Update the init scripts to reference the new location in Unity Catalog.\n4. Test the init scripts to ensure they are working as expected.\n5. Update any references to the old DBFS location in your workspaces or jobs to point to the new Unity Catalog volume.\n\nIt's important to note that Unity Catalog provides a more secure and governed way to manage data and scripts, so it's recommended to use Unity Catalog volumes instead of DBFS for storing init scripts.","expected_retrieved_context":[{"doc_uri":"https:\/\/docs.databricks.com\/init-scripts\/index.html","content":"What are init scripts?  \nAn init script (initialization script) is a shell script that runs during startup of each cluster node before the Apache Spark driver or executor JVM starts. This article provides recommendations for init scripts and configuration information if you must use them.  \nRecommendations for init scripts\nRecommendations for init scripts\nDatabricks recommends using built-in platform features instead of init scripts whenever possible. Widespread use of init scripts can slow migration to new Databricks Runtime versions and prevent adoption of some Databricks optimizations.  \nImportant  \nIf you need to migrate from init scripts on DBFS, see Migrate init scripts from DBFS.  \nThe following Databricks features address some of the common use cases for init scripts:  \nUse compute policies to set system properties, environmental variables, and Spark configuration parameters. See Compute policy reference.  \nAdd libraries to cluster policies. See Add libraries to a policy.  \nIf you must use init scripts:  \nManage init scripts using compute policies or cluster-scoped init scripts rather than global init scripts. See init script types.  \nManage library installation for production and interactive environments using compute policies. Don\u2019t install libraries using init scripts.  \nUse shared access mode for all workloads. Only use the single user access mode if required functionality is not supported by shared access mode.  \nUse new Databricks Runtime versions and Unity Catalog for all workloads.  \nThe following table provides recommendations organized by Databricks Runtime version and Unity Catalog enablement.  \nEnvironment  \nRecommendation  \nDatabricks Runtime 13.3 LTS and above with Unity Catalog  \nStore init scripts in Unity Catalog volumes.  \nDatabricks Runtime 11.3 LTS and above without Unity Catalog  \nStore init scripts as workspace files. (File size limit is 500 MB).  \nDatabricks Runtime 10.4 LTS and below  \nStore init scripts using cloud object storage.\n\nWhat types of init scripts does Databricks support?\nWhat types of init scripts does Databricks support?\nDatabricks supports two kinds of init scripts: cluster-scoped and global, but using cluster-scoped init scripts are recommended.  \nCluster-scoped: run on every cluster configured with the script. This is the recommended way to run an init script. See Use cluster-scoped init scripts.  \nGlobal: run on all clusters in the workspace configured with single user access mode or no-isolation shared access mode. These init scripts can cause unexpected issues, such as library conflicts. Only workspace admin users can create global init scripts. See Use global init scripts.  \nWhenever you change any type of init script, you must restart all clusters affected by the script.  \nGlobal init-scripts run before cluster-scoped init scripts.  \nImportant  \nLegacy global and legacy cluster-named init scripts run before other init scripts. These init scripts are end-of-life, but might be present in workspaces created before February 21, 2023. See Cluster-named init scripts (legacy) and Global init scripts (legacy).\n\nWhere can init scripts be installed?\nWhere can init scripts be installed?\nYou can store and configure init scripts from workspace files, Unity Catalog volumes, and cloud object storage, but init scripts are not supported on all cluster configurations and not all files can be referenced from init scripts.  \nThe following table indicates the support for init scripts based on the source location and the cluster access mode. The Databricks Runtime version listed is the minimum version required to use the combination. For information about cluster access modes, see Access modes.  \nNote  \nShared access mode requires an admin to add init scripts to an allowlist. See Allowlist libraries and init scripts on shared compute.  \nShared access mode  \nSingle access mode  \nNo-isolation shared access mode  \nWorkspace files  \nNot supported  \nAll supported Databricks Runtime versions  \nAll supported Databricks Runtime versions  \nVolumes  \n13.3 LTS  \n13.3 LTS  \nNot supported  \nCloud storage  \n13.3 LTS  \nAll supported Databricks Runtime versions  \nAll supported Databricks Runtime versions\n\nMigrate init scripts from DBFS\nMigrate init scripts from DBFS\nWarning  \nInit scripts on DBFS have reached end-of-life and can no longer be used. You must migrate your init scripts to a supported location before you can start compute. Store init scripts on Unity Catalog Volumes, as workspace files, or in cloud object storage.  \nUsers that need to migrate init scripts from DBFS can use the following guides. Make sure you\u2019ve identified the correct target for your configuration. See Recommendations for init scripts.  \nMigrate init scripts from DBFS to volumes  \nMigrate init scripts from DBFS to workspace files  \nMigrate init scripts from DBFS to S3"}],"expected_response":"To migrate init scripts from DBFS to a supported location like Unity Catalog Volumes, workspace files, or cloud object storage, you can follow these steps:\n\n1. Identify the init scripts that need to be migrated from DBFS.\n2. Depending on the Databricks Runtime version and Unity Catalog enablement, choose the appropriate target location for the init scripts. For Databricks Runtime 13.3 LTS and above with Unity Catalog, store init scripts in Unity Catalog volumes. For Databricks Runtime 11.3 LTS and above without Unity Catalog, store init scripts as workspace files. For Databricks Runtime 10.4 LTS and below, store init scripts using cloud object storage.\n3. Migrate the init scripts from DBFS to the chosen target location. You can use the following guides to help with the migration:\n\t* Migrate init scripts from DBFS to volumes\n\t* Migrate init scripts from DBFS to workspace files\n\t* Migrate init scripts from DBFS to S3\n\nRemember to update the cluster configurations to point to the new location of the init scripts after migration. Also, make sure to restart all clusters affected by the script change.","retrieved_context":[{"doc_uri":"https:\/\/docs.databricks.com\/archive\/dev-tools\/cli\/unity-catalog-cli.html","content":"Option and field sample values in the following examples are for illustrative purposes only.\n\nUse the Unity Catalog CLI to work with:\n\n* [Unity Catalog](..\/..\/..\/data-governance\/unity-catalog\/index.html) resources such as metastores, storage credentials, external locations, catalogs, schemas, tables, and their permissions.\n* [Delta Sharing](..\/..\/..\/data-sharing\/index.html) resources such as shares, recipients, and providers.\n\nYou run Unity Catalog CLI subcommands by appending them to `databricks unity-catalog`. These subcommands call the [Unity Catalog API](https:\/\/api-docs.databricks.com\/rest\/latest\/unity-catalog-api-specification-2-1.html), which also includes the Delta Sharing API.\n\nTo display usage documentation, run `databricks unity-catalog --help`.\n\nOutput:\n\n```\nUsage: databricks unity-catalog [OPTIONS] COMMAND [ARGS]...\n\n  Utility to interact with Databricks Unity Catalog.\n\nOptions:\n  -v, --version  0.17.1\n  -h, --help     Show this message and exit.\n\nCommands:\n  catalogs\n  external-locations\n  lineage\n  metastores\n  permissions\n  providers\n  recipients\n  schemas\n  shares\n  storage-credentials\n  tables\n\n```\n\n## Metastores\n\nUse the `unity-catalog metastores` subcommand to work with [metastores](..\/..\/..\/data-governance\/unity-catalog\/index.html#metastore).\n\nTo display usage documentation, run `databricks unity-catalog metastores --help`.\n\n* [Create a metastore](#create-a-metastore)\n* [Link a metastore with a workspace](#link-a-metastore-with-a-workspace)\n* [Update a metastore\u2019s settings](#update-a-metastores-settings)\n* [List available metastores](#list-available-metastores)\n* [Get information about a metastore](#get-information-about-a-metastore)\n* [Get summarized information about the current metastore](#get-summarized-information-about-the-current-metastore)\n* [Get the current metastore assignment for a workspace](#get-the-current-metastore-assignment-for-a-workspace)\n* [Unlink a metastore from a workspace](#unlink-a-metastore-from-a-workspace)\n* [Delete a metastore](#delete-a-metastore)\n\n### [Create a metastore](#id16)\n\nTo display usage documentation, run `databricks unity-catalog metastores create --help`. See also [Create a Unity Catalog metastore](..\/..\/..\/data-governance\/unity-catalog\/create-metastore.html).\n\n```\ndatabricks unity-catalog metastores create --name my-metastore \\\n                                           --region us-west-2 \\\n                                           --storage-root s3:\/\/my-bucket\/abcdefghij\n\n```\n\nIf you do not specify the region, the metastore is created in your current workspace region.\n\nOutput:\n\n```\n{\n  \"name\": \"my-metastore\",\n  \"storage_root\": \"s3:\/\/my-bucket\/abcdefghij\/12a345b6-7890-1cd2-3456-e789f0a12b34\",\n  \"metastore_id\": \"12a345b6-7890-1cd2-3456-e789f0a12b34\",\n  \"created_at\": 1656112029354,\n  \"created_by\": \"someone@example.com\",\n  \"updated_at\": 1656112029354,\n  \"updated_by\": \"someone@example.com\",\n  \"delta_sharing_enabled\": false,\n  \"owner\": \"someone@example.com\",\n  \"region\": \"us-west-2\",\n  \"cloud\": \"aws\",\n  \"global_metastore_id\": \"aws:us-west-2:12a345b6-7890-1cd2-3456-e789f0a12b34\",\n  \"delta_sharing_scope\": \"INTERNAL\",\n  \"privilege_model_version\": \"0.1\"\n}\n\n```\n\nNote\n\nCreating a metastore does not automatically associate the metastore with a workspace or a storage credential. To complete these tasks, see [Link a metastore with a workspace](#link-a-metastore-with-a-workspace), [Create a storage credential](#create-a-storage-credential), and [Update a metastore\u2019s settings](#update-a-metastores-settings).\n\n### [Link a metastore with a workspace](#id17)\n\nIf you run this command on a workspace that already has an assigned metastore, you will update that workspace\u2019s assigned metastore.\n\nTo display usage documentation, run `databricks unity-catalog metastores assign --help`.\n\n```\ndatabricks unity-catalog metastores assign --workspace-id 1234567890123456 \\\n                                           --metastore-id 12a345b6-7890-1cd2-3456-e789f0a12b34 \\\n                                           --default-catalog-name main\n\n```\n\nTo get the workspace ID, see [Workspace instance names, URLs, and IDs](..\/..\/..\/workspace\/workspace-details.html#workspace-url).\n\nTo get the metastore ID, run `databricks unity-catalog metastores get-summary`. For an example, see [Get summarized information about the current metastore](#get-summarized-information-about-the-current-metastore).\n\nOutput:\n\n```\n{}\n\n```\n\nSee also [Create a Unity Catalog metastore](..\/..\/..\/data-governance\/unity-catalog\/create-metastore.html).\n\n### [Update a metastore\u2019s settings](#id18)\n\nTo display usage documentation, run `databricks unity-catalog metastores update --help`.\n\n```\ndatabricks unity-catalog metastores update --id 12a345b6-7890-1cd2-3456-e789f0a12b34 \\\n                                           --json-file update-metastore.json\n\n```\n\nTo get the metastore ID, run `databricks unity-catalog metastores get-summary`. For an example, see [Get summarized information about the current metastore](#get-summarized-information-about-the-current-metastore).\n\n`update-metastore.json`:\n\n```\n{\n  \"storage_root_credential_id\": \"12a345b6-7890-1cd2-3456-e789f0a12b34\"\n}\n\n```\n\nTo get the storage root credential ID, see [Get information about a storage credential](#get-information-about-a-storage-credential).\n\nOutput:\n\n```\n{\n  \"name\": \"my-metastore\",\n  \"storage_root\": \"s3:\/\/my-bucket\/abcdefghij\/12a345b6-7890-1cd2-3456-e789f0a12b34\",\n  \"default_data_access_config_id\": \"12a345b6-7890-1cd2-3456-e789f0a12b34\",\n  \"metastore_id\": \"12a345b6-7890-1cd2-3456-e789f0a12b34\",\n  \"created_at\": 1656365260817,\n  \"created_by\": \"someone@example.com\",\n  \"updated_at\": 1656366434069,\n  \"updated_by\": \"someone@example.com\",\n  \"delta_sharing_enabled\": true,\n  \"owner\": \"someone@example.com\",\n  \"delta_sharing_recipient_token_lifetime_in_seconds\": 86400,\n  \"region\": \"us-west-2\",\n  \"storage_root_credential_id\": \"12a345b6-7890-1cd2-3456-e789f0a12b34\",\n  \"cloud\": \"aws\",\n  \"global_metastore_id\": \"aws:us-west-2:12a345b6-7890-1cd2-3456-e789f0a12b34\",\n  \"delta_sharing_scope\": \"INTERNAL_AND_EXTERNAL\",\n  \"privilege_model_version\": \"0.1\"\n}\n\n```\n\n### [List available metastores](#id19)\n\nTo display usage documentation, run `databricks unity-catalog metastores list --help`.\n\n```\ndatabricks unity-catalog metastores list\n\n```\n\nOutput:\n\n```\n{\n  \"metastores\": [\n    {\n      \"name\": \"my-metastore\",\n      \"storage_root\": \"s3:\/\/my-bucket\/abcdefghij\/12a345b6-7890-1cd2-3456-e789f0a12b34\",\n      \"default_data_access_config_id\": \"12a345b6-7890-1cd2-3456-e789f0a12b34\",\n      \"metastore_id\": \"12a345b6-7890-1cd2-3456-e789f0a12b34\",\n      \"created_at\": 1656365260817,\n      \"created_by\": \"someone@example.com\",\n      \"updated_at\": 1656366434069,\n      \"updated_by\": \"someone@example.com\",\n      \"delta_sharing_enabled\": true,\n      \"owner\": \"someone@example.com\",\n      \"delta_sharing_recipient_token_lifetime_in_seconds\": 86400,\n      \"region\": \"us-west-2\",\n      \"storage_root_credential_id\": \"12a345b6-7890-1cd2-3456-e789f0a12b34\",\n      \"cloud\": \"aws\",\n      \"global_metastore_id\": \"aws:us-west-2:12a345b6-7890-1cd2-3456-e789f0a12b34\",\n      \"delta_sharing_scope\": \"INTERNAL_AND_EXTERNAL\",\n      \"privilege_model_version\": \"0.1\"\n    },\n    {\n      \"...\": \"...\"\n    }\n  ]\n}\n\n```\n\n### [Get information about a metastore](#id20)\n\nTo display usage documentation, run `databricks unity-catalog metastores get --help`.\n\n```\ndatabricks unity-catalog metastores get --id 12a345b6-7890-1cd2-3456-e789f0a12b34\n\n```\n\nTo get the metastore ID, run `databricks unity-catalog metastores get-summary`. For an example, see [Get summarized information about the current metastore](#get-summarized-information-about-the-current-metastore).\n\nOutput:\n\n```\n{\n  \"name\": \"my-metastore\",\n  \"storage_root\": \"s3:\/\/my-bucket\/abcdefghij\/12a345b6-7890-1cd2-3456-e789f0a12b34\",\n  \"default_data_access_config_id\": \"12a345b6-7890-1cd2-3456-e789f0a12b34\",\n  \"metastore_id\": \"12a345b6-7890-1cd2-3456-e789f0a12b34\",\n  \"created_at\": 1656365260817,\n  \"created_by\": \"someone@example.com\",\n  \"updated_at\": 1656366434069,\n  \"updated_by\": \"someone@example.com\",\n  \"delta_sharing_enabled\": true,\n  \"owner\": \"someone@example.com\",\n  \"delta_sharing_recipient_token_lifetime_in_seconds\": 86400,\n  \"region\": \"us-west-2\",\n  \"storage_root_credential_id\": \"12a345b6-7890-1cd2-3456-e789f0a12b34\",\n  \"cloud\": \"aws\",\n  \"global_metastore_id\": \"aws:us-west-2:12a345b6-7890-1cd2-3456-e789f0a12b34\",\n  \"delta_sharing_scope\": \"INTERNAL_AND_EXTERNAL\",\n  \"privilege_model_version\": \"0.1\"\n}\n\n```\n\n### [Get summarized information about the current metastore](#id21)\n\nTo display usage documentation, run `databricks unity-catalog metastores get-summary --help`.\n\n```\ndatabricks unity-catalog metastores get-summary\n\n```\n\nOutput:\n\n```\n{\n  \"metastore_id\": \"12a345b6-7890-1cd2-3456-e789f0a12b34\",\n  \"name\": \"my-metastore\",\n  \"default_data_access_config_id\": \"12a345b6-7890-1cd2-3456-e789f0a12b34\",\n  \"storage_root_credential_id\": \"12a345b6-7890-1cd2-3456-e789f0a12b34\",\n  \"cloud\": \"aws\",\n  \"region\": \"us-west-2\",\n  \"global_metastore_id\": \"aws:us-west-2:12a345b6-7890-1cd2-3456-e789f0a12b34\",\n  \"storage_root_credential_name\": \"my-storage-root-credential\"\n}\n\n```\n\n### [Get the current metastore assignment for a workspace](#id22)\n\nTo display usage documentation, run `databricks unity-catalog metastores get-assignment --help`.\n\n```\ndatabricks unity-catalog metastores get-assignment\n\n```\n\nOutput:\n\n```\n{\n  \"workspace_id\": 1234567890123456,\n  \"metastore_id\": \"12a345b6-7890-1cd2-3456-e789f0a12b34\",\n  \"default_catalog_name\": \"main\"\n}\n\n```\n\n### [Unlink a metastore from a workspace](#id23)\n\nTo display usage documentation, run `databricks unity-catalog metastores unassign --help`.\n\n```\ndatabricks unity-catalog metastores unassign --workspace-id 1234567890123456 \\\n                                             --metastore-id 12a345b6-7890-1cd2-3456-e789f0a12b34\n\n```"},{"doc_uri":"https:\/\/docs.databricks.com\/data-governance\/unity-catalog\/index.html","content":"A table can be *managed* or *external*.\n\n#### Managed tables\n\nManaged tables are the preferred way to create tables in Unity Catalog. Unity Catalog manages the lifecycle and file layout for these tables. Unity Catalog also [optimizes their performance automatically](..\/..\/optimizations\/predictive-optimization.html). You should not use tools outside of Databricks to manipulate files in these tables directly. Managed tables always use the [Delta](..\/..\/delta\/index.html) table format.\n\nFor workspaces that were enabled for Unity Catalog manually, managed tables are stored in the root storage location that you configure when you create a metastore. You can optionally specify managed table storage locations at the catalog or schema levels, overriding the root storage location.\n\nFor workspaces that were enabled for Unity Catalog automatically, the metastore root storage location is optional, and managed tables are typically stored at the catalog or schema levels.\n\nWhen a managed table is dropped, its underlying data is deleted from your cloud tenant within 30 days.\n\nSee [Managed tables](create-tables.html#managed-table).\n\n#### External tables\n\nExternal tables are tables whose data lifecycle and file layout are not managed by Unity Catalog. Use external tables to register large amounts of existing data in Unity Catalog, or if you require direct access to the data using tools outside of Databricks clusters or Databricks SQL warehouses.\n\nWhen you drop an external table, Unity Catalog does not delete the underlying data. You can manage privileges on external tables and use them in queries in the same way as managed tables.\n\nExternal tables can use the following file formats:\n\n* DELTA\n* CSV\n* JSON\n* AVRO\n* PARQUET\n* ORC\n* TEXT\n\nSee [External tables](create-tables.html#external-table).\n\n### Views\n\nA view is a read-only object created from one or more tables and views in a metastore. It resides in the third layer of Unity Catalog\u2019s three-level namespace. A view can be created from tables and other views in multiple schemas and catalogs. You can create [dynamic views](#) to enable row- and column-level permissions.\n\nSee [Create a dynamic view](create-views.html#dynamic-view).\n\n### Volumes\n\nA volume resides in the third layer of Unity Catalog\u2019s three-level namespace. Volumes are siblings to tables, views, and other objects organized under a schema in Unity Catalog.\n\nVolumes contain directories and files for data stored in any format. Volumes provide non-tabular access to data, meaning that files in volumes cannot be registered as tables.\n\n* To create a volume, users must have `CREATE VOLUME` and `USE SCHEMA` permissions on the schema, and they must have the `USE CATALOG` permission on its parent catalog.\n* To read files and directories stored inside a volume, users must have the `READ VOLUME` permission, the `USE SCHEMA` permission on its parent schema, and the `USE CATALOG` permission on its parent catalog.\n* To add, remove, or modify files and directories stored inside a volume, users must have `WRITE VOLUME` permission, the `USE SCHEMA` permission on its parent schema, and the `USE CATALOG` permission on its parent catalog.\n\nA volume can be *managed* or *external*.\n\nNote\n\nWhen you define a volume, cloud URI access to data under the volume path is governed by the permissions of the volume.\n\n#### Managed volumes\n\nManaged volumes are a convenient solution when you want to provision a governed location for working with non-tabular files.\n\nManaged volumes store files in the Unity Catalog managed storage location for the schema in which they\u2019re contained. For workspaces that were enabled for Unity Catalog manually, managed volumes are stored in the root storage location that you configure when you create a metastore. You can optionally specify managed volume storage locations at the catalog or schema levels, overriding the root storage location. For workspaces that were enabled for Unity Catalog automatically, the metastore root storage location is optional, and managed volumes are typically stored at the catalog or schema levels.\n\nThe following precedence governs which location is used for a managed volume:\n\n* Schema location\n* Catalog location\n* Unity Catalog metastore root storage location\n\nWhen you delete a managed volume, the files stored in this volume are also deleted from your cloud tenant within 30 days.\n\nSee [What is a managed volume?](..\/..\/connect\/unity-catalog\/volumes.html#managed).\n\n#### External volumes\n\nAn external volume is registered to a Unity Catalog external location and provides access to existing files in cloud storage without requiring data migration. Users must have the `CREATE EXTERNAL VOLUME` permission on the external location to create an external volume.\n\nExternal volumes support scenarios where files are produced by other systems and staged for access from within Databricks using object storage or where tools outside Databricks require direct file access.\n\nUnity Catalog does not manage the lifecycle and layout of the files in external volumes. When you drop an external volume, Unity Catalog does not delete the underlying data.\n\nSee [What is an external volume?](..\/..\/connect\/unity-catalog\/volumes.html#external).\n\n### Models\n\nA model resides in the third layer of Unity Catalog\u2019s three-level namespace. In this context, \u201cmodel\u201d refers to a machine learning model that is registered in the [MLflow Model Registry](..\/..\/machine-learning\/manage-model-lifecycle\/index.html). To create a model in Unity Catalog, users must have the `CREATE MODEL` privilege for the catalog or schema. The user must also have the `USE CATALOG` privilege on the parent catalog and `USE SCHEMA` on the parent schema.\n\n## Managed storage\n\nYou can store managed tables and managed volumes at any of these levels in the Unity Catalog object hierarchy: metastore, catalog, or schema. Storage at lower levels in the hierarchy overrides storage defined at higher levels.\n\nWhen an account admin creates a metastore manually, they have the option to assign a storage location in an S3 bucket or Cloudflare R2 bucket in your own cloud storage account to use as metastore-level storage for managed tables and volumes. If a metastore-level managed storage location has been assigned, then managed storage locations at the catalog and schema levels are optional. That said, metastore-level storage is optional, and Databricks recommends assigning managed storage at the catalog level for logical data isolation. See [Data governance and data isolation building blocks](best-practices.html#building-blocks).\n\nImportant\n\nIf your workspace was enabled for Unity Catalog automatically, the Unity Catalog metastore was created without metastore-level managed storage. You can opt to add metastore-level storage, but Databricks recommends assigning managed storage at the catalog and schema levels. For help deciding whether you need metastore-level storage, see [(Optional) Create metastore-level storage](get-started.html#metastore-storage) and [Data is physically separated in storage](best-practices.html#physically-separate).\n\nManaged storage has the following properties:\n\n* Managed tables and managed volumes store data and metadata files in managed storage.\n* Managed storage locations cannot overlap with external tables or external volumes.\n\nThe following table describes how managed storage is declared and associated with Unity Catalog objects:\n\n| Associated Unity Catalog object | How to set | Relation to external locations |\n| --- | --- | --- |\n| Metastore | Configured by account admin during metastore creation or added after metastore creation if no storage was specified at creation. | Cannot overlap an external location. |\n| Catalog | Specified during catalog creation using the `MANAGED LOCATION` keyword. | Must be contained within an external location. |\n| Schema | Specified during schema creation using the `MANAGED LOCATION` keyword. | Must be contained within an external location. |\n\nThe managed storage location used to store data and metadata for managed tables and managed volumes uses the following rules:\n\n* If the containing schema has a managed location, the data is stored in the schema managed location.\n* If the containing schema does not have a managed location but the catalog has a managed location, the data is stored in the catalog managed location.\n* If neither the containing schema nor the containing catalog have a managed location, data is stored in the metastore managed location.\n\n## Storage credentials and external locations\n\nTo manage access to the underlying cloud storage for external tables, external volumes, and managed storage, Unity Catalog uses the following object types:\n\n* **Storage credentials** encapsulate a long-term cloud credential that provides access to cloud storage, for example, an IAM role that can access S3 buckets or a Cloudflare R2 API token. See [Create a storage credential for connecting to AWS S3](..\/..\/connect\/unity-catalog\/storage-credentials.html) and [Create a storage credential for connecting to Cloudflare R2](..\/..\/connect\/unity-catalog\/storage-credentials-r2.html).\n* **External locations** contain a reference to a storage credential and a cloud storage path.\n\nSee [Connect to cloud object storage using Unity Catalog](..\/..\/connect\/unity-catalog\/index.html).\n\n## Identity management for Unity Catalog\n\nUnity Catalog uses the identities in the Databricks account to resolve users, service principals, and groups, and to enforce permissions.\n\nTo configure identities in the account, follow the instructions in [Manage users, service principals, and groups](..\/..\/admin\/users-groups\/index.html). Refer to those users, service principals, and groups when you create [access-control policies](manage-privileges\/index.html) in Unity Catalog.\n\nUnity Catalog users, service principals, and groups must also be added to workspaces to access Unity Catalog data in a notebook, a Databricks SQL query, Catalog Explorer, or a REST API command. The assignment of users, service principals, and groups to workspaces is called *identity federation*.\n\nAll workspaces that have a Unity Catalog metastore attached to them are enabled for identity federation.\n\n### Special considerations for groups\n\nAny groups that already exist in the workspace are labeled **Workspace local** in the account console. These workspace-local groups cannot be used in Unity Catalog to define access policies. You must use account-level groups. If a workspace-local group is referenced in a command, that command will return an error that the group was not found. If you previously used workspace-local groups to manage access to notebooks and other artifacts, these permissions remain in effect.\n\nSee [Manage groups](..\/..\/admin\/users-groups\/groups.html).\n\n## Admin roles for Unity Catalog\n\nAccount admins, metastore admins, and workspace admins are involved in managing Unity Catalog:\n\nSee [Admin privileges in Unity Catalog](manage-privileges\/admin-privileges.html).\n\n## Data permissions in Unity Catalog\n\nIn Unity Catalog, data is secure by default. Initially, users have no access to data in a metastore. Access can be granted by either a metastore admin, the owner of an object, or the owner of the catalog or schema that contains the object. Securable objects in Unity Catalog are hierarchical and privileges are inherited downward.\n\nYou can assign and revoke permissions using Catalog Explorer, SQL commands, or REST APIs.\n\nSee [Manage privileges in Unity Catalog](manage-privileges\/index.html).\n\n## Supported compute and cluster access modes for Unity Catalog\n\nUnity Catalog is supported on clusters that run Databricks Runtime 11.3 LTS or above. Unity Catalog is supported by default on all [SQL warehouse](..\/..\/compute\/sql-warehouse\/index.html) compute versions.\n\nClusters running on earlier versions of Databricks Runtime do not provide support for all Unity Catalog GA features and functionality.\n\nTo access data in Unity Catalog, clusters must be configured with the correct *access mode*. Unity Catalog is secure by default. If a cluster is not configured with one of the Unity-Catalog-capable access modes (that is, shared or assigned), the cluster can\u2019t access data in Unity Catalog. See [Access modes](..\/..\/compute\/configure.html#access-mode).\n\nFor detailed information about Unity Catalog functionality changes in each Databricks Runtime version, see the [release notes](..\/..\/release-notes\/runtime\/index.html).\n\nLimitations for Unity Catalog vary by access mode and Databricks Runtime version. See [Compute access mode limitations for Unity Catalog](..\/..\/compute\/access-mode-limitations.html).\n\n## Data lineage for Unity Catalog\n\nYou can use Unity Catalog to capture runtime data lineage across queries in any language executed on a Databricks cluster or SQL warehouse. Lineage is captured down to the column level, and includes notebooks, workflows and dashboards related to the query. To learn more, see [Capture and view data lineage using Unity Catalog](data-lineage.html).\n\n## Lakehouse Federation and Unity Catalog\n\nLakehouse Federation is the query federation platform for Databricks. The term *query federation* describes a collection of features that enable users and systems to run queries against multiple siloed data sources without needing to migrate all data to a unified system.\n\nDatabricks uses Unity Catalog to manage query federation. You use Unity Catalog to configure read-only *connections* to popular external database systems and create *foreign catalogs* that mirror external databases. Unity Catalog\u2019s data governance and data lineage tools ensure that data access is managed and audited for all federated queries made by the users in your Databricks workspaces.\n\nSee [What is Lakehouse Federation](..\/..\/query-federation\/index.html).\n\n## How do I set up Unity Catalog for my organization?\n\nTo learn how to set up Unity Catalog, see [Set up and manage Unity Catalog](get-started.html).\n\n## Supported regions\n\nAll regions support Unity Catalog. For details, see [Databricks clouds and regions](..\/..\/resources\/supported-regions.html).\n\n## Supported data file formats\n\nUnity Catalog supports the following table formats:\n\n* [Managed tables](create-tables.html#managed-table) must use the `delta` table format.\n* [External tables](create-tables.html#external-table) can use `delta`, `CSV`, `JSON`, `avro`, `parquet`, `ORC`, or `text`.\n\n## Unity Catalog limitations\n\nUnity Catalog has the following limitations.\n\nNote\n\nIf your cluster is running on a Databricks Runtime version below 11.3 LTS, there may be additional limitations, not listed here. Unity Catalog is supported on Databricks Runtime 11.3 LTS or above.\n\nUnity Catalog limitations vary by Databricks Runtime and access mode. Structured Streaming workloads have additional limitations based on Databricks Runtime and access mode. See [Compute access mode limitations for Unity Catalog](..\/..\/compute\/access-mode-limitations.html)."},{"doc_uri":"https:\/\/docs.databricks.com\/data-governance\/unity-catalog\/best-practices.html","content":"### Data can be managed only by designated people or teams\n\nUnity Catalog gives you the ability to choose between centralized and distributed governance models.\n\nIn the centralized governance model, your governance administrators are owners of the metastore and can take ownership of any object and grant and revoke permissions.\n\nIn a distributed governance model, the catalog or a set of catalogs is the data domain. The owner of that catalog can create and own all assets and manage governance within that domain. The owners of any given domain can operate independently of the owners of other domains.\n\nRegardless of whether you choose the metastore or catalogs as your data domain, Databricks strongly recommends that you set a group as the metastore admin or catalog owner.\n\n![Unity Catalog ownership and access](..\/..\/_images\/ownership.png)\n\n### Data is physically separated in storage\n\nAn organization can require that data of certain types be stored within specific accounts or buckets in their cloud tenant.\n\nUnity Catalog gives the ability to configure storage locations at the metastore, catalog, or schema level to satisfy such requirements.\n\nFor example, let\u2019s say your organization has a company compliance policy that requires production data relating to human resources to reside in the bucket s3:\/\/mycompany-hr-prod. In Unity Catalog, you can achieve this requirement by setting a location on a catalog level, creating a catalog called, for example `hr_prod`, and assigning the location s3:\/\/mycompany-hr-prod\/unity-catalog to it. This means that managed tables or volumes created in the `hr_prod` catalog (for example, using `CREATE TABLE hr_prod.default.table \u2026`) store their data in s3:\/\/mycompany-hr-prod\/unity-catalog. Optionally, you can choose to provide schema-level locations to organize data within the `hr_prod catalog` at a more granular level.\n\nIf such a storage isolation is not required, you can set a storage location at the metastore level. The result is that this location serves as a default location for storing managed tables and volumes across catalogs and schemas in the metastore.\n\nThe system evaluates the hierarchy of storage locations from schema to catalog to metastore.\n\nFor example, if a table `myCatalog.mySchema.myTable` is created in `my-region-metastore`, the table storage location is determined according to the following rule:\n\n1. If a location has been provided for `mySchema`, it will be stored there.\n2. If not, and a location has been provided on `myCatalog`, it will be stored there.\n3. Finally, if no location has been provided on `myCatalog`, it will be stored in the location associated with the `my-region-metastore`.\n\n![Unity Catalog storage hierarchy](..\/..\/_images\/managed-storage.png)\n\n### Data can be accessed only in designated environments\n\nOrganizational and compliance requirements often specify that you keep certain data, like personal data, accessible only in certain environments. You may also want to keep production data isolated from development environments or ensure that certain data sets and domains are never joined together.\n\nIn Databricks, the workspace is the primary data processing environment, and catalogs are the primary data domain. Unity Catalog lets metastore admins and catalog owners assign, or \u201cbind,\u201d catalogs to specific workspaces. These environment-aware bindings give you the ability to ensure that only certain catalogs are available within a workspace, regardless of the specific privileges on data objects granted to a user.\n\nNow let\u2019s take a deeper look at the process of setting up Unity Catalog to meet your needs.\n\n## Configure a Unity Catalog metastore\n\nA metastore is the top-level container of objects in Unity Catalog. Metastores manage data assets (tables, views, and volumes) as well as other securable objects managed by Unity Catalog. For the complete list of securable objects, see [Securable objects in Unity Catalog](manage-privileges\/privileges.html#securable-objects).\n\nThis section provides tips for creating and configuring metastores. If your workspace was automatically enabled for Unity Catalog, you do not need to create a metastore, but the information presented in this section might still be useful. See [Automatic enablement of Unity Catalog](get-started.html#enablement).\n\nTips for configuring metastores:\n\n* You should set up one metastore for each region in which you have Databricks workspaces.\n\nEvery workspace attached to a single regional metastore has access to the data managed by the metastore. If you want to share data between metastores, use [Delta Sharing](#delta-sharing).\n* Each metastore can be configured with a managed storage location (also called root storage) in your cloud tenant that can be used to store managed tables and managed volumes.\n\nIf you choose to create a metastore-level managed location, you must ensure that no users have direct access to it (that is, through the cloud account that contains it). Giving access to this storage location could allow a user to bypass access controls in a Unity Catalog metastore and disrupt auditability. For these reasons, your metastore managed storage should be a dedicated bucket. You should not reuse a bucket that is also your DBFS root file system or has previously been a DBFS root file system.\n\nYou also have the option of defining managed storage at the catalog and schema levels, overriding the metastore\u2019s root storage location. In most scenarios, Databricks recommends storing managed data at the catalog level.\n* You should understand the privileges of workspace admins in workspaces that are enabled for Unity Catalog, and review your existing workspace admin assignments.\n\nWorkspace admins can manage operations for their workspace including adding users and service principals, creating clusters, and delegating other users to be workspace admins. If your workspace was enabled for Unity Catalog automatically, workspace admins have the ability to create catalogs and many other Unity Catalog objects by default. See [Workspace admin privileges when workspaces are enabled for Unity Catalog automatically](manage-privileges\/admin-privileges.html#workspace-admins-auto)\n\nWorkspace admins also have the ability to perform workspace management tasks such as managing job ownership and viewing notebooks, which may give indirect access to data registered in Unity Catalog. Workspace admin is a privileged role that you should distribute carefully.\n* If you use workspaces to isolate user data access, you might want to use workspace-catalog bindings. Workspace-catalog bindings enable you to limit catalog access by workspace boundaries. For example, you can ensure that workspace admins and users can only access production data in `prod_catalog` from a production workspace environment, `prod_workspace`. The default is to share the catalog with all workspaces attached to the current metastore. See [(Optional) Assign a catalog to specific workspaces](create-catalogs.html#catalog-binding).\n\nIf your workspace was enabled for Unity Catalog automatically, the pre-provisioned workspace catalog is bound to your workspace by default.\n\nSee [Create a Unity Catalog metastore](create-metastore.html).\n\n## Configure external locations and storage credentials\n\n*External locations* allow Unity Catalog to read and write data on your cloud tenant on behalf of users. External locations are defined as a path to cloud storage, combined with a *storage credential* that can be used to access that location.\n\nYou can use external locations to register external tables and external volumes in Unity Catalog. The content of these entities is physically located on a sub-path in an external location that is referenced when a user creates the volume or the table.\n\nA *storage credential* encapsulates a long-term cloud credential that provides access to cloud storage. For example, in AWS you can configure an IAM role to access S3 buckets.\n\nFor increased data isolation, you can bind storage credentials and external locations to specific workspaces. See [(Optional) Assign an external location to specific workspaces](..\/..\/connect\/unity-catalog\/external-locations.html#workspace-binding) and [(Optional) Assign a storage credential to specific workspaces](..\/..\/connect\/unity-catalog\/storage-credentials.html#workspace-binding).\n\nTip\n\nExternal locations, by combining storage credentials and storage paths, provide strong control and auditability of storage access. To prevent users from bypassing the access control provided by Unity Catalog, you should ensure that you limit the number of users with direct access to any bucket that is being used as an external location. For the same reason, you should not mount storage accounts to DBFS if they are also being used as external locations. Databricks recommends that you migrate mounts on cloud storage locations to external locations in Unity Catalog using [Catalog Explorer](..\/..\/catalog-explorer\/index.html).\n\nFor a list of best practices for managing external locations, see [Manage external locations, external tables, and external volumes](#manage-external). See also [Create an external location to connect cloud storage to Databricks](..\/..\/connect\/unity-catalog\/external-locations.html).\n\n## Organize your data\n\nDatabricks recommends using catalogs to provide segregation across your organization\u2019s information architecture. Often this means that catalogs correspond to a software development environment scope, team, or business unit. If you use workspaces as a data isolation tool\u2014for example, using different workspaces for production and development environments, or a specific workspace for working with highly sensitive data, you can also bind a catalog to specific workspaces. This ensures that all processing of specified data is handled in the appropriate workspace. See [(Optional) Assign a catalog to specific workspaces](create-catalogs.html#catalog-binding).\n\n![Unity Catalog catalogs](..\/..\/_images\/uc-catalogs.png)\n\nA schema (also called a database) is the second layer of Unity Catalog\u2019s three-level namespace and organizes tables, views, and volumes. You can use schemas to organize and define permissions for your assets.\n\nObjects governed by Unity Catalog can be *managed* or *external*:\n\n* **Managed objects** are the default way to create data objects in Unity Catalog.\n\nUnity Catalog manages the lifecycle and file layout for these securables. You should not use tools outside of Databricks to manipulate files in managed tables or volumes directly.\n\nManaged tables and volumes are stored in *managed storage*, which can exist at the metastore, catalog, or schema level for any given table or volume. See [Data is physically separated in storage](#physically-separate).\n\nManaged tables and volumes are a convenient solution when you want to provision a governed location for your content without the overhead of creating and managing external locations and storage credentials.\n\nManaged tables always use the [Delta](..\/..\/delta\/index.html) table format.\n* **External objects** are securables whose data lifecycle and file layout are not managed by Unity Catalog.\n\nExternal volumes and tables are registered on an external location to provide access to large numbers of files that already exist in cloud storage without requiring data copy activity. Use external objects when you have files that are produced by other systems and want them staged for access from within Databricks, or when tools outside of Databricks require direct access to these files.\n\nExternal tables support Delta Lake and many other data formats, including Parquet, JSON, and CSV. Both managed and external volumes can be used to access and store files of arbitrary formats: data can be structured, semi-structured, or unstructured.\n\nFor more information about creating tables and volumes, see [Create tables in Unity Catalog](create-tables.html) and [Create and work with volumes](..\/..\/connect\/unity-catalog\/volumes.html).\n\n## Manage external locations, external tables, and external volumes\n\nThe diagram below represents the filesystem hierarchy of a single cloud storage bucket, with four external locations that share one storage credential.\n\n![External locations](..\/..\/_images\/external-locations.png)\n\nOnce you have external locations configured in Unity Catalog, you can create external tables and volumes on directories inside the external locations. You can then use Unity Catalog to manage user and group access to these tables and volumes. This allows you to provide specific users or groups access to specific directories and files in the cloud storage bucket.\n\nNote\n\nWhen you define a volume, cloud URI access to data under the volume path is governed by the permissions of the volume.\n\n### Recommendations for using external locations\n\nRecommendations for granting permissions on external locations:\n\n* Grant the ability to create external locations only to an administrator who is tasked with setting up connections between Unity Catalog and cloud storage, or to trusted data engineers.\n\nExternal locations provide access from within Unity Catalog to a broadly encompassing location in cloud storage\u2014for example, an entire bucket or container (s3:\/\/mybucket) or a broad subpath (s3:\/\/mybucket\/alotofdata). The intention is that a cloud administrator can be involved in setting up a few external locations and then delegate the responsibility of managing those locations to a Databricks administrator in your organization. The Databricks administrator can then further organize the external location into areas with more granular permissions by registering external volumes or external tables at specific prefixes under the external location.\n\nBecause external locations are so encompassing, Databricks recommends giving the `CREATE EXTERNAL LOCATION` permission only to an administrator who is tasked with setting up connections between Unity Catalog and cloud storage, or to trusted data engineers. To provide other users with more granular access, Databricks recommends registering external tables or volumes on top of external locations and granting users access to data using volumes or tables. Since tables and volumes are children of a catalog and schema, catalog or schema administrators have the ultimate control over access permissions.\n\nYou can also control access to an external location by binding it to specific workspaces. See [(Optional) Assign an external location to specific workspaces](..\/..\/connect\/unity-catalog\/external-locations.html#workspace-binding).\n* Don\u2019t grant general `READ FILES` or `WRITE FILES` permissions on external locations to end users.\n\nWith the availability of volumes, users shouldn\u2019t use external locations for anything but creating tables, volumes, or managed locations. They should not use external locations for path-based access for data science or other non-tabular data use cases.\n\nVolumes provide support for working with files using SQL commands, dbutils, Spark APIs, REST APIs, Terraform, and a user interface for browsing, uploading, and downloading files. Moreover, volumes offer a FUSE mount that is accessible on the local file system under `\/Volumes\/<catalog_name>\/<schema_name>\/<volume_name>\/`. The FUSE mount allows data scientists and ML engineers to access files as if they were in a local filesystem, as required by many machine learning or operating system libraries.\n\nIf you must grant direct access to files in an external location (for exploring files in cloud storage before a user creates an external table or volume, for example), you can grant `READ FILES`. Use cases for granting `WRITE FILES` are rare.\n\nYou should use external locations to do the following:\n\n* Register external tables and volumes using the `CREATE EXTERNAL VOLUME` or `CREATE TABLE` commands.\n* Explore existing files in cloud storage before you create an external table or volume at a specific prefix. The `READ FILES` privilege is a precondition.\n* Register a location as managed storage for catalogs and schemas instead of the metastore root bucket. The `CREATE MANAGED STORAGE` privilege is a precondition.\n\nMore recommendations for using external locations:\n\n* Avoid path overlap conflicts: never create external volumes or tables at the root of an external location.\n\nIf you do create external volumes or tables at the external location root, you can\u2019t create any additional external volumes or tables on the external location. Instead, create external volumes or tables on a sub-directory inside the external location.\n\n### Recommendations for using external volumes\n\nYou should use external volumes to do the following:"},{"doc_uri":"https:\/\/docs.databricks.com\/connect\/unity-catalog\/managed-storage.html","content":"Specify a managed storage location in Unity Catalog | Databricks on AWS\n\n* [![Databricks](..\/..\/_static\/small-scale-lockup-full-color-rgb.svg)](https:\/\/www.databricks.com\/)\n\n* [![Databricks](..\/..\/_static\/small-scale-lockup-full-color-rgb.svg)](https:\/\/www.databricks.com\/)\n* [Help Center](https:\/\/help.databricks.com\/s\/)\n* [Documentation](https:\/\/docs.databricks.com\/en\/index.html)\n* [Knowledge Base](https:\/\/kb.databricks.com\/)\n\n* [Community](https:\/\/community.databricks.com)\n* [Support](https:\/\/help.databricks.com)\n* [Feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n* [Try Databricks](https:\/\/databricks.com\/try-databricks)\n\n[![](..\/..\/_static\/icons\/globe.png)English](javascript:void(0))\n* [English](..\/..\/..\/en\/connect\/unity-catalog\/managed-storage.html)\n* [\u65e5\u672c\u8a9e](..\/..\/..\/ja\/connect\/unity-catalog\/managed-storage.html)\n* [Portugu\u00eas](..\/..\/..\/pt\/connect\/unity-catalog\/managed-storage.html)\n\n[![](..\/..\/_static\/icons\/aws.svg)Amazon\n Web Services](javascript:void(0))\n* [![](..\/..\/_static\/icons\/azure.svg)Microsoft Azure](https:\/\/learn.microsoft.com\/azure\/databricks\/connect\/unity-catalog\/managed-storage)\n* [![](..\/..\/_static\/icons\/gcp.svg)Google Cloud Platform](https:\/\/docs.gcp.databricks.com\/connect\/unity-catalog\/managed-storage.html)\n\n[Databricks on AWS](..\/..\/index.html)\nGet started\n\n* [Get started](..\/..\/getting-started\/index.html)\n* [What is Databricks?](..\/..\/introduction\/index.html)\n* [DatabricksIQ](..\/..\/databricksiq\/index.html)\n* [Release notes](..\/..\/release-notes\/index.html)\n\nLoad & manage data\n\n* [Connect to data sources](..\/index.html)\n\t+ [Unity Catalog storage](index.html)\n\t\t- [Create a storage credential for connecting to AWS S3](storage-credentials.html)\n\t\t- [Create a storage credential for connecting to Cloudflare R2](storage-credentials-r2.html)\n\t\t- [Create an external location to connect cloud storage to Databricks](external-locations.html)\n\t\t- [Specify a managed storage location in Unity Catalog](#)\n\t\t- [Create volumes](volumes.html)\n\t\t- [Manage storage credentials](manage-storage-credentials.html)\n\t\t- [Manage external locations](manage-external-locations.html)\n\t+ [Lakehouse federation](..\/..\/query-federation\/index.html)\n\t+ [Streaming data connectors](..\/streaming\/index.html)\n\t+ [External systems](..\/external-systems\/index.html)\n\t+ [Cloud object storage (legacy)](..\/storage\/index.html)\n* [Connect to compute](..\/..\/compute\/index.html)\n* [Discover data](..\/..\/discover\/index.html)\n* [Query data](..\/..\/query\/index.html)\n* [Ingest data](..\/..\/ingestion\/index.html)\n* [Transform data](..\/..\/transform\/index.html)\n* [Monitor data and AI assets](..\/..\/lakehouse-monitoring\/index.html)\n* [Share data (Delta Sharing)](..\/..\/data-sharing\/index.html)\n* [Databricks Marketplace](..\/..\/marketplace\/index.html)\n\nWork with data\n\n* [Data engineering](..\/..\/workspace-index.html)\n* [Generative AI & LLMs](..\/..\/generative-ai\/generative-ai.html)\n* [Machine learning](..\/..\/machine-learning\/index.html)\n* [Model serving](..\/..\/machine-learning\/model-serving\/index.html)\n* [Data warehousing](..\/..\/sql\/index.html)\n* [Delta Lake](..\/..\/delta\/index.html)\n* [Developers](..\/..\/languages\/index.html)\n* [Technology partners](..\/..\/integrations\/index.html)\n\nAdministration\n\n* [Account and workspace administration](..\/..\/admin\/index.html)\n* [Security and compliance](..\/..\/security\/index.html)\n* [Data governance (Unity Catalog)](..\/..\/data-governance\/index.html)\n* [Lakehouse architecture](..\/..\/lakehouse-architecture\/index.html)\n\nReference & resources\n\n* [Reference](..\/..\/reference\/api.html)\n* [Resources](..\/..\/resources\/index.html)\n* [What\u2019s coming?](..\/..\/whats-coming.html)\n* [Documentation archive](..\/..\/archive\/index.html)\n\n Updated May 28, 2024\n \n\n[Send us feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n\n* [Documentation](..\/..\/index.html)\n* [Connect to data sources](..\/index.html)\n* [Connect to cloud object storage using Unity Catalog](index.html)\n* Specify a managed storage location in Unity Catalog\n* \n\n# Specify a managed storage location in Unity Catalog\n\nA managed storage location specifies a location in cloud object storage for storing data for managed tables and managed volumes.\n\nYou can associate a managed storage location with a metastore, catalog, or schema. Managed storage locations at lower levels in the hierarchy override storage locations defined at higher levels when managed tables or managed volumes are created.\n\nWhen an account admin creates a metastore, they can associate a storage location in an AWS S3 or Cloudflare R2 bucket in your cloud provider account to use as a managed storage location. Managed storage locations at the catalog and schema levels are optional, but Databricks recommends assigning managed storage locations at the catalog level for logical data isolation. See [Data governance and data isolation building blocks](..\/..\/data-governance\/unity-catalog\/best-practices.html#building-blocks).\n\nImportant\n\nIf your workspace was enabled for Unity Catalog automatically, the Unity Catalog metastore was created without a metastore-level managed storage location. You should assign a managed storage location at the catalog or schema level. See [Automatic enablement of Unity Catalog](..\/..\/data-governance\/unity-catalog\/get-started.html#enablement) and [Data governance and data isolation building blocks](..\/..\/data-governance\/unity-catalog\/best-practices.html#building-blocks).\n\n## What is a managed storage location?\n\nManaged storage locations have the following properties:\n\n* Managed tables and managed volumes store data and metadata files in managed storage locations.\n* Managed storage locations cannot overlap with external tables or external volumes.\n\nThe following table describes how a managed storage location is declared and associated with Unity Catalog objects:\n\n| Associated Unity Catalog object | How to set | Relation to external locations |\n| --- | --- | --- |\n| Metastore | Configured by account admin during metastore creation. | Cannot overlap an external location. |\n| Catalog | Specified during catalog creation using the `MANAGED LOCATION` keyword. | Must be contained within an external location. |\n| Schema | Specified during schema creation using the `MANAGED LOCATION` keyword. | Must be contained within an external location. |\n\nThe managed storage location that stores data and metadata for managed tables and managed volumes uses the following rules:\n\n* If the containing schema has a managed location, the data is stored in the schema managed location.\n* If the containing schema does not have a managed location but the catalog has a managed location, the data is stored in the catalog managed location.\n* If neither the containing schema nor the containing catalog have a managed location, data is stored in the metastore managed location.\n\nUnity Catalog prevents overlap of location governance. See [How do paths work for data managed by Unity Catalog?](..\/..\/data-governance\/unity-catalog\/paths.html).\n\n## Managed storage location, storage root, and storage location\n\nWhen you specify a `MANAGED LOCATION` for a catalog or schema, the provided location is tracked as the **Storage Root** in Unity Catalog. To ensure that all managed entities have a unique location, Unity Catalog adds hashed subdirectories to the specified location, using the following format:\n\n| Object | Path |\n| --- | --- |\n| Schema | `<storage-root>\/__unitystorage\/schemas\/00000000-0000-0000-0000-000000000000` |\n| Catalog | `<storage-root>\/__unitystorage\/catalogs\/00000000-0000-0000-0000-000000000000` |\n\nThe fully qualified path for the managed storage location is tracked as the **Storage Location** in Unity Catalog.\n\nYou can specify the same managed storage location for multiple schemas and catalogs.\n\n## Required privileges\n\nUsers who have the `CREATE MANAGED STORAGE` privilege on an external location can configure managed storage locations during catalog or schema creation.\n\nManaged storage locations set at the metastore level must be configured by account admins during metastore creation.\n\n## Set a managed storage location for a catalog\n\nSet a managed storage location for a catalog by using the `MANAGED LOCATION` keyword during catalog creation, as in the following example:\n\n```\nCREATE CATALOG <catalog-name>\nMANAGED LOCATION 's3:\/\/<external-location-bucket-path>\/<directory>';\n\n```\n\n## Set a managed storage location for a schema\n\nSet a managed storage location for a schema by using the `MANAGED LOCATION` keyword during schema creation, as in the following example:\n\n```\nCREATE SCHEMA <catalog>.<schema-name>\nMANAGED LOCATION 's3:\/\/<external-location-bucket-path>\/<directory>';\n\n```\n\n## Next steps\n\nManage storage locations are used for creating managed tables and managed volumes. See [Create tables in Unity Catalog](..\/..\/data-governance\/unity-catalog\/create-tables.html) and [Create and work with volumes](volumes.html).\n\n---\n\n \u00a9 Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the [Apache Software Foundation](http:\/\/www.apache.org\/).\n \n\n[Send us feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n \n | [Privacy Policy](https:\/\/databricks.com\/privacy-policy) | [Terms of Use](https:\/\/databricks.com\/terms-of-use)\n\n { # 's' variable stands for the search page here. TODO: Fix it. #}\n \n [![DATA+AU Summit 2024](..\/..\/_static\/images\/dais\/docs-dais-promo-image.png)](https:\/\/www.databricks.com\/dataaisummit?utm_source=databricks&utm_medium=web&utm_campaign=7018y000001f7cvqai&utm_offer=docs \"DATA+AU Summit 2024\")"},{"doc_uri":"https:\/\/docs.databricks.com\/dbfs\/unity-catalog.html","content":"Best practices for DBFS and Unity Catalog | Databricks on AWS\n\n* [![Databricks](..\/_static\/small-scale-lockup-full-color-rgb.svg)](https:\/\/www.databricks.com\/)\n\n* [![Databricks](..\/_static\/small-scale-lockup-full-color-rgb.svg)](https:\/\/www.databricks.com\/)\n* [Help Center](https:\/\/help.databricks.com\/s\/)\n* [Documentation](https:\/\/docs.databricks.com\/en\/index.html)\n* [Knowledge Base](https:\/\/kb.databricks.com\/)\n\n* [Community](https:\/\/community.databricks.com)\n* [Support](https:\/\/help.databricks.com)\n* [Feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n* [Try Databricks](https:\/\/databricks.com\/try-databricks)\n\n[![](..\/_static\/icons\/globe.png)English](javascript:void(0))\n* [English](..\/..\/en\/dbfs\/unity-catalog.html)\n* [\u65e5\u672c\u8a9e](..\/..\/ja\/dbfs\/unity-catalog.html)\n* [Portugu\u00eas](..\/..\/pt\/dbfs\/unity-catalog.html)\n\n[![](..\/_static\/icons\/aws.svg)Amazon\n Web Services](javascript:void(0))\n* [![](..\/_static\/icons\/azure.svg)Microsoft Azure](https:\/\/learn.microsoft.com\/azure\/databricks\/dbfs\/unity-catalog)\n* [![](..\/_static\/icons\/gcp.svg)Google Cloud Platform](https:\/\/docs.gcp.databricks.com\/dbfs\/unity-catalog.html)\n\n[Databricks on AWS](..\/index.html)\nGet started\n\n* [Get started](..\/getting-started\/index.html)\n* [What is Databricks?](..\/introduction\/index.html)\n* [DatabricksIQ](..\/databricksiq\/index.html)\n* [Release notes](..\/release-notes\/index.html)\n\nLoad & manage data\n\n* [Connect to data sources](..\/connect\/index.html)\n* [Connect to compute](..\/compute\/index.html)\n* [Discover data](..\/discover\/index.html)\n* [Query data](..\/query\/index.html)\n* [Ingest data](..\/ingestion\/index.html)\n* [Transform data](..\/transform\/index.html)\n* [Monitor data and AI assets](..\/lakehouse-monitoring\/index.html)\n* [Share data (Delta Sharing)](..\/data-sharing\/index.html)\n* [Databricks Marketplace](..\/marketplace\/index.html)\n\nWork with data\n\n* [Data engineering](..\/workspace-index.html)\n\t+ [Delta Live Tables](..\/delta-live-tables\/index.html)\n\t+ [Structured Streaming](..\/structured-streaming\/index.html)\n\t+ [Apache Spark](..\/spark\/index.html)\n\t+ [Notebooks](..\/notebooks\/index.html)\n\t+ [Workflows](..\/workflows\/index.html)\n\t+ [Libraries](..\/libraries\/index.html)\n\t+ [Init scripts](..\/init-scripts\/index.html)\n\t+ [Git folders](..\/repos\/index.html)\n\t+ [DBFS](index.html)\n\t\t- [Best practices for DBFS and Unity Catalog](#)\n\t\t- [Recommendations for working with DBFS root](dbfs-root.html)\n\t\t- [What are the root directories?](root-locations.html)\n\t\t- [Mounting cloud object storage on Databricks](mounts.html)\n\t+ [Files](..\/files\/index.html)\n\t+ [Migration](..\/migration\/index.html)\n\t+ [Optimization & performance](..\/optimizations\/index.html)\n* [Generative AI & LLMs](..\/generative-ai\/generative-ai.html)\n* [Machine learning](..\/machine-learning\/index.html)\n* [Model serving](..\/machine-learning\/model-serving\/index.html)\n* [Data warehousing](..\/sql\/index.html)\n* [Delta Lake](..\/delta\/index.html)\n* [Developers](..\/languages\/index.html)\n* [Technology partners](..\/integrations\/index.html)\n\nAdministration\n\n* [Account and workspace administration](..\/admin\/index.html)\n* [Security and compliance](..\/security\/index.html)\n* [Data governance (Unity Catalog)](..\/data-governance\/index.html)\n* [Lakehouse architecture](..\/lakehouse-architecture\/index.html)\n\nReference & resources\n\n* [Reference](..\/reference\/api.html)\n* [Resources](..\/resources\/index.html)\n* [What\u2019s coming?](..\/whats-coming.html)\n* [Documentation archive](..\/archive\/index.html)\n\n Updated May 28, 2024\n \n\n[Send us feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n\n* [Documentation](..\/index.html)\n* [Databricks data engineering](..\/workspace-index.html)\n* [What is DBFS?](index.html)\n* Best practices for DBFS and Unity Catalog\n* \n\n# Best practices for DBFS and Unity Catalog\n\nUnity Catalog introduces a number of new configurations and concepts that approach data governance entirely differently than DBFS. This article outlines several best practices around working with Unity Catalog external locations and DBFS.\n\nDatabricks recommends against using DBFS and mounted cloud object storage for most use cases in Unity Catalog-enabled Databricks workspaces. This article describes a few scenarios in which you should use mounted cloud object storage. Note that Databricks does not recommend using the DBFS root in conjunction with Unity Catalog, unless you must migrate files or data stored there into Unity Catalog.\n\n## How is DBFS used in Unity Catalog-enabled workspaces?\n\nActions performed against tables in the `hive_metastore` use legacy data access patterns, which might include data and storage credentials managed by DBFS. Managed tables in the workspace-scoped `hive_metastore` are stored on the DBFS root.\n\n## How does DBFS work in single user access mode?\n\nClusters configured with single user access mode have full access to DBFS, including all files in the DBFS root and mounted data.\n\n## How does DBFS work in shared access mode?\n\nShared access mode combines Unity Catalog data governance with Databricks legacy table ACLs. Access to data in the `hive_metastore` is only available to users that have permissions explicitly granted.\n\nTo interact with files directly using DBFS, you must have `ANY FILE` permissions granted. Because `ANY FILE` allows users to bypass legacy tables ACLs in the `hive_metastore` and access all data managed by DBFS, Databricks recommends caution when granting this privilege.\n\n## Do not use DBFS with Unity Catalog external locations\n\nUnity Catalog secures access to data in external locations by using full cloud URI paths to identify grants on managed object storage directories. DBFS mounts use an entirely different data access model that bypasses Unity Catalog entirely. Databricks recommends that you do not reuse cloud object storage volumes between DBFS mounts and UC external volumes, including when sharing data across workspaces or accounts.\n\n## Secure your Unity Catalog-managed storage\n\nUnity Catalog using managed storage locations for storing data files for managed tables and volumes.\n\nDatabricks recommends the following for managed storage locations:\n\n* Use new storage accounts or buckets.\n* Define a custom identity policy for Unity Catalog.\n* Restrict all access to Databricks managed by Unity Catalog.\n* Restrict all access to identity access policies created for Unity Catalog.\n\n## Add existing data to external locations\n\nIt is possible to load existing storage accounts into Unity Catalog using external locations. For greatest security, Databricks recommends only loading storage accounts to external locations after revoking all other storage credentials and access patterns.\n\nYou should never load a storage account used as a DBFS root as an external location in Unity Catalog.\n\n## Cluster configurations are ignored by Unity Catalog filesystem access\n\nUnity Catalog does not respect cluster configurations for filesystem settings. This means that Hadoop filesystem settings for configuring custom behavior with cloud object storage do not work when accessing data using Unity Catalog.\n\n## Limitation around multiple path access\n\nWhile you can generally use Unity Catalog and DBFS together, paths that are equal or share a parent\/child relationship cannot be referenced in the same command or notebook cell using different access methods.\n\nFor example, if an external table `foo` is defined in the `hive_metastore` at location `a\/b\/c` and an external location is defined in Unity Catalog on `a\/b\/`, the following code would throw an error:\n\n```\nspark.read.table(\"foo\").filter(\"id IS NOT NULL\").write.mode(\"overwrite\").save(\"a\/b\/c\")\n\n```\n\nThis error would not arise if this logic is broken into two cells:\n\n```\ndf = spark.read.table(\"foo\").filter(\"id IS NOT NULL\")\n\n```\n\n```\ndf.write.mode(\"overwrite\").save(\"a\/b\/c\")\n\n```\n\n---\n\n \u00a9 Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the [Apache Software Foundation](http:\/\/www.apache.org\/).\n \n\n[Send us feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n \n | [Privacy Policy](https:\/\/databricks.com\/privacy-policy) | [Terms of Use](https:\/\/databricks.com\/terms-of-use)\n\n { # 's' variable stands for the search page here. TODO: Fix it. #}\n \n [![DATA+AU Summit 2024](..\/_static\/images\/dais\/docs-dais-promo-image.png)](https:\/\/www.databricks.com\/dataaisummit?utm_source=databricks&utm_medium=web&utm_campaign=7018y000001f7cvqai&utm_offer=docs \"DATA+AU Summit 2024\")"}]}
{"request_id":"id-1135","request":"\"How can I resolve the INCONSISTENT_BEHAVIOR_CROSS_VERSION error when dealing with datetime patterns in Spark 3.0 and above?\"","response":"The error class for the INCONSISTENT\\_BEHAVIOR\\_CROSS\\_VERSION error when dealing with datetime patterns in Spark 3.0 and above is `DATETIME_PATTERN_RECOGNITION`. To resolve this issue, you can set the SQL config `spark.sql.legacy.timeParserPolicy` to `LEGACY` to restore the behavior before Spark 3.0, or form a valid datetime pattern with the guide from `spark.sql.datetimePatternHelp`.","expected_retrieved_context":[{"doc_uri":"https:\/\/docs.databricks.com\/error-messages\/inconsistent-behavior-cross-version-error-class.html","content":"INCONSISTENT_BEHAVIOR_CROSS_VERSION error class  \nSQLSTATE: 42K0B  \nYou may get a different result due to the upgrading to:  \nDATETIME_PATTERN_RECOGNITION\nDATETIME_PATTERN_RECOGNITION\nSpark >= 3.0:  \nFail to recognize <pattern> pattern in the DateTimeFormatter.  \nYou can set <config> to \u201cLEGACY\u201d to restore the behavior before Spark 3.0.  \nYou can form a valid datetime pattern with the guide from \u2018<docroot>\/sql-ref-datetime-pattern.html\u2019.\n\nPARSE_DATETIME_BY_NEW_PARSER\nPARSE_DATETIME_BY_NEW_PARSER\nSpark >= 3.0:  \nFail to parse <datetime> in the new parser.  \nYou can set <config> to \u201cLEGACY\u201d to restore the behavior before Spark 3.0, or set to \u201cCORRECTED\u201d and treat it as an invalid datetime string.\n\nREAD_ANCIENT_DATETIME\nREAD_ANCIENT_DATETIME\nSpark >= 3.0:  \nreading dates before 1582-10-15 or timestamps before 1900-01-01T00:00:00Z  \nfrom <format> files can be ambiguous, as the files may be written by  \nSpark 2.x or legacy versions of Hive, which uses a legacy hybrid calendar  \nthat is different from Spark 3.0+\u2019s Proleptic Gregorian calendar.  \nSee more details in SPARK-31404. You can set the SQL config <config> or  \nthe datasource option <option> to \u201cLEGACY\u201d to rebase the datetime values  \nw.r.t. the calendar difference during reading. To read the datetime values  \nas it is, set the SQL config <config> or the datasource option <option>  \nto \u201cCORRECTED\u201d.\n\nWRITE_ANCIENT_DATETIME\nWRITE_ANCIENT_DATETIME\nSpark >= 3.0:  \nwriting dates before 1582-10-15 or timestamps before 1900-01-01T00:00:00Z into <format> files can be dangerous, as the files may be read by Spark 2.x or legacy versions of Hive later, which uses a legacy hybrid calendar that is different from Spark 3.0+\u2019s Proleptic Gregorian calendar.  \nSee more details in SPARK-31404.  \nYou can set <config> to \u201cLEGACY\u201d to rebase the datetime values w.r.t. the calendar difference during writing, to get maximum interoperability.  \nOr set the config to \u201cCORRECTED\u201d to write the datetime values as it is, if you are sure that the written files will only be read by Spark 3.0+ or other systems that use Proleptic Gregorian calendar."}],"expected_response":"To resolve the INCONSISTENT_BEHAVIOR_CROSS_VERSION error related to datetime patterns in Spark 3.0 and above, you can follow these steps:\n\n1. Identify the specific error class you are encountering. The error message should indicate whether it is related to DATETIME_PATTERN_RECOGNITION, PARSE_DATETIME_BY_NEW_PARSER, READ_ANCIENT_DATETIME, or WRITE_ANCIENT_DATETIME.\n2. Depending on the error class, you can set the corresponding SQL config or datasource option to \"LEGACY\" to restore the behavior before Spark 3.0. For example, if you are dealing with a DATETIME_PATTERN_RECOGNITION error, you can set the <config> to \"LEGACY\" to restore the behavior before Spark 3.0.\n3. If you prefer to use the new parser and treat the datetime string as invalid, you can set the <config> to \"CORRECTED\" instead.\n4. For READ_ANCIENT_DATETIME and WRITE_ANCIENT_DATETIME errors, you can also set the <config> or <option> to \"CORRECTED\" to read or write the datetime values as they are, if you are sure that the written files will only be read by Spark 3.0+ or other systems that use the Proleptic Gregorian calendar.\n\nIt is recommended to consult the documentation at '<docroot>\/sql-ref-datetime-pattern.html' for forming a valid datetime pattern.","retrieved_context":[{"doc_uri":"https:\/\/docs.databricks.com\/error-messages\/inconsistent-behavior-cross-version-error-class.html","content":"Updated May 28, 2024\n \n\n[Send us feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n\n* [Documentation](..\/index.html)\n* [Databricks reference documentation](..\/reference\/api.html)\n* [Error handling in Databricks](index.html)\n* [Error classes in Databricks](error-classes.html)\n* INCONSISTENT\\_BEHAVIOR\\_CROSS\\_VERSION error class\n* \n\n# INCONSISTENT\\_BEHAVIOR\\_CROSS\\_VERSION error class\n\n[SQLSTATE: 42K0B](sqlstates.html#class-42-syntax-error-or-access-rule-violation)\n\nYou may get a different result due to the upgrading to:\n\n## DATETIME\\_PATTERN\\_RECOGNITION\n\nSpark >= 3.0:\n\nFail to recognize `<pattern>` pattern in the DateTimeFormatter.\n\n1. You can set `<config>` to \u201cLEGACY\u201d to restore the behavior before Spark 3.0.\n2. You can form a valid datetime pattern with the guide from \u2018`<docroot>`\/sql-ref-datetime-pattern.html\u2019.\n\n## PARSE\\_DATETIME\\_BY\\_NEW\\_PARSER\n\nSpark >= 3.0:\n\nFail to parse `<datetime>` in the new parser.\n\nYou can set `<config>` to \u201cLEGACY\u201d to restore the behavior before Spark 3.0, or set to \u201cCORRECTED\u201d and treat it as an invalid datetime string.\n\n## READ\\_ANCIENT\\_DATETIME\n\nSpark >= 3.0:\n\nreading dates before 1582-10-15 or timestamps before 1900-01-01T00:00:00Z\n\nfrom `<format>` files can be ambiguous, as the files may be written by\n\nSpark 2.x or legacy versions of Hive, which uses a legacy hybrid calendar\n\nthat is different from Spark 3.0+\u2019s Proleptic Gregorian calendar.\n\nSee more details in SPARK-31404. You can set the SQL config `<config>` or\n\nthe datasource option `<option>` to \u201cLEGACY\u201d to rebase the datetime values\n\nw.r.t. the calendar difference during reading. To read the datetime values\n\nas it is, set the SQL config `<config>` or the datasource option `<option>`\n\nto \u201cCORRECTED\u201d.\n\n## WRITE\\_ANCIENT\\_DATETIME\n\nSpark >= 3.0:\n\nwriting dates before 1582-10-15 or timestamps before 1900-01-01T00:00:00Z into `<format>` files can be dangerous, as the files may be read by Spark 2.x or legacy versions of Hive later, which uses a legacy hybrid calendar that is different from Spark 3.0+\u2019s Proleptic Gregorian calendar.\n\nSee more details in SPARK-31404.\n\nYou can set `<config>` to \u201cLEGACY\u201d to rebase the datetime values w.r.t. the calendar difference during writing, to get maximum interoperability.\n\nOr set the config to \u201cCORRECTED\u201d to write the datetime values as it is, if you are sure that the written files will only be read by Spark 3.0+ or other systems that use Proleptic Gregorian calendar.\n\n---\n\n \u00a9 Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the [Apache Software Foundation](http:\/\/www.apache.org\/).\n \n\n[Send us feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n \n | [Privacy Policy](https:\/\/databricks.com\/privacy-policy) | [Terms of Use](https:\/\/databricks.com\/terms-of-use)\n\n { # 's' variable stands for the search page here. TODO: Fix it. #}\n \n [![DATA+AU Summit 2024](..\/_static\/images\/dais\/docs-dais-promo-image.png)](https:\/\/www.databricks.com\/dataaisummit?utm_source=databricks&utm_medium=web&utm_campaign=7018y000001f7cvqai&utm_offer=docs \"DATA+AU Summit 2024\")"},{"doc_uri":"https:\/\/docs.databricks.com\/archive\/runtime-release-notes\/12.1.html","content":"* [[SPARK-27561]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-27561) [SC-101081][12.x][SQL] Support implicit lateral column alias resolution on Project\n* [[SPARK-41535]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-41535) [SC-118645][SQL] Set null correctly for calendar interval fields in `InterpretedUnsafeProjection` and `InterpretedMutableProjection`\n* [[SPARK-40687]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-40687) [SC-118439][SQL] Support data masking built-in function \u2018mask\u2019\n* [[SPARK-41520]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-41520) [SC-118440][SQL] Split AND\\_OR TreePattern to separate AND and OR TreePatterns\n* [[SPARK-41349]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-41349) [SC-118668][CONNECT][PYTHON] Implement DataFrame.hint\n* [[SPARK-41546]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-41546) [SC-118541][CONNECT][PYTHON] `pyspark_types_to_proto_types` should support StructType.\n* [[SPARK-41334]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-41334) [SC-118549][CONNECT][PYTHON] Move `SortOrder` proto from relations to expressions\n* [[SPARK-41387]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-41387) [SC-118450][SS] Assert current end offset from Kafka data source for Trigger.AvailableNow\n* [[SPARK-41508]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-41508) [SC-118445][CORE][SQL] Rename `_LEGACY_ERROR_TEMP_1180` to `UNEXPECTED_INPUT_TYPE` and remove `_LEGACY_ERROR_TEMP_1179`\n* [[SPARK-41319]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-41319) [SC-118441][CONNECT][PYTHON] Implement Column.{when, otherwise} and Function `when` with `UnresolvedFunction`\n* [[SPARK-41541]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-41541) [SC-118460][SQL] Fix call to wrong child method in SQLShuffleWriteMetricsReporter.decRecordsWritten()\n* [[SPARK-41453]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-41453) [SC-118458][CONNECT][PYTHON] Implement `DataFrame.subtract`\n* [[SPARK-41248]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-41248) [SC-118436][SC-118303][SQL] Add \u201cspark.sql.json.enablePartialResults\u201d to enable\/disable JSON partial results\n* [[SPARK-41437]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-41437) Revert \u201c[SC-117601][SQL] Do not optimize the inputquery twice for v1 write fallback\u201d\n* [[SPARK-41472]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-41472) [SC-118352][CONNECT][PYTHON] Implement the rest of string\/binary functions\n* [[SPARK-41526]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-41526) [SC-118355][CONNECT][PYTHON] Implement `Column.isin`\n* [[SPARK-32170]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-32170) [SC-118384] [CORE] Improve the speculation through the stage task metrics.\n* [[SPARK-41524]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-41524) [SC-118399][SS] Differentiate SQLConf and extraOptions in StateStoreConf for its usage in RocksDBConf\n* [[SPARK-41465]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-41465) [SC-118381][SQL] Assign a name to the error class *LEGACY*ERROR\\_TEMP\\_1235\n* [[SPARK-41511]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-41511) [SC-118365][SQL] LongToUnsafeRowMap support ignoresDuplicatedKey\n* [[SPARK-41409]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-41409) [SC-118302][CORE][SQL] Rename `_LEGACY_ERROR_TEMP_1043` to `WRONG_NUM_ARGS.WITHOUT_SUGGESTION`\n* [[SPARK-41438]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-41438) [SC-118344][CONNECT][PYTHON] Implement `DataFrame.colRegex`\n* [[SPARK-41437]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-41437) [SC-117601][SQL] Do not optimize the input query twice for v1 write fallback\n* [[SPARK-41314]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-41314) [SC-117172][SQL] Assign a name to the error class `_LEGACY_ERROR_TEMP_1094`\n* [[SPARK-41443]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-41443) [SC-118004][SQL] Assign a name to the error class *LEGACY*ERROR\\_TEMP\\_1061\n* [[SPARK-41506]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-41506) [SC-118241][CONNECT][PYTHON] Refactor LiteralExpression to support DataType\n* [[SPARK-41448]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-41448) [SC-118046] Make consistent MR job IDs in FileBatchWriter and FileFormatWriter\n* [[SPARK-41456]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-41456) [SC-117970][SQL] Improve the performance of try\\_cast\n* [[SPARK-41495]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-41495) [SC-118125][CONNECT][PYTHON] Implement `collection` functions: P~Z\n* [[SPARK-41478]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-41478) [SC-118167][SQL] Assign a name to the error class *LEGACY*ERROR\\_TEMP\\_1234\n* [[SPARK-41406]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-41406) [SC-118161][SQL] Refactor error message for `NUM_COLUMNS_MISMATCH` to make it more generic\n* [[SPARK-41404]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-41404) [SC-118016][SQL] Refactor `ColumnVectorUtils#toBatch` to make `ColumnarBatchSuite#testRandomRows` test more primitive dataType\n* [[SPARK-41468]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-41468) [SC-118044][SQL] Fix PlanExpression handling in EquivalentExpressions\n* [[SPARK-40775]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-40775) [SC-118045][SQL] Fix duplicate description entries for V2 file scans\n* [[SPARK-41492]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-41492) [SC-118042][CONNECT][PYTHON] Implement MISC functions\n* [[SPARK-41459]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-41459) [SC-118005][SQL] fix thrift server operation log output is empty\n* [[SPARK-41395]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-41395) [SC-117899][SQL] `InterpretedMutableProjection` should use `setDecimal` to set null values for decimals in an unsafe row\n* [[SPARK-41376]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-41376) [SC-117840][CORE][3.3] Correct the Netty preferDirectBufs check logic on executor start\n* [[SPARK-41484]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-41484) [SC-118159][SC-118036][CONNECT][PYTHON][12.x] Implement `collection` functions: E~M\n* [[SPARK-41389]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-41389) [SC-117426][CORE][SQL] Reuse `WRONG_NUM_ARGS` instead of `_LEGACY_ERROR_TEMP_1044`\n* [[SPARK-41462]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-41462) [SC-117920][SQL] Date and timestamp type can up cast to TimestampNTZ\n* [[SPARK-41435]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-41435) [SC-117810][SQL] Change to call `invalidFunctionArgumentsError` for `curdate()` when `expressions` is not empty\n* [[SPARK-41187]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-41187) [SC-118030][CORE] LiveExecutor MemoryLeak in AppStatusListener when ExecutorLost happen\n* [[SPARK-41360]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-41360) [SC-118083][CORE] Avoid BlockManager re-registration if the executor has been lost\n* [[SPARK-41378]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-41378) [SC-117686][SQL] Support Column Stats in DS v2\n* [[SPARK-41402]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-41402) [SC-117910][SQL][CONNECT][12.X] Override prettyName of StringDecode\n* [[SPARK-41414]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-41414) [SC-118041][CONNECT][PYTHON][12.x] Implement date\/timestamp functions\n* [[SPARK-41329]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-41329) [SC-117975][CONNECT] Resolve circular imports in Spark Connect\n* [[SPARK-41477]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-41477) [SC-118025][CONNECT][PYTHON] Correctly infer the datatype of literal integers\n* [[SPARK-41446]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-41446) [SC-118024][CONNECT][PYTHON][12.x] Make `createDataFrame` support schema and more input dataset types\n* [[SPARK-41475]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-41475) [SC-117997][CONNECT] Fix lint-scala command error and typo\n* [[SPARK-38277]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-38277) [SC-117799][SS] Clear write batch after RocksDB state store\u2019s commit\n* [[SPARK-41375]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-41375) [SC-117801][SS] Avoid empty latest KafkaSourceOffset\n* [[SPARK-41412]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-41412) [SC-118015][CONNECT] Implement `Column.cast`\n* [[SPARK-41439]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-41439) [SC-117893][CONNECT][PYTHON] Implement `DataFrame.melt` and `DataFrame.unpivot`\n* [[SPARK-41399]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-41399) [SC-118007][SC-117474][CONNECT] Refactor column related tests to test\\_connect\\_column\n* [[SPARK-41351]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-41351) [SC-117957][SC-117412][CONNECT][12.x] Column should support != operator\n* [[SPARK-40697]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-40697) [SC-117806][SC-112787][SQL] Add read-side char padding to cover external data files\n* [[SPARK-41349]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-41349) [SC-117594][CONNECT][12.X] Implement DataFrame.hint\n* [[SPARK-41338]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-41338) [SC-117170][SQL] Resolve outer references and normal columns in the same analyzer batch\n* [[SPARK-41436]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-41436) [SC-117805][CONNECT][PYTHON] Implement `collection` functions: A~C\n* [[SPARK-41445]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-41445) [SC-117802][CONNECT] Implement DataFrameReader.parquet\n* [[SPARK-41452]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-41452) [SC-117865][SQL] `to_char` should return null when format is null"},{"doc_uri":"https:\/\/docs.databricks.com\/release-notes\/runtime\/15.2.html","content":"* [[SPARK-47487]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-47487) [SC-160534][SQL] Simplify code in AnsiTypeCoercion\n* [[SPARK-47443]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-47443) [SC-160459][SQL] Window Aggregate support for collations\n* [[SPARK-47296]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-47296) [SC-160457][SQL][COLLATION] Fail unsupported functions for non-binary collations\n* [[SPARK-47380]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-47380) [SC-160164][CONNECT] Ensure on the server side that the SparkSession is the same\n* [[SPARK-47327]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-47327) [SC-160069][SQL] Move sort keys concurrency test to CollationFactorySuite\n* [[SPARK-47494]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-47494) [SC-160495][Doc] Add migration doc for the behavior change of Parquet timestamp inference since Spark 3.3\n* [[SPARK-47449]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-47449) [SC-160372][SS] Refactor and split list\/timer unit tests\n* [[SPARK-46473]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-46473) [SC-155663][SQL] Reuse `getPartitionedFile` method\n* [[SPARK-47423]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-47423) [SC-160068][SQL] Collations - Set operation support for strings with collations\n* [[SPARK-47439]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-47439) [SC-160115][PYTHON] Document Python Data Source API in API reference page\n* [[SPARK-47457]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-47457) [SC-160234][SQL] Fix `IsolatedClientLoader.supportsHadoopShadedClient` to handle Hadoop 3.4+\n* [[SPARK-47366]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-47366) [SC-159348][SQL] Implement parse\\_json.\n* [[SPARK-46331]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-46331) [SC-152982][SQL] Removing CodegenFallback from subset of DateTime expressions and version() expression\n* [[SPARK-47395]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-47395) [SC-159404] Add collate and collation to other APIs\n* [[SPARK-47437]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-47437) [SC-160117][PYTHON][CONNECT] Correct the error class for `DataFrame.sort*`\n* [[SPARK-47174]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-47174) [SC-154483][CONNECT][SS][1\/2] Server side SparkConnectListenerBusListener for Client side streaming query listener\n* [[SPARK-47324]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-47324) [SC-158720][SQL] Add missing timestamp conversion for JDBC nested types\n* [[SPARK-46962]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-46962) [SC-158834][SS][PYTHON] Add interface for python streaming data source API and implement python worker to run python streaming data source\n* [[SPARK-45827]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-45827) [SC-158498][SQL] Move data type checks to CreatableRelationProvider\n* [[SPARK-47342]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-47342) [SC-158874][SQL] Support TimestampNTZ for DB2 TIMESTAMP WITH TIME ZONE\n* [[SPARK-47399]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-47399) [SC-159378][SQL] Disable generated columns on expressions with collations\n* [[SPARK-47146]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-47146) [SC-158247][CORE] Possible thread leak when doing sort merge join\n* [[SPARK-46913]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-46913) [SC-159149][SS] Add support for processing\/event time based timers with transformWithState operator\n* [[SPARK-47375]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-47375) [SC-159063][SQL] Add guidelines for timestamp mapping in `JdbcDialect#getCatalystType`\n* [[SPARK-47394]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-47394) [SC-159282][SQL] Support TIMESTAMP WITH TIME ZONE for H2Dialect\n* [[SPARK-45827]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-45827) Revert \u201c[SC-158498][SQL] Move data type checks to \u2026\n* [[SPARK-47208]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-47208) [SC-159279][CORE] Allow overriding base overhead memory\n* [[SPARK-42627]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-42627) [SC-158021][SPARK-26494][SQL] Support Oracle TIMESTAMP WITH LOCAL TIME ZONE\n* [[SPARK-47055]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-47055) [SC-156916][PYTHON] Upgrade MyPy 1.8.0\n* [[SPARK-46906]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-46906) [SC-157205][SS] Add a check for stateful operator change for streaming\n* [[SPARK-47391]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-47391) [SC-159283][SQL] Remove the test case workaround for JDK 8\n* [[SPARK-47272]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-47272) [SC-158960][SS] Add MapState implementation for State API v2.\n* [[SPARK-47375]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-47375) [SC-159278][Doc][FollowUp] Fix a mistake in JDBC\u2019s preferTimestampNTZ option doc\n* [[SPARK-42328]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-42328) [SC-157363][SQL] Remove *LEGACY*ERROR\\_TEMP\\_1175 from error classes\n* [[SPARK-47375]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-47375) [SC-159261][Doc][FollowUp] Correct the preferTimestampNTZ option description in JDBC doc\n* [[SPARK-47344]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-47344) [SC-159146] Extend INVALID\\_IDENTIFIER error beyond catching \u2018-\u2018 in an unquoted identifier and fix \u201cIS ! NULL\u201d et al.\n* [[SPARK-47340]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-47340) [SC-159039][SQL] Change \u201ccollate\u201d in StringType typename to lowercase\n* [[SPARK-47087]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-47087) [SC-157077][SQL] Raise Spark\u2019s exception with an error class in config value check\n* [[SPARK-47327]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-47327) [SC-158824][SQL] Fix thread safety issue in ICU Collator\n* [[SPARK-47082]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-47082) [SC-157058][SQL] Fix out-of-bounds error condition\n* [[SPARK-47331]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-47331) [SC-158719][SS] Serialization using case classes\/primitives\/POJO based on SQL encoder for Arbitrary State API v2.\n* [[SPARK-47250]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-47250) [SC-158840][SS] Add additional validations and NERF changes for RocksDB state provider and use of column families\n* [[SPARK-47328]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-47328) [SC-158745][SQL] Rename UCS\\_BASIC collation to UTF8\\_BINARY\n* [[SPARK-47207]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-47207) [SC-157845][CORE] Support `spark.driver.timeout` and `DriverTimeoutPlugin`\n* [[SPARK-47370]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-47370) [SC-158956][Doc] Add migration doc: TimestampNTZ type inference on Parquet files\n* [[SPARK-47309]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-47309) [SC-158827][SQL][XML] Add schema inference unit tests\n* [[SPARK-47295]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-47295) [SC-158850][SQL] Added ICU StringSearch for the `startsWith` and `endsWith` functions\n* [[SPARK-47343]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-47343) [SC-158851][SQL] Fix NPE when `sqlString` variable value is null string in execute immediate\n* [[SPARK-46293]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-46293) [SC-150117][CONNECT][PYTHON] Use `protobuf` transitive dependency\n* [[SPARK-46795]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-46795) [SC-154143][SQL] Replace `UnsupportedOperationException` by `SparkUnsupportedOperationException` in `sql\/core`\n* [[SPARK-46087]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-46087) [SC-149023][PYTHON] Sync PySpark dependencies in docs and dev requirements\n* [[SPARK-47169]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-47169) [SC-158848][SQL] Disable bucketing on collated columns\n* [[SPARK-42332]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-42332) [SC-153996][SQL] Changing the require to a SparkException in ComplexTypeMergingExpression\n* [[SPARK-45827]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-45827) [SC-158498][SQL] Move data type checks to CreatableRelationProvider\n* [[SPARK-47341]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-47341) [SC-158825][Connect] Replace commands with relations in a few tests in SparkConnectClientSuite\n* [[SPARK-43255]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-43255) [SC-158026][SQL] Replace the error class *LEGACY*ERROR\\_TEMP\\_2020 by an internal error\n* [[SPARK-47248]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-47248) [SC-158494][SQL][COLLATION] Improved string function support: contains\n* [[SPARK-47334]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-47334) [SC-158716][SQL] Make `withColumnRenamed` reuse the implementation of `withColumnsRenamed`\n* [[SPARK-46442]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-46442) [SC-153168][SQL] DS V2 supports push down PERCENTILE\\_CONT and PERCENTILE\\_DISC\n* [[SPARK-47313]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-47313) [SC-158747][SQL] Added scala.MatchError handling inside QueryExecution.toInternalError\n* [[SPARK-45827]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-45827) [SC-158732][SQL] Add variant singleton type for Java\n* [[SPARK-47337]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-47337) [SC-158743][SQL][DOCKER] Upgrade DB2 docker image version to 11.5.8.0\n* [[SPARK-47302]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-47302) [SC-158609][SQL] Collate keyword as identifier\n* [[SPARK-46817]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-46817) [SC-154196][CORE] Fix `spark-daemon.sh` usage by adding `decommission` command\n* [[SPARK-46739]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-46739) [SC-153553][SQL] Add the error class `UNSUPPORTED_CALL`"},{"doc_uri":"https:\/\/docs.databricks.com\/archive\/runtime-release-notes\/7.0.html","content":"* Deprecate Python 2 support ([SPARK-27884](https:\/\/issues.apache.org\/jira\/browse\/SPARK-27884))\n* Deprecate R < 3.4 support ([SPARK-26014](https:\/\/issues.apache.org\/jira\/browse\/SPARK-26014))\n\n### [Known issues](#id8)\n\n* Parsing day of year using pattern letter \u2018D\u2019 returns the wrong result if the year field is missing. This can happen in SQL functions like `to_timestamp` which parses datetime string to datetime values using a pattern string. ([SPARK-31939](https:\/\/issues.apache.org\/jira\/browse\/SPARK-31939))\n* Join\/Window\/Aggregate inside subqueries may lead to wrong results if the keys have values -0.0 and 0.0. ([SPARK-31958](https:\/\/issues.apache.org\/jira\/browse\/SPARK-31958))\n* A window query may fail with ambiguous self-join error unexpectedly. ([SPARK-31956](https:\/\/issues.apache.org\/jira\/browse\/SPARK-31956))\n* Streaming queries with `dropDuplicates` operator may not be able to restart with the checkpoint written by Spark 2.x. ([SPARK-31990](https:\/\/issues.apache.org\/jira\/browse\/SPARK-31990))\n\n## Maintenance updates\n\nSee [Databricks Runtime 7.0 maintenance updates](maintenance-updates-archive.html#70).\n\n## System environment\n\n* **Operating System**: Ubuntu 18.04.4 LTS\n* **Java**: 1.8.0\\_252\n* **Scala**: 2.12.10\n* **Python**: 3.7.5\n* **R**: R version 3.6.3 (2020-02-29)\n* **Delta Lake** 0.7.0\n\n### Installed Python libraries\n\n| Library | Version | Library | Version | Library | Version |\n| --- | --- | --- | --- | --- | --- |\n| asn1crypto | 1.3.0 | backcall | 0.1.0 | boto3 | 1.12.0 |\n| botocore | 1.15.0 | certifi | 2020.4.5 | cffi | 1.14.0 |\n| chardet | 3.0.4 | cryptography | 2.8 | cycler | 0.10.0 |\n| Cython | 0.29.15 | decorator | 4.4.1 | docutils | 0.15.2 |\n| entrypoints | 0.3 | idna | 2.8 | ipykernel | 5.1.4 |\n| ipython | 7.12.0 | ipython-genutils | 0.2.0 | jedi | 0.14.1 |\n| jmespath | 0.9.4 | joblib | 0.14.1 | jupyter-client | 5.3.4 |\n| jupyter-core | 4.6.1 | kiwisolver | 1.1.0 | matplotlib | 3.1.3 |\n| numpy | 1.18.1 | pandas | 1.0.1 | parso | 0.5.2 |\n| patsy | 0.5.1 | pexpect | 4.8.0 | pickleshare | 0.7.5 |\n| pip | 20.0.2 | prompt-toolkit | 3.0.3 | psycopg2 | 2.8.4 |\n| ptyprocess | 0.6.0 | pyarrow | 0.15.1 | pycparser | 2.19 |\n| Pygments | 2.5.2 | PyGObject | 3.26.1 | pyOpenSSL | 19.1.0 |\n| pyparsing | 2.4.6 | PySocks | 1.7.1 | python-apt | 1.6.5+ubuntu0.3 |\n| python-dateutil | 2.8.1 | pytz | 2019.3 | pyzmq | 18.1.1 |\n| requests | 2.22.0 | s3transfer | 0.3.3 | scikit-learn | 0.22.1 |\n| scipy | 1.4.1 | seaborn | 0.10.0 | setuptools | 45.2.0 |\n| six | 1.14.0 | ssh-import-id | 5.7 | statsmodels | 0.11.0 |\n| tornado | 6.0.3 | traitlets | 4.3.3 | unattended-upgrades | 0.1 |\n| urllib3 | 1.25.8 | virtualenv | 16.7.10 | wcwidth | 0.1.8 |\n| wheel | 0.34.2 |  |  |  |  |\n\n### Installed R libraries\n\nR libraries are installed from Microsoft CRAN snapshot on 2020-04-22.\n\n| Library | Version | Library | Version | Library | Version |\n| --- | --- | --- | --- | --- | --- |\n| askpass | 1.1 | assertthat | 0.2.1 | backports | 1.1.6 |\n| base | 3.6.3 | base64enc | 0.1-3 | BH | 1.72.0-3 |\n| bit | 1.1-15.2 | bit64 | 0.9-7 | blob | 1.2.1 |\n| boot | 1.3-25 | brew | 1.0-6 | broom | 0.5.6 |\n| callr | 3.4.3 | caret | 6.0-86 | cellranger | 1.1.0 |\n| chron | 2.3-55 | class | 7.3-17 | cli | 2.0.2 |\n| clipr | 0.7.0 | cluster | 2.1.0 | codetools | 0.2-16 |\n| colorspace | 1.4-1 | commonmark | 1.7 | compiler | 3.6.3 |\n| config | 0.3 | covr | 3.5.0 | crayon | 1.3.4 |\n| crosstalk | 1.1.0.1 | curl | 4.3 | data.table | 1.12.8 |\n| datasets | 3.6.3 | DBI | 1.1.0 | dbplyr | 1.4.3 |\n| desc | 1.2.0 | devtools | 2.3.0 | digest | 0.6.25 |\n| dplyr | 0.8.5 | DT | 0.13 | ellipsis | 0.3.0 |\n| evaluate | 0.14 | fansi | 0.4.1 | farver | 2.0.3 |\n| fastmap | 1.0.1 | forcats | 0.5.0 | foreach | 1.5.0 |\n| foreign | 0.8-76 | forge | 0.2.0 | fs | 1.4.1 |\n| generics | 0.0.2 | ggplot2 | 3.3.0 | gh | 1.1.0 |\n| git2r | 0.26.1 | glmnet | 3.0-2 | globals | 0.12.5 |\n| glue | 1.4.0 | gower | 0.2.1 | graphics | 3.6.3 |\n| grDevices | 3.6.3 | grid | 3.6.3 | gridExtra | 2.3 |\n| gsubfn | 0.7 | gtable | 0.3.0 | haven | 2.2.0 |\n| highr | 0.8 | hms | 0.5.3 | htmltools | 0.4.0 |\n| htmlwidgets | 1.5.1 | httpuv | 1.5.2 | httr | 1.4.1 |\n| hwriter | 1.3.2 | hwriterPlus | 1.0-3 | ini | 0.3.1 |\n| ipred | 0.9-9 | isoband | 0.2.1 | iterators | 1.0.12 |\n| jsonlite | 1.6.1 | KernSmooth | 2.23-17 | knitr | 1.28 |\n| labeling | 0.3 | later | 1.0.0 | lattice | 0.20-41 |\n| lava | 1.6.7 | lazyeval | 0.2.2 | lifecycle | 0.2.0 |\n| lubridate | 1.7.8 | magrittr | 1.5 | markdown | 1.1 |\n| MASS | 7.3-51.6 | Matrix | 1.2-18 | memoise | 1.1.0 |\n| methods | 3.6.3 | mgcv | 1.8-31 | mime | 0.9 |\n| ModelMetrics | 1.2.2.2 | modelr | 0.1.6 | munsell | 0.5.0 |\n| nlme | 3.1-147 | nnet | 7.3-14 | numDeriv | 2016.8-1.1 |\n| openssl | 1.4.1 | parallel | 3.6.3 | pillar | 1.4.3 |\n| pkgbuild | 1.0.6 | pkgconfig | 2.0.3 | pkgload | 1.0.2 |\n| plogr | 0.2.0 | plyr | 1.8.6 | praise | 1.0.0 |\n| prettyunits | 1.1.1 | pROC | 1.16.2 | processx | 3.4.2 |\n| prodlim | 2019.11.13 | progress | 1.2.2 | promises | 1.1.0 |\n| proto | 1.0.0 | ps | 1.3.2 | purrr | 0.3.4 |\n| r2d3 | 0.2.3 | R6 | 2.4.1 | randomForest | 4.6-14 |\n| rappdirs | 0.3.1 | rcmdcheck | 1.3.3 | RColorBrewer | 1.1-2 |\n| Rcpp | 1.0.4.6 | readr | 1.3.1 | readxl | 1.3.1 |\n| recipes | 0.1.10 | rematch | 1.0.1 | rematch2 | 2.1.1 |\n| remotes | 2.1.1 | reprex | 0.3.0 | reshape2 | 1.4.4 |\n| rex | 1.2.0 | rjson | 0.2.20 | rlang | 0.4.5 |\n| rmarkdown | 2.1 | RODBC | 1.3-16 | roxygen2 | 7.1.0 |\n| rpart | 4.1-15 | rprojroot | 1.3-2 | Rserve | 1.8-6 |\n| RSQLite | 2.2.0 | rstudioapi | 0.11 | rversions | 2.0.1 |\n| rvest | 0.3.5 | scales | 1.1.0 | selectr | 0.4-2 |\n| sessioninfo | 1.1.1 | shape | 1.4.4 | shiny | 1.4.0.2 |\n| sourcetools | 0.1.7 | sparklyr | 1.2.0 | SparkR | 3.0.0 |\n| spatial | 7.3-11 | splines | 3.6.3 | sqldf | 0.4-11 |\n| SQUAREM | 2020.2 | stats | 3.6.3 | stats4 | 3.6.3 |\n| stringi | 1.4.6 | stringr | 1.4.0 | survival | 3.1-12 |\n| sys | 3.3 | tcltk | 3.6.3 | TeachingDemos | 2.10 |\n| testthat | 2.3.2 | tibble | 3.0.1 | tidyr | 1.0.2 |\n| tidyselect | 1.0.0 | tidyverse | 1.3.0 | timeDate | 3043.102 |\n| tinytex | 0.22 | tools | 3.6.3 | usethis | 1.6.0 |\n| utf8 | 1.1.4 | utils | 3.6.3 | vctrs | 0.2.4 |\n| viridisLite | 0.3.0 | whisker | 0.4 | withr | 2.2.0 |\n| xfun | 0.13 | xml2 | 1.3.1 | xopen | 1.0.0 |\n| xtable | 1.8-4 | yaml | 2.2.1 |  |  |\n\n### Installed Java and Scala libraries (Scala 2.12 cluster version)"},{"doc_uri":"https:\/\/docs.databricks.com\/archive\/runtime-release-notes\/3.0.html","content":"+ [[SPARK-20195]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-20195): deprecate createExternalTable\n\n### Changes of Behavior\n\n* MLlib\n\n\t+ [[SPARK-19787]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-19787): DeveloperApi ALS.train() uses default regParam value 0.1 instead of 1.0, in order to match regular ALS API\u2019s default regParam setting.\n* SPARKR\n\n\t+ [[SPARK-19291]](https:\/\/issues.apache.org\/jira\/browse\/SPARK-19291): This added log-likelihood for SparkR Gaussian Mixture Models, but doing so introduced a SparkR model persistence incompatibility: Gaussian Mixture Models saved from SparkR 2.1 may not be loaded into SparkR 2.2. We plan to put in place backwards compatibility guarantees for SparkR in the future.\n\n## Known Issues\n\n* Log links on the executor page are not set correctly. Please use the worker page to access stdout and stderr links of an executor for now.\n\n## System Environment\n\n* **Operating System**: Ubuntu 16.04.2 LTS\n* **Java**: 1.8.0\\_131\n* **Scala**: 2.10.6 (Scala 2.10 cluster version)\/2.11.8 (Scala 2.11 cluster version)\n* **Python**: 2.7.12 (or 3.5.2 if using Python 3)\n* **R**: R version 3.2.3 (2015-12-10)\n\n### Pre-installed Python Libraries\n\n| Library | Version | Library | Version | Library | Version |\n| --- | --- | --- | --- | --- | --- |\n| ansi2html | 1.1.1 | argparse | 1.2.1 | boto | 2.42.0 |\n| boto3 | 1.4.1 | botocore | 1.4.70 | brewer2mpl | 1.4.1 |\n| certifi | 2016.2.28 | cffi | 1.7.0 | chardet | 2.3.0 |\n| colorama | 0.3.7 | configobj | 5.0.6 | cryptography | 1.5 |\n| cycler | 0.10.0 | Cython | 0.24.1 | decorator | 4.0.10 |\n| docutils | 0.13.1 | enum34 | 1.1.6 | et-xmlfile | 1.0.1 |\n| freetype-py | 1.0.2 | funcsigs | 1.0.2 | fusepy | 2.0.4 |\n| futures | 3.1.1 | ggplot | 0.6.8 | html5lib | 0.999 |\n| idna | 2.1 | ipaddress | 1.0.16 | ipython | 2.2.0 |\n| ipython-genutils | 0.1.0 | jdcal | 1.2 | Jinja2 | 2.8 |\n| jmespath | 0.9.0 | llvmlite | 0.13.0 | lxml | 3.6.4 |\n| MarkupSafe | 0.23 | matplotlib | 1.5.3 | mpld3 | 0.2 |\n| msgpack-python | 0.4.7 | ndg-httpsclient | 0.3.3 | numba | 0.28.1 |\n| numpy | 1.11.1 | openpyxl | 2.3.2 | pandas | 0.18.1 |\n| pathlib2 | 2.1.0 | patsy | 0.4.1 | pexpect | 4.0.1 |\n| pickleshare | 0.7.4 | Pillow | 3.3.1 | pip | 9.0.1 |\n| ply | 3.9 | prompt-toolkit | 1.0.7 | psycopg2 | 2.6.2 |\n| ptyprocess | 0.5.1 | py4j | 0.10.3 | pyasn1 | 0.1.9 |\n| pycparser | 2.14 | Pygments | 2.1.3 | PyGObject | 3.20.0 |\n| pyOpenSSL | 16.0.0 | pyparsing | 2.2.0 | pypng | 0.0.18 |\n| Python | 2.7.12 | python-dateutil | 2.5.3 | python-geohash | 0.8.5 |\n| pytz | 2016.6.1 | requests | 2.11.1 | s3transfer | 0.1.9 |\n| scikit-learn | 0.17.1 | scipy | 0.18.1 | scour | 0.32 |\n| seaborn | 0.7.1 | setuptools | 36.0.1 | simplejson | 3.8.2 |\n| simples3 | 1.0 | singledispatch | 3.4.0.3 | six | 1.10.0 |\n| statsmodels | 0.6.1 | traitlets | 4.3.0 | urllib3 | 1.19.1 |\n| virtualenv | 15.0.1 | wcwidth | 0.1.7 | wheel | 0.30.0a0 |\n| wsgiref | 0.1.2 |  |  |  |  |\n\n### Pre-installed R Libraries\n\n| Library | Version | Library | Version | Library | Version |\n| --- | --- | --- | --- | --- | --- |\n| abind | 1.4-3 | assertthat | 0.1 | base | 3.2.3 |\n| BH | 1.60.0-2 | bitops | 1.0-6 | boot | 1.3-17 |\n| brew | 1.0-6 | car | 2.1-3 | caret | 6.0-71 |\n| chron | 2.3-47 | class | 7.3-14 | cluster | 2.0.5 |\n| codetools | 0.2-14 | colorspace | 1.2-4 | compiler | 3.2.3 |\n| crayon | 1.3.1 | curl | 2.2 | data.table | 1.9.6 |\n| datasets | 3.2.3 | DBI | 0.5-1 | devtools | 1.12.0 |\n| dichromat | 2.0-0 | digest | 0.6.9 | doMC | 1.3.4 |\n| dplyr | 0.5.0 | foreach | 1.4.3 | foreign | 0.8-66 |\n| gbm | 2.1.1 | ggplot2 | 2.1.0 | git2r | 0.15.0 |\n| glmnet | 2.0-5 | graphics | 3.2.3 | grDevices | 3.2.3 |\n| grid | 3.2.3 | gsubfn | 0.6-6 | gtable | 0.1.2 |\n| h2o | 3.10.0.8 | httr | 1.2.1 | hwriter | 1.3.2 |\n| hwriterPlus | 1.0-3 | iterators | 1.0.8 | jsonlite | 1.1 |\n| KernSmooth | 2.23-15 | labeling | 0.3 | lattice | 0.20-34 |\n| lazyeval | 0.2.0 | littler | 0.3.0 | lme4 | 1.1-12 |\n| lubridate | 1.6.0 | magrittr | 1.5 | mapproj | 1.2-4 |\n| maps | 3.0.2 | MASS | 7.3-45 | Matrix | 1.2-7.1 |\n| MatrixModels | 0.4-1 | memoise | 1.0.0 | methods | 3.2.3 |\n| mgcv | 1.8-11 | mime | 0.5 | minqa | 1.2.4 |\n| multicore | 0.2 | munsell | 0.4.2 | mvtnorm | 1.0-5 |\n| nlme | 3.1-124 | nloptr | 1.0.4 | nnet | 7.3-12 |\n| openssl | 0.9.4 | parallel | 3.2.3 | pbkrtest | 0.4-6 |\n| pkgKitten | 0.1.3 | plyr | 1.8.4 | praise | 1.0.0 |\n| pROC | 1.8 | proto | 0.3-10 | quantreg | 5.29 |\n| R.methodsS3 | 1.7.1 | R.oo | 1.20.0 | R.utils | 2.4.0 |\n| R6 | 2.2.0 | randomForest | 4.6-12 | RColorBrewer | 1.1-2 |\n| Rcpp | 0.12.7 | RcppEigen | 0.3.2.9.0 | RCurl | 1.95-4.8 |\n| reshape2 | 1.4.2 | RODBC | 1.3-12 | roxygen2 | 5.0.1 |\n| rpart | 4.1-10 | Rserve | 1.7-3 | RSQLite | 1.0.0 |\n| rstudioapi | 0.6 | scales | 0.3.0 | sp | 1.0-15 |\n| SparkR | 2.2.0 | SparseM | 1.72 | spatial | 7.3-11 |\n| splines | 3.2.3 | sqldf | 0.4-10 | statmod | 1.4.26 |\n| stats | 3.2.3 | stats4 | 3.2.3 | stringi | 1.0-1 |\n| stringr | 1.0.0 | survival | 2.38-3 | tcltk | 3.2.3 |\n| TeachingDemos | 2.10 | testthat | 1.0.2 | tibble | 1.2 |\n| tools | 3.2.3 | utils | 3.2.3 | whisker | 0.3-2 |\n| withr | 1.0.2 |  |  |  |  |\n\n### Pre-installed Java and Scala libraries (Scala 2.10 cluster version)"}]}
{"request_id":"id-3831","request":"How can I enable Hive metastore table access control on a cluster in Databricks?","response":"To enable Hive metastore table access control on a cluster in Databricks, you can follow these steps:\n\n1. In your Databricks workspace, click on the \"Clusters\" tab.\n2. Select the cluster you want to enable table access control for.\n3. Click on the \"Edit\" button.\n4. Scroll down to the \"Advanced Options\" section and click on the \"Spark\" tab.\n5. In the \"Spark Config\" box, add the following configuration key and value:\n```makefile\nspark.databricks.acl.sqlOnly true\n```\n6. Click on the \"Confirm\" button to save the changes.\n\nThis will enable SQL-only table access control for the cluster, restricting users to SQL commands only. If you want to enable Python and SQL table access control, you can add the following configuration key and value instead:\n```makefile\nspark.databricks.acl.sqlOnly false\nspark.databricks.pyspark.iptable.outbound.whitelisted.ports <port1>,<port2>,...\n```\nReplace `<port1>,<port2>,...` with the ports you want to allow access to.\n\nTo manage privileges on objects in the Hive metastore, you can use SQL commands such as `GRANT`, `REVOKE`, and `DENY`. For example, to grant the `SELECT` privilege on a table to a user, you can use the following command:\n```sql\nGRANT SELECT ON TABLE <table-name> TO <user-name>\n```\nReplace `<table-name>` with the name of the table and `<user-name>` with the name of the user.\n\nTo revoke the `SELECT` privilege on a table from a user, you can use the following command:\n```sql\nREVOKE SELECT ON TABLE <table-name> FROM <user-name>\n```\nReplace `<table-name>` with the name of the table and `<user-name>` with the name of the user.\n\nTo deny the `SELECT` privilege on a table to a user, you can use the following command:\n```sql\nDENY SELECT ON TABLE <table-name> TO <user-name>\n```\nReplace `<table-name>` with the name of the table and `<user-name>` with the name of the user.\n\nNote that table access control is not supported with Machine Learning Runtime, and even if table access control is enabled for a cluster, Databricks workspace administrators have access to file-level data.","expected_retrieved_context":[{"doc_uri":"https:\/\/docs.databricks.com\/data-governance\/table-acls\/index.html","content":"Hive metastore table access control (legacy)  \nEach Databricks workspace deploys with a built-in Hive metastore as a managed service. An instance of the metastore deploys to each cluster and securely accesses metadata from a central per-workspace repository.  \nBy default, a cluster allows all users to access all data managed by the workspace\u2019s built-in Hive metastore unless table access control is enabled for that cluster. Table access control lets you programmatically grant and revoke access to objects in your workspace\u2019s Hive metastore from Python and SQL. When table access control is enabled, users can set permissions for data objects that are accessed using that cluster.  \nNote  \nHive metastore table access control is a legacy data governance model. Databricks recommends that you upgrade the tables managed by the Hive metastore to the Unity Catalog metastore. Unity Catalog simplifies security and governance of your data by providing a central place to administer and audit data access across multiple workspaces in your account.  \nRequirements\nRequirements\nThis feature requires the Premium plan or above.  \nThis feature requires a Data Science & Engineering cluster with an appropriate configuration or a SQL warehouse.  \nThis section covers:  \nEnable Hive metastore table access control on a cluster (legacy) Hive metastore privileges and securable objects (legacy) What is the ANY FILE securable?"}],"expected_response":"To enable Hive metastore table access control on a cluster in Databricks, you need to follow these steps:\n\n1. Navigate to the Databricks workspace.\n2. Create a new cluster or select an existing one.\n3. In the cluster configuration page, scroll down to the \"Advanced Options\" section.\n4. Click on the \"Access Control\" tab.\n5. Check the box for \"Enable table access control (legacy)\".\n6. Click on the \"Create\" or \"Update\" button to save the changes.\n\nPlease note that this feature requires the Premium plan or above and a Data Science & Engineering cluster with an appropriate configuration or a SQL warehouse.\n\nRegarding the ANY FILE securable, it is a special type of securable object in the legacy Hive metastore table access control model that represents all files in a given directory or bucket. It allows you to grant or revoke access to all files in a specific location, regardless of their file name or extension.","retrieved_context":[{"doc_uri":"https:\/\/docs.databricks.com\/data-governance\/table-acls\/table-acl.html","content":"Enable Hive metastore table access control on a cluster (legacy) | Databricks on AWS\n\n* [![Databricks](..\/..\/_static\/small-scale-lockup-full-color-rgb.svg)](https:\/\/www.databricks.com\/)\n\n* [![Databricks](..\/..\/_static\/small-scale-lockup-full-color-rgb.svg)](https:\/\/www.databricks.com\/)\n* [Help Center](https:\/\/help.databricks.com\/s\/)\n* [Documentation](https:\/\/docs.databricks.com\/en\/index.html)\n* [Knowledge Base](https:\/\/kb.databricks.com\/)\n\n* [Community](https:\/\/community.databricks.com)\n* [Support](https:\/\/help.databricks.com)\n* [Feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n* [Try Databricks](https:\/\/databricks.com\/try-databricks)\n\n[![](..\/..\/_static\/icons\/globe.png)English](javascript:void(0))\n* [English](..\/..\/..\/en\/data-governance\/table-acls\/table-acl.html)\n* [\u65e5\u672c\u8a9e](..\/..\/..\/ja\/data-governance\/table-acls\/table-acl.html)\n* [Portugu\u00eas](..\/..\/..\/pt\/data-governance\/table-acls\/table-acl.html)\n\n[![](..\/..\/_static\/icons\/aws.svg)Amazon\n Web Services](javascript:void(0))\n* [![](..\/..\/_static\/icons\/azure.svg)Microsoft Azure](https:\/\/learn.microsoft.com\/azure\/databricks\/data-governance\/table-acls\/table-acl)\n* [![](..\/..\/_static\/icons\/gcp.svg)Google Cloud Platform](https:\/\/docs.gcp.databricks.com\/data-governance\/table-acls\/table-acl.html)\n\n[Databricks on AWS](..\/..\/index.html)\nGet started\n\n* [Get started](..\/..\/getting-started\/index.html)\n* [What is Databricks?](..\/..\/introduction\/index.html)\n* [DatabricksIQ](..\/..\/databricksiq\/index.html)\n* [Release notes](..\/..\/release-notes\/index.html)\n\nLoad & manage data\n\n* [Connect to data sources](..\/..\/connect\/index.html)\n* [Connect to compute](..\/..\/compute\/index.html)\n* [Discover data](..\/..\/discover\/index.html)\n* [Query data](..\/..\/query\/index.html)\n* [Ingest data](..\/..\/ingestion\/index.html)\n* [Transform data](..\/..\/transform\/index.html)\n* [Monitor data and AI assets](..\/..\/lakehouse-monitoring\/index.html)\n* [Share data (Delta Sharing)](..\/..\/data-sharing\/index.html)\n* [Databricks Marketplace](..\/..\/marketplace\/index.html)\n\nWork with data\n\n* [Data engineering](..\/..\/workspace-index.html)\n* [Generative AI & LLMs](..\/..\/generative-ai\/generative-ai.html)\n* [Machine learning](..\/..\/machine-learning\/index.html)\n* [Model serving](..\/..\/machine-learning\/model-serving\/index.html)\n* [Data warehousing](..\/..\/sql\/index.html)\n* [Delta Lake](..\/..\/delta\/index.html)\n* [Developers](..\/..\/languages\/index.html)\n* [Technology partners](..\/..\/integrations\/index.html)\n\nAdministration\n\n* [Account and workspace administration](..\/..\/admin\/index.html)\n* [Security and compliance](..\/..\/security\/index.html)\n* [Data governance (Unity Catalog)](..\/index.html)\n\t+ [Unity Catalog](..\/unity-catalog\/index.html)\n\t+ [What is Catalog Explorer?](..\/..\/catalog-explorer\/index.html)\n\t+ [Hive metastore table access control (legacy)](index.html)\n\t\t- [Enable Hive metastore table access control on a cluster (legacy)](#)\n\t\t- [Hive metastore privileges and securable objects (legacy)](object-privileges.html)\n\t\t- [What is the `ANY FILE` securable?](any-file.html)\n* [Lakehouse architecture](..\/..\/lakehouse-architecture\/index.html)\n\nReference & resources\n\n* [Reference](..\/..\/reference\/api.html)\n* [Resources](..\/..\/resources\/index.html)\n* [What\u2019s coming?](..\/..\/whats-coming.html)\n* [Documentation archive](..\/..\/archive\/index.html)\n\n Updated May 28, 2024\n \n\n[Send us feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n\n* [Documentation](..\/..\/index.html)\n* [Data governance with Unity Catalog](..\/index.html)\n* [Hive metastore table access control (legacy)](index.html)\n* Enable Hive metastore table access control on a cluster (legacy)\n* \n\n# Enable Hive metastore table access control on a cluster (legacy)\n\nThis article describes how to enable table access control for the built-in Hive metastore on a cluster.\n\nFor information about how to set privileges on Hive metastore securable objects once table access control has been enabled on a cluster, see [Hive metastore privileges and securable objects (legacy)](object-privileges.html).\n\nNote\n\nHive metastore table access control is a legacy data governance model. Databricks recommends that you use Unity Catalog instead for its simplicity and account-centered governance model. You can [upgrade the tables managed by the Hive metastore to the Unity Catalog metastore](..\/unity-catalog\/migrate.html).\n\n## Enable table access control for a cluster\n\nTable access control is available in two versions:\n\n* [SQL-only table access control](#sql-only-table-access-control), which restricts users to SQL commands.\n* [Python and SQL table access control](#python-and-sql-table-access-control), which allows users to run SQL, Python, and PySpark commands.\n\nTable access control is not supported with [Machine Learning Runtime](..\/..\/machine-learning\/index.html).\n\nImportant\n\nEven if table access control is enabled for a cluster, Databricks workspace administrators have access to file-level data.\n\n### SQL-only table access control\n\nThis version of table access control restricts users to SQL commands only.\n\nTo enable SQL-only table access control on a cluster and restrict that cluster to use only SQL commands, set\nthe following flag in the cluster\u2019s [Spark conf](..\/..\/compute\/configure.html#spark-configuration):\n\n```\nspark.databricks.acl.sqlOnly true\n\n```\n\nNote\n\nAccess to SQL-only table access control is not affected by the [Enable Table Access Control](#enable-table-acl-workspace) setting in the admin settings page. That setting controls only the workspace-wide enablement of Python and SQL table access control.\n\n### Python and SQL table access control\n\nThis version of table access control lets users run Python commands that use the DataFrame API as well as SQL. When\nit is enabled on a cluster, users on that cluster:\n\n* Can access Spark only using the Spark SQL API or DataFrame API. In both cases, access to tables and views is restricted by administrators according to the Databricks [Privileges you can grant on Hive metastore objects](object-privileges.html#privilege-types).\n* Must run their commands on cluster nodes as a low-privilege user forbidden from accessing sensitive parts of the filesystem or creating network connections to ports other than 80 and 443.\n\n\t+ Only built-in Spark functions can create network connections on ports other than 80 and 443.\n\t+ Only workspace admin users or users with [ANY FILE](object-privileges.html#privilege-types) privilege can read data from external databases through the [PySpark JDBC connector](..\/..\/connect\/external-systems\/jdbc.html).\n\t+ If you want Python processes to be able to access additional outbound ports, you can set the [Spark config](..\/..\/compute\/configure.html#spark-configuration) `spark.databricks.pyspark.iptable.outbound.whitelisted.ports` to the ports you want to allow access. The supported format of the configuration value is `[port[:port][,port[:port]]...]`, for example: `21,22,9000:9999`. The port must be within the valid range, that is, `0-65535`.\n\nAttempts to get around these restrictions will fail with an exception. These restrictions are in place so that users can never access unprivileged data through the cluster.\n\n## Enable table access control for your workspace\n\nBefore users can configure Python and SQL table access control, a Databricks workspace must enable table access control for the Databricks workspace and deny users access to clusters that are not enabled for table access control.\n\n1. Go to the [settings page](..\/..\/admin\/index.html#admin-settings).\n2. Click the **Security** tab.\n3. Turn on the **Table Access Control** option.\n\n### Enforce table access control\n\nTo ensure that your users access only the data that you want them to, you must restrict your users to clusters with table access control enabled. In particular, you should ensure that:\n\n* Users do not have permission to create clusters. If they create a cluster without table access control, they can access any data from that cluster.\n* Users do not have CAN ATTACH TO permission for any cluster that is not enabled for table access control.\n\nSee [Compute permissions](..\/..\/compute\/clusters-manage.html#cluster-level-permissions) for more information.\n\n## Create a cluster enabled for table access control\n\nTable access control is enabled by default in clusters with [Shared access mode](..\/..\/compute\/configure.html#access-mode).\n\nTo create the cluster using the REST API, see [Create new cluster](https:\/\/docs.databricks.com\/api\/workspace\/clusters\/create).\n\n## Set privileges on a data object\n\nSee [Hive metastore privileges and securable objects (legacy)](object-privileges.html).\n\n---\n\n \u00a9 Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the [Apache Software Foundation](http:\/\/www.apache.org\/).\n \n\n[Send us feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n \n | [Privacy Policy](https:\/\/databricks.com\/privacy-policy) | [Terms of Use](https:\/\/databricks.com\/terms-of-use)\n\n { # 's' variable stands for the search page here. TODO: Fix it. #}\n \n [![DATA+AU Summit 2024](..\/..\/_static\/images\/dais\/docs-dais-promo-image.png)](https:\/\/www.databricks.com\/dataaisummit?utm_source=databricks&utm_medium=web&utm_campaign=7018y000001f7cvqai&utm_offer=docs \"DATA+AU Summit 2024\")"},{"doc_uri":"https:\/\/docs.databricks.com\/data-governance\/table-acls\/object-privileges.html","content":"Hive metastore privileges and securable objects (legacy) | Databricks on AWS\n\n* [![Databricks](..\/..\/_static\/small-scale-lockup-full-color-rgb.svg)](https:\/\/www.databricks.com\/)\n\n* [![Databricks](..\/..\/_static\/small-scale-lockup-full-color-rgb.svg)](https:\/\/www.databricks.com\/)\n* [Help Center](https:\/\/help.databricks.com\/s\/)\n* [Documentation](https:\/\/docs.databricks.com\/en\/index.html)\n* [Knowledge Base](https:\/\/kb.databricks.com\/)\n\n* [Community](https:\/\/community.databricks.com)\n* [Support](https:\/\/help.databricks.com)\n* [Feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n* [Try Databricks](https:\/\/databricks.com\/try-databricks)\n\n[![](..\/..\/_static\/icons\/globe.png)English](javascript:void(0))\n* [English](..\/..\/..\/en\/data-governance\/table-acls\/object-privileges.html)\n* [\u65e5\u672c\u8a9e](..\/..\/..\/ja\/data-governance\/table-acls\/object-privileges.html)\n* [Portugu\u00eas](..\/..\/..\/pt\/data-governance\/table-acls\/object-privileges.html)\n\n[![](..\/..\/_static\/icons\/aws.svg)Amazon\n Web Services](javascript:void(0))\n* [![](..\/..\/_static\/icons\/azure.svg)Microsoft Azure](https:\/\/learn.microsoft.com\/azure\/databricks\/data-governance\/table-acls\/object-privileges)\n* [![](..\/..\/_static\/icons\/gcp.svg)Google Cloud Platform](https:\/\/docs.gcp.databricks.com\/data-governance\/table-acls\/object-privileges.html)\n\n[Databricks on AWS](..\/..\/index.html)\nGet started\n\n* [Get started](..\/..\/getting-started\/index.html)\n* [What is Databricks?](..\/..\/introduction\/index.html)\n* [DatabricksIQ](..\/..\/databricksiq\/index.html)\n* [Release notes](..\/..\/release-notes\/index.html)\n\nLoad & manage data\n\n* [Connect to data sources](..\/..\/connect\/index.html)\n* [Connect to compute](..\/..\/compute\/index.html)\n* [Discover data](..\/..\/discover\/index.html)\n* [Query data](..\/..\/query\/index.html)\n* [Ingest data](..\/..\/ingestion\/index.html)\n* [Transform data](..\/..\/transform\/index.html)\n* [Monitor data and AI assets](..\/..\/lakehouse-monitoring\/index.html)\n* [Share data (Delta Sharing)](..\/..\/data-sharing\/index.html)\n* [Databricks Marketplace](..\/..\/marketplace\/index.html)\n\nWork with data\n\n* [Data engineering](..\/..\/workspace-index.html)\n* [Generative AI & LLMs](..\/..\/generative-ai\/generative-ai.html)\n* [Machine learning](..\/..\/machine-learning\/index.html)\n* [Model serving](..\/..\/machine-learning\/model-serving\/index.html)\n* [Data warehousing](..\/..\/sql\/index.html)\n* [Delta Lake](..\/..\/delta\/index.html)\n* [Developers](..\/..\/languages\/index.html)\n* [Technology partners](..\/..\/integrations\/index.html)\n\nAdministration\n\n* [Account and workspace administration](..\/..\/admin\/index.html)\n* [Security and compliance](..\/..\/security\/index.html)\n* [Data governance (Unity Catalog)](..\/index.html)\n\t+ [Unity Catalog](..\/unity-catalog\/index.html)\n\t+ [What is Catalog Explorer?](..\/..\/catalog-explorer\/index.html)\n\t+ [Hive metastore table access control (legacy)](index.html)\n\t\t- [Enable Hive metastore table access control on a cluster (legacy)](table-acl.html)\n\t\t- [Hive metastore privileges and securable objects (legacy)](#)\n\t\t- [What is the `ANY FILE` securable?](any-file.html)\n* [Lakehouse architecture](..\/..\/lakehouse-architecture\/index.html)\n\nReference & resources\n\n* [Reference](..\/..\/reference\/api.html)\n* [Resources](..\/..\/resources\/index.html)\n* [What\u2019s coming?](..\/..\/whats-coming.html)\n* [Documentation archive](..\/..\/archive\/index.html)\n\n Updated May 28, 2024\n \n\n[Send us feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n\n* [Documentation](..\/..\/index.html)\n* [Data governance with Unity Catalog](..\/index.html)\n* [Hive metastore table access control (legacy)](index.html)\n* Hive metastore privileges and securable objects (legacy)\n* \n\n# Hive metastore privileges and securable objects (legacy)\n\nThis article describes the privilege model for the legacy Databricks Hive metastore, which is built in to each Databricks workspace. It also describes how to grant, deny, and revoke privileges for objects in the built-in Hive metastore. Unity Catalog uses a different model for granting privileges. See [Unity Catalog privileges and securable objects](..\/unity-catalog\/manage-privileges\/privileges.html).\n\nNote\n\nTable access control for data managed by the Hive metastore is a legacy data governance model. Databricks recommends that you [upgrade the tables managed by the Hive metastore to the Unity Catalog metastore](..\/unity-catalog\/migrate.html). Unity Catalog simplifies security and governance of your data by providing a central place to administer and audit data access across multiple workspaces in your account. To learn more about how the legacy privilege model differs from the Unity Catalog privilege model, see [Work with Unity Catalog and the legacy Hive metastore](..\/unity-catalog\/hive-metastore.html).\n\n## Requirements\n\n* An administrator must [enable and enforce table access control](table-acl.html#enable-table-acl-workspace) for the workspace.\n* The cluster must be enabled for [table access control](table-acl.html#table-access-control).\n\nNote\n\n* Data access control is *always enabled* in Databricks SQL even if table access control is *not enabled* for the workspace.\n* If table access control is enabled for the workspace and you have already specified ACLs (granted and denied privileges) in the workspace, those ACLs are respected in Databricks SQL.\n\n## Manage privileges on objects in the Hive metastore\n\nPrivileges on data objects managed by the Hive metastore can be granted by either a workspace admin or the owner of an object. You can manage privileges for Hive metastore objects by using SQL commands.\n\nTo manage privileges in SQL, you use [GRANT](..\/..\/sql\/language-manual\/security-grant.html), [REVOKE](..\/..\/sql\/language-manual\/security-revoke.html), [DENY](..\/..\/sql\/language-manual\/security-deny.html), [MSCK](..\/..\/sql\/language-manual\/security-msck.html), and [SHOW GRANTS](..\/..\/sql\/language-manual\/security-show-grant.html) statements in a notebook or the Databricks SQL query editor, using the syntax:\n\n```\nGRANT privilege_type ON securable_object TO principal\n\n```\n\nWhere:\n\n* `privilege_type` is a [Hive metastore privilege type](#privilege-types)\n* `securable_object` is a [securable object in the Hive metastore](#securable-objects)\n* `principal` is a user, service principal (represented by its applicationId value), or group. You must enclose users, service principals, and group names with [special characters](..\/..\/sql\/language-manual\/sql-ref-identifiers.html#delimited-identifiers) in backticks ( `` `` ). See [Principal](..\/..\/sql\/language-manual\/sql-ref-principal.html).\n\nTo grant a privilege to all users in your workspace, grant the privilege to the `users` group. For example:\n\n```\nGRANT SELECT ON TABLE <schema-name>.<table-name> TO users\n\n```\n\nFor more information about managing privileges for objects in the Hive metastore using SQL commands, see [Privileges and securable objects in the Hive metastore](..\/..\/sql\/language-manual\/sql-ref-privileges-hms.html).\n\nYou can also manage table access control in a fully automated setup using the [Databricks Terraform](..\/..\/dev-tools\/terraform\/index.html) provider and [databricks\\_sql\\_permissions](https:\/\/registry.terraform.io\/providers\/databricks\/databricks\/latest\/docs\/resources\/sql_permissions#example-usage).\n\n## Object ownership\n\nWhen table access control is enabled on a cluster or SQL warehouse, a user who creates a schema, table, view, or function\nbecomes its owner. The owner is granted all privileges and can grant privileges to other users.\n\nGroups may own objects, in which case all members of that group are considered owners.\n\nEither the owner of an object or a workspace admin can transfer ownership of an object using the following command:\n\n```\nALTER <object> OWNER TO `<user-name>@<user-domain>.com`\n\n```\n\nNote\n\nWhen table access control is disabled on a cluster or SQL warehouse, owners are not registered when a schema, table, or view is created. A workspace admin must assign an owner to the object using the `ALTER <object> OWNER TO` command.\n\n## Securable objects in the Hive metastore\n\nThe securable objects are:\n\n* `CATALOG`: controls access to the entire data catalog.\n\n\t+ `SCHEMA`: controls access to a schema.\n\t\n\t\n\t\t- `TABLE`: controls access to a managed or external table.\n\t\t- `VIEW`: controls access to SQL views.\n\t\t- `FUNCTION`: controls access to a named function.\n* `ANONYMOUS FUNCTION`: controls access to [anonymous or temporary functions](..\/..\/sql\/language-manual\/sql-ref-syntax-ddl-create-function.html).\n\nNote\n\n`ANONYMOUS FUNCTION` objects are not supported in Databricks SQL.\n* `ANY FILE`: controls access to the underlying filesystem.\n\nWarning\n\nUsers granted access to `ANY FILE` can bypass the restrictions put on the catalog, schemas, tables, and views by reading from the filesystem directly.\n\nNote\n\nPrivileges on global and local temporary views are not supported. Local temporary views are visible only within the same session, and views created in the `global_temp` schema are visible to all users sharing a cluster or SQL warehouse. However, privileges on the underlying tables and views referenced by any temporary views are enforced.\n\n## Privileges you can grant on Hive metastore objects\n\n* `SELECT`: gives read access to an object.\n* `CREATE`: gives ability to create an object (for example, a table in a schema).\n* `MODIFY`: gives ability to add, delete, and modify data to or from an object.\n* `USAGE`: does not give any abilities, but is an additional requirement to perform any action on a schema object.\n* `READ_METADATA`: gives ability to view an object and its metadata.\n* `CREATE_NAMED_FUNCTION`: gives ability to create a named UDF in an existing catalog or schema.\n* `MODIFY_CLASSPATH`: gives ability to add files to the Spark class path.\n* `ALL PRIVILEGES`: gives all privileges (is translated into all the above privileges).\n\nNote\n\nThe `MODIFY_CLASSPATH` privilege is not supported in Databricks SQL.\n\n### `USAGE` privilege\n\nTo perform an action on a schema object in the Hive metastore, a user must have the `USAGE` privilege on that schema in addition to the privilege to perform that action. Any one of the following satisfies the `USAGE` requirement:\n\n* Be a workspace admin\n* Have the `USAGE` privilege on the schema or be in a group that has the `USAGE` privilege on the schema\n* Have the `USAGE` privilege on the `CATALOG` or be in a group that has the `USAGE` privilege\n* Be the owner of the schema or be in a group that owns the schema\n\nEven the owner of an object inside a schema must have the `USAGE` privilege in order to use it.\n\n## Privilege hierarchy\n\nWhen table access control is enabled on the workspace and on all clusters, SQL objects in Databricks are hierarchical and privileges are inherited downward. This means that granting or denying a privilege on the `CATALOG` automatically grants or denies the privilege to all schemas in the catalog. Similarly, privileges granted on a schema object are inherited by all objects in that schema. This pattern is true for all securable objects.\n\nIf you deny a user privileges on a table, the user can\u2019t see the table by attempting to list all tables in the schema. If you deny a user privileges on a schema, the user can\u2019t see that the schema exists by attempting to list all schemas in the catalog.\n\n## Dynamic view functions\n\nDatabricks includes two user functions that allow you to express column- and row-level permissions dynamically in the body of a view definition that is managed by the Hive metastore.\n\n* `current_user()`: return the current user name.\n* `is_member()`: determine if the current user is a member of a specific Databricks [group](..\/..\/admin\/users-groups\/groups.html) at the workspace level.\n\nThe following example combines both functions to determine if a user has the appropriate group membership:\n\n```\n-- Return: true if the user is a member and false if they are not\nSELECT\n  current_user as user,\n-- Check to see if the current user is a member of the \"Managers\" group.\n  is_member(\"Managers\") as admin\n\n```\n\n### Column-level permissions\n\nYou can use dynamic views to limit the columns a specific group or user can see. Consider the following example where only users who belong to the `auditors` group are able to see email addresses from the `sales_raw` table. At analysis time Spark replaces the `CASE` statement with either the literal `'REDACTED'` or the column `email`. This behavior allows for all the usual performance optimizations provided by Spark.\n\n```\n-- Alias the field 'email' to itself (as 'email') to prevent the\n-- permission logic from showing up directly in the column name results.\nCREATE VIEW sales_redacted AS\nSELECT\n  user_id,\n  CASE WHEN\n    is_group_member('auditors') THEN email\n    ELSE 'REDACTED'\n  END AS email,\n  country,\n  product,\n  total\nFROM sales_raw\n\n```\n\n### Row-level permissions"},{"doc_uri":"https:\/\/docs.databricks.com\/data-governance\/table-acls\/index.html","content":"Hive metastore table access control (legacy) | Databricks on AWS\n\n* [![Databricks](..\/..\/_static\/small-scale-lockup-full-color-rgb.svg)](https:\/\/www.databricks.com\/)\n\n* [![Databricks](..\/..\/_static\/small-scale-lockup-full-color-rgb.svg)](https:\/\/www.databricks.com\/)\n* [Help Center](https:\/\/help.databricks.com\/s\/)\n* [Documentation](https:\/\/docs.databricks.com\/en\/index.html)\n* [Knowledge Base](https:\/\/kb.databricks.com\/)\n\n* [Community](https:\/\/community.databricks.com)\n* [Support](https:\/\/help.databricks.com)\n* [Feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n* [Try Databricks](https:\/\/databricks.com\/try-databricks)\n\n[![](..\/..\/_static\/icons\/globe.png)English](javascript:void(0))\n* [English](..\/..\/..\/en\/data-governance\/table-acls\/index.html)\n* [\u65e5\u672c\u8a9e](..\/..\/..\/ja\/data-governance\/table-acls\/index.html)\n* [Portugu\u00eas](..\/..\/..\/pt\/data-governance\/table-acls\/index.html)\n\n[![](..\/..\/_static\/icons\/aws.svg)Amazon\n Web Services](javascript:void(0))\n* [![](..\/..\/_static\/icons\/azure.svg)Microsoft Azure](https:\/\/learn.microsoft.com\/azure\/databricks\/data-governance\/table-acls\/)\n* [![](..\/..\/_static\/icons\/gcp.svg)Google Cloud Platform](https:\/\/docs.gcp.databricks.com\/data-governance\/table-acls\/index.html)\n\n[Databricks on AWS](..\/..\/index.html)\nGet started\n\n* [Get started](..\/..\/getting-started\/index.html)\n* [What is Databricks?](..\/..\/introduction\/index.html)\n* [DatabricksIQ](..\/..\/databricksiq\/index.html)\n* [Release notes](..\/..\/release-notes\/index.html)\n\nLoad & manage data\n\n* [Connect to data sources](..\/..\/connect\/index.html)\n* [Connect to compute](..\/..\/compute\/index.html)\n* [Discover data](..\/..\/discover\/index.html)\n* [Query data](..\/..\/query\/index.html)\n* [Ingest data](..\/..\/ingestion\/index.html)\n* [Transform data](..\/..\/transform\/index.html)\n* [Monitor data and AI assets](..\/..\/lakehouse-monitoring\/index.html)\n* [Share data (Delta Sharing)](..\/..\/data-sharing\/index.html)\n* [Databricks Marketplace](..\/..\/marketplace\/index.html)\n\nWork with data\n\n* [Data engineering](..\/..\/workspace-index.html)\n* [Generative AI & LLMs](..\/..\/generative-ai\/generative-ai.html)\n* [Machine learning](..\/..\/machine-learning\/index.html)\n* [Model serving](..\/..\/machine-learning\/model-serving\/index.html)\n* [Data warehousing](..\/..\/sql\/index.html)\n* [Delta Lake](..\/..\/delta\/index.html)\n* [Developers](..\/..\/languages\/index.html)\n* [Technology partners](..\/..\/integrations\/index.html)\n\nAdministration\n\n* [Account and workspace administration](..\/..\/admin\/index.html)\n* [Security and compliance](..\/..\/security\/index.html)\n* [Data governance (Unity Catalog)](..\/index.html)\n\t+ [Unity Catalog](..\/unity-catalog\/index.html)\n\t+ [What is Catalog Explorer?](..\/..\/catalog-explorer\/index.html)\n\t+ [Hive metastore table access control (legacy)](#)\n\t\t- [Enable Hive metastore table access control on a cluster (legacy)](table-acl.html)\n\t\t- [Hive metastore privileges and securable objects (legacy)](object-privileges.html)\n\t\t- [What is the `ANY FILE` securable?](any-file.html)\n* [Lakehouse architecture](..\/..\/lakehouse-architecture\/index.html)\n\nReference & resources\n\n* [Reference](..\/..\/reference\/api.html)\n* [Resources](..\/..\/resources\/index.html)\n* [What\u2019s coming?](..\/..\/whats-coming.html)\n* [Documentation archive](..\/..\/archive\/index.html)\n\n Updated May 28, 2024\n \n\n[Send us feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n\n* [Documentation](..\/..\/index.html)\n* [Data governance with Unity Catalog](..\/index.html)\n* Hive metastore table access control (legacy)\n* \n\n# Hive metastore table access control (legacy)\n\nEach Databricks workspace deploys with a built-in Hive metastore as a managed service. An instance of the metastore deploys to each cluster and securely accesses metadata from a central per-workspace repository.\n\nBy default, a cluster allows all users to access all data managed by the workspace\u2019s built-in Hive metastore unless table access control is enabled for that cluster. Table access control lets you programmatically grant and revoke access to objects in your workspace\u2019s Hive metastore from Python and SQL. When table access control is enabled, users can set permissions for data objects that are accessed using that cluster.\n\nNote\n\nHive metastore table access control is a legacy data governance model. Databricks recommends that you [upgrade the tables managed by the Hive metastore to the Unity Catalog metastore](..\/unity-catalog\/migrate.html). Unity Catalog simplifies security and governance of your data by providing a central place to administer and audit data access across multiple workspaces in your account.\n\n## Requirements\n\n* This feature requires the [Premium plan or above](https:\/\/databricks.com\/product\/pricing\/platform-addons).\n* This feature requires a Data Science & Engineering cluster with an [appropriate configuration](table-acl.html#table-access-control) or a SQL warehouse.\n\nThis section covers:\n\n* [Enable Hive metastore table access control on a cluster (legacy)](table-acl.html)\n* [Hive metastore privileges and securable objects (legacy)](object-privileges.html)\n* [What is the `ANY FILE` securable?](any-file.html)\n\n---\n\n \u00a9 Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the [Apache Software Foundation](http:\/\/www.apache.org\/).\n \n\n[Send us feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n \n | [Privacy Policy](https:\/\/databricks.com\/privacy-policy) | [Terms of Use](https:\/\/databricks.com\/terms-of-use)\n\n { # 's' variable stands for the search page here. TODO: Fix it. #}\n \n [![DATA+AU Summit 2024](..\/..\/_static\/images\/dais\/docs-dais-promo-image.png)](https:\/\/www.databricks.com\/dataaisummit?utm_source=databricks&utm_medium=web&utm_campaign=7018y000001f7cvqai&utm_offer=docs \"DATA+AU Summit 2024\")"},{"doc_uri":"https:\/\/docs.databricks.com\/sql\/language-manual\/sql-ref-privileges-hms.html","content":"Updated May 28, 2024\n \n\n[Send us feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n\n* [Documentation](..\/..\/index.html)\n* [Develop on Databricks](..\/..\/languages\/index.html)\n* [SQL language reference](index.html)\n* Privileges and securable objects in the Hive metastore\n* \n\n# Privileges and securable objects in the Hive metastore\n\n**Applies to:** ![check marked yes](..\/..\/_images\/check.png) Databricks SQL ![check marked yes](..\/..\/_images\/check.png) Databricks Runtime\n\nA privilege is a right granted to a [principal](sql-ref-principal.html) to operate on a [securable object](#securable-objects) in the metastore.\n\nThe privilege model and securable objects differ depending on whether you are using a Unity Catalog metastore or the legacy Hive metastore. This article describes the privilege model for the legacy Hive metastore. If you are using Unity Catalog, see [Privileges and securable objects in Unity Catalog](sql-ref-privileges.html).\n\n## Securable objects in the Hive metastore\n\nA securable object is an object defined in the metastore on which privileges can be granted to a [principal](sql-ref-principal.html).\n\nTo manage privileges on any object you must be its owner or an administrator.\n\n### Syntax\n\n```\n  securable_object\n    { ANY FILE |\n      CATALOG [ catalog_name ] |\n      { SCHEMA | DATABASE } schema_name |\n      FUNCTION function_name |\n      [ TABLE ] table_name |\n      VIEW view_name\n      }\n\n```\n\n### Parameters\n\n* **`ANY FILE`**\n\nControls access to the underlying filesystem.\n* **`CATALOG` [catalog\\_name](sql-ref-names.html#catalog-name)**\n\nControls access to the entire data catalog.\n* **`{ SCHEMA | DATABASE }` [schema\\_name](sql-ref-names.html#schema-name)**\n\nControls access to a schema.\n* **`FUNCTION` [function\\_name](sql-ref-names.html#function-name)**\n\nControls access to a named function.\n* **`[ TABLE ]` [table\\_name](sql-ref-names.html#table-name)**\n\nControls access to a managed or external table.\n* **`VIEW` [view\\_name](sql-ref-names.html#view-name)**\n\nControls access to SQL views.\n\n## Inheritance model\n\nSecurable objects in the Hive metastore are hierarchical and privileges are inherited downward. This means that granting or denying a privilege on the `CATALOG` automatically grants or denies the privilege to all schemas in the catalog. Similarly, privileges granted on a schema object are inherited by all objects in that schema. This pattern is true for all securable objects.\n\nIf you deny a user privileges on a table, the user can\u2019t see the table by attempting to list all tables in the schema. If you deny a user privileges on a schema, the user can\u2019t see that the schema exists by attempting to list all schemas in the catalog.\n\n## Privilege types\n\nThe following table shows which privileges are associated with which securable objects.\n\n| Privilege type | ANONYMOUS FUNCTION | ANY FILE | CATALOG | SCHEMA | FUNCTION | TABLE | VIEW |  |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- |\n| CREATE |  |  | Yes | Yes |  |  |  |  |\n| MODIFY |  | Yes | Yes | Yes |  | Yes |  |  |\n| READ\\_METADATA |  |  | Yes | Yes |  | Yes | Yes |  |\n| SELECT | Yes | Yes | Yes | Yes | Yes | Yes | Yes |  |\n| USAGE |  |  | Yes | Yes |  |  |  |  |\n\n* **`ALL PRIVILEGES`**\n\nUsed to grant or revoke all privileges applicable to the securable and its child objects without explicitly specifying them. This expands to all available privileges at the time permissions checks are made.\n* **`CREATE`**\n\nCreate objects within the catalog or schema.\n* **`MODIFY`**\n\n[COPY INTO](delta-copy-into.html), [UPDATE](delta-update.html) [DELETE](delta-delete-from.html), [INSERT](sql-ref-syntax-dml-insert-into.html), or [MERGE INTO](delta-merge-into.html) the table.\n\nIf the securable\\_object is the `hive_metastore` or a schema within it, granting `MODIFY` will grant `MODIFY` on all current and future tables and views within the securable object.\n* **`READ_METADATA`**\n\nDiscover the securable object in [SHOW](index.html#show-statements) and interrogate the object in [DESCRIBE](index.html#describe-statements)\n\nIf the securable object is the `hive_metastore` catalog or a schema within it, granting `READ_METADATA` will grant `READ_METADATA` on all current and future tables and views within the securable object.\n* **`READ FILES`**\n\nQuery files directly using the [storage credential](sql-ref-storage-credentials.html)\nor [external location](sql-ref-external-locations.html).\n* **`SELECT`**\n\nQuery a table or view, invoke a user defined or anonymous function, or select `ANY FILE`.\nThe user needs `SELECT` on the table, view, or function, as well as `USAGE` on the object\u2019s schema and catalog.\n\nIf the securable object is the `hive_metastore` or a schema within it, granting `SELECT` will grant `SELECT` on all current and future tables and views within the securable object.\n* **`USAGE`**\n\nRequired, but not sufficient to reference any objects in a catalog or schema.\nThe principal also needs to have privileges on the individual securable objects.\n* **`WRITE FILES`**\n\nDirectly [COPY INTO](delta-copy-into.html) files governed by the [storage credential](sql-ref-storage-credentials.html)\nor [external location](sql-ref-external-locations.html).\n\n## Examples\n\n```\n-- Grant a privilege to the user alf@melmak.et\n> GRANT SELECT ON TABLE t TO `alf@melmak.et`;\n\n-- Revoke a privilege from the general public group.\n> REVOKE USAGE ON SCHEMA some_schema FROM `alf@melmak.et`;\n\n```\n\n## Related\n\n* [GRANT](security-grant.html)\n* [Principal](sql-ref-principal.html)\n* [REVOKE](security-revoke.html)\n* [DENY](security-deny.html)\n\n---\n\n \u00a9 Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the [Apache Software Foundation](http:\/\/www.apache.org\/).\n \n\n[Send us feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n \n | [Privacy Policy](https:\/\/databricks.com\/privacy-policy) | [Terms of Use](https:\/\/databricks.com\/terms-of-use)\n\n { # 's' variable stands for the search page here. TODO: Fix it. #}\n \n [![DATA+AU Summit 2024](..\/..\/_static\/images\/dais\/docs-dais-promo-image.png)](https:\/\/www.databricks.com\/dataaisummit?utm_source=databricks&utm_medium=web&utm_campaign=7018y000001f7cvqai&utm_offer=docs \"DATA+AU Summit 2024\")"},{"doc_uri":"https:\/\/docs.databricks.com\/data-governance\/unity-catalog\/manage-privileges\/index.html","content":"1. In your Databricks workspace, click ![Catalog icon](..\/..\/..\/_images\/data-icon.png) **Catalog**.\n2. Select the object, such as a catalog, schema, table, or view.\n3. Go to the **Permissions** tab.\n4. Click **Grant**.\n5. Enter the email address for a user or the name of a group.\n6. Select the permissions to grant.\n7. Click **OK**.\n\nRun the following SQL command in a notebook or SQL query editor.\n\n```\n  GRANT <privilege-type> ON <securable-type> <securable-name> TO <principal>\n\n```\n\n For example, the following command grants a group named *finance-team* access to create tables in a schema named *default* with the parent catalog named *main*:\n\n```\n  GRANT CREATE TABLE ON SCHEMA main.default TO `finance-team`;\n  GRANT USE SCHEMA ON SCHEMA main.default TO `finance-team`;\n  GRANT USE CATALOG ON CATALOG main TO `finance-team`;\n\n```\n\n### Revoke permissions on objects in a Unity Catalog metastore\n\n**Permissions required:** Metastore admin, the owner of the object, or the owner of the catalog or schema that contains the object.\n\n1. In your Databricks workspace, click ![Catalog icon](..\/..\/..\/_images\/data-icon.png) **Catalog**.\n2. Select the object, such as a catalog, schema, table, or view.\n3. Go to the **Permissions** tab.\n4. Select a privilege that has been granted to a user, service principal, or group.\n5. Click **Revoke**.\n6. To confirm, click **Revoke**.\n\nRun the following SQL command in a notebook or SQL query editor.\n\n```\n  REVOKE <privilege-type> ON <securable-type> <securable-name> TO <principal>\n\n```\n\n For example, the following command revokes a group named *finance-team* access to create tables in a schema named *default* with the parent catalog named *main*:\n\n```\n  REVOKE CREATE TABLE ON SCHEMA main.default TO `finance-team`;\n\n```\n\n### Show grants on a metastore\n\n**Permissions required:** Metastore admin or account admin. You can also view your own grants on a metastore.\n\n1. In your Databricks workspace, click ![Catalog icon](..\/..\/..\/_images\/data-icon.png) **Catalog**.\n2. Next to the **Catalog Explorer** page label, click the icon next to the metastore name.\n3. Go to the **Permissions** tab.\n\nRun the following SQL command in a notebook or SQL query editor. You can show grants on a specific principal, or you can show all grants on a metastore.\n\n```\n  SHOW GRANTS [principal] ON METASTORE\n\n```\n\n### Grant permissions on a metastore\n\n**Permissions required:** Metastore admin or account admin.\n\n1. In your Databricks workspace, click ![Catalog icon](..\/..\/..\/_images\/data-icon.png) **Catalog**.\n2. Next to the **Catalog Explorer** page label, click the icon next to the metastore name.\n3. On the **Permissions** tab, click **Grant**.\n4. Enter the email address for a user or the name of a group.\n5. Select the permissions to grant.\n6. Click **OK**.\n\n1. Run the following SQL command in a notebook or SQL query editor.\n\n```\nGRANT <privilege-type> ON METASTORE TO <principal>`;\n\n```\n\nWhen you grant privileges on a metastore, you do not include the metastore name, because the metastore that is attached to your workspace is assumed.\n\n### Revoke permissions on a metastore\n\n**Permissions required:** Metastore admin or account admin..\n\n1. In your Databricks workspace, click ![Catalog icon](..\/..\/..\/_images\/data-icon.png) **Catalog**.\n2. Next to the **Catalog Explorer** page label, click the icon next to the metastore name.\n3. On the **Permissions** tab, select a user or group and click **Revoke**.\n4. To confirm, click **Revoke**.\n\n1. Run the following SQL command in a notebook or SQL query editor.\n\n```\nREVOKE <privilege-type> ON METASTORE TO <principal>;\n\n```\n\nWhen you revoke privileges on a metastore, you do not include the metastore name, because the metastore that is attached to your workspace is assumed.\n\n---\n\n \u00a9 Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the [Apache Software Foundation](http:\/\/www.apache.org\/).\n \n\n[Send us feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n \n | [Privacy Policy](https:\/\/databricks.com\/privacy-policy) | [Terms of Use](https:\/\/databricks.com\/terms-of-use)\n\n { # 's' variable stands for the search page here. TODO: Fix it. #}\n \n [![DATA+AU Summit 2024](..\/..\/..\/_static\/images\/dais\/docs-dais-promo-image.png)](https:\/\/www.databricks.com\/dataaisummit?utm_source=databricks&utm_medium=web&utm_campaign=7018y000001f7cvqai&utm_offer=docs \"DATA+AU Summit 2024\")"}]}
{"request_id":"id-2835","request":"How can I specify the row tag when reading XML files in Databricks?","response":"The passage is about how to read and write XML files using the `spark-xml` library in Databricks. It provides an example of how to retrieve an XML file, upload it to DBFS, and then read and write the XML data using the `spark-xml` library. The example shows how to infer the schema and how to specify column names and types. The passage also mentions the requirements for using the `spark-xml` library, which includes creating the library as a Maven library and installing it on a cluster. The options for reading and writing XML data are also provided, including the `rowTag`, `path`, `rootTag`, and `schema` options.","expected_retrieved_context":[{"doc_uri":"https:\/\/docs.databricks.com\/ingestion\/auto-loader\/options.html","content":"STOP_AT_CLOSING_QUOTE: If unescaped quotes are found in the input, accumulate the quote character and proceed parsing the value as a quoted value, until a closing quote is found.  \nBACK_TO_DELIMITER: If unescaped quotes are found in the input, consider the value as an unquoted value. This will make the parser accumulate all characters of the current parsed value until the delimiter defined by sep is found. If no delimiter is found in the value, the parser will continue accumulating characters from the input until a delimiter or line ending is found.  \nSTOP_AT_DELIMITER: If unescaped quotes are found in the input, consider the value as an unquoted value. This will make the parser accumulate all characters until the delimiter defined by sep, or a line ending is found in the input.  \nSKIP_VALUE: If unescaped quotes are found in the input, the content parsed for the given value will be skipped (until the next delimiter is found) and the value set in nullValue will be produced instead.  \nRAISE_ERROR: If unescaped quotes are found in the input, a TextParsingException will be thrown.  \nDefault value: STOP_AT_DELIMITER  \nXML options  \nOption  \nDescription  \nScope  \nrowTag  \nThe row tag of the XML files to treat as a row. In the example XML <books> <book><book>...<books>, the appropriate value is book. This is a required option.  \nread  \nsamplingRatio  \nDefines a fraction of rows used for schema inference. XML built-in functions ignore this option. Default: 1.0.  \nread  \nexcludeAttribute  \nWhether to exclude attributes in elements. Default: false.  \nread  \nmode  \nMode for dealing with corrupt records during parsing.  \nPERMISSIVE: For corrupted records, puts the malformed string into a field configured by columnNameOfCorruptRecord, and sets malformed fields to null. To keep corrupt records, you can set a string type field named columnNameOfCorruptRecord in a user-defined schema. If a schema does not have the field, corrupt records are dropped during parsing. When inferring a schema, the parser implicitly adds a columnNameOfCorruptRecord field in an output schema.  \nDROPMALFORMED: Ignores corrupted records. This mode is unsupported for XML built-in functions.  \nFAILFAST: Throws an exception when the parser meets corrupted records.  \nread  \ninferSchema  \nIf true, attempts to infer an appropriate type for each resulting DataFrame column. If false, all resulting columns are of string type. Default: true. XML built-in functions ignore this option.  \nread  \ncolumnNameOfCorruptRecord  \nAllows renaming the new field that contains a malformed string created by PERMISSIVE mode. Default: spark.sql.columnNameOfCorruptRecord.  \nread  \nattributePrefix  \nThe prefix for attributes to differentiate attributes from elements. This will be the prefix for field names. Default is _. Can be empty for reading XML, but not for writing.  \nread, write  \nvalueTag  \nThe tag used for the character data within elements that also have attribute(s) or child element(s) elements. User can specify the valueTag field in the schema or it will be added automatically during schema inference when character data is present in elements with other elements or attributes. Default: _VALUE  \nread,write  \nencoding  \nFor reading, decodes the XML files by the given encoding type. For writing, specifies encoding (charset) of saved XML files. XML built-in functions ignore this option. Default: UTF-8.  \nread, write  \nignoreSurroundingSpaces  \nDefines whether surrounding white spaces from values being read should be skipped. Default: true. Whitespace-only character data are ignored.  \nread  \nrowValidationXSDPath  \nPath to an optional XSD file that is used to validate the XML for each row individually. Rows that fail to validate are treated like parse errors as above. The XSD does not otherwise affect the schema provided, or inferred.  \nread  \nignoreNamespace  \nIf true, namespaces\u2019 prefixes on XML elements and attributes are ignored. Tags <abc:author> and <def:author>, for example, are treated as if both are just <author>. Namespaces cannot be ignored on the rowTag element, only its read children. XML parsing is not namespace-aware even if false. Default: false.  \nread  \ntimestampFormat  \nCustom timestamp format string that follows the datetime pattern format. This applies to timestamp type. Default: yyyy-MM-dd'T'HH:mm:ss[.SSS][XXX].  \nread, write  \ntimestampNTZFormat  \nCustom format string for timestamp without timezone that follows the datetime pattern format. This applies to TimestampNTZType type. Default: yyyy-MM-dd'T'HH:mm:ss[.SSS]  \nread, write  \ndateFormat  \nCustom date format string that follows the datetime pattern format. This applies to date type. Default: yyyy-MM-dd.  \nread, write  \nlocale  \nSets a locale as a language tag in IETF BCP 47 format. For instance, locale is used while parsing dates and timestamps. Default: en-US.  \nread  \nrootTag  \nRoot tag of the XML files. For example, in <books> <book><book>...<\/books>, the appropriate value is books. You can include basic attributes by specifying a value like books foo=\"bar\". Default: ROWS.  \nwrite  \ndeclaration  \nContent of XML declaration to write at the start of every output XML file, before the rootTag. For example, a value of foo causes <?xml foo?> to be written. Set to an empty string to suppress. Default: version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\".  \nwrite  \narrayElementName  \nName of XML element that encloses each element of an array-valued column when writing. Default: item.  \nwrite  \nnullValue  \nSets the string representation of a null value. Default: string null. When this is null, the parser does not write attributes and elements for fields.  \nread, write  \ncompression  \nCompression code to use when saving to file. This can be one of the known case-insensitive shortened names (none, bzip2, gzip,lz4, snappy', and deflate`). XML built-in functions ignore this option. Default: none.  \nwrite  \nvalidateName  \nIf true, throws an error on XML element name validation failure. For example, SQL field names can have spaces, but XML element names cannot. Default: true.  \nwrite  \nreaderCaseSensitive"}],"expected_response":"To specify the row tag when reading XML files in Databricks, you can use the `rowTag` option as part of the XML options. Here's an example of how you can use it:\n```python\ndf = spark.read.format(\"xml\") \\\n   .option(\"rowTag\", \"book\") \\\n   .load(\"\/path\/to\/your\/xml\/files\")\n```\nIn this example, the `rowTag` option is set to `\"book\"`, which means that each `<book>` element in the XML files will be treated as a row in the resulting DataFrame. Make sure to replace `\"\/path\/to\/your\/xml\/files\"` with the actual path to your XML files in Databricks.\n\nNote that the `rowTag` option is required when reading XML files in Databricks, and it should be set to the tag that encloses the data you want to read as rows in the DataFrame.","retrieved_context":[{"doc_uri":"https:\/\/docs.databricks.com\/query\/formats\/xml.html","content":"val selectedData = df.select(\"author\", \"_id\")\nselectedData.write\n  .option(\"rootTag\", \"books\")\n  .option(\"rowTag\", \"book\")\n  .xml(\"newbooks.xml\")\n\n```\n\n```\ndf <- loadDF(\"books.xml\", source = \"xml\", rowTag = \"book\")\n# In this case, `rootTag` is set to \"ROWS\" and `rowTag` is set to \"ROW\".\nsaveDF(df, \"newbooks.xml\", \"xml\", \"overwrite\")\n\n```\n\nYou can manually specify the schema when reading data:\n\n```\nfrom pyspark.sql.types import StructType, StructField, StringType, DoubleType\n\ncustom_schema = StructType([\n    StructField(\"_id\", StringType(), True),\n    StructField(\"author\", StringType(), True),\n    StructField(\"description\", StringType(), True),\n    StructField(\"genre\", StringType(), True),\n    StructField(\"price\", DoubleType(), True),\n    StructField(\"publish_date\", StringType(), True),\n    StructField(\"title\", StringType(), True)\n])\ndf = spark.read.options(rowTag='book').xml('books.xml', schema = customSchema)\n\nselected_data = df.select(\"author\", \"_id\")\nselected_data.write.options(rowTag='book', rootTag='books').xml('newbooks.xml')\n\n```\n\n```\nimport org.apache.spark.sql.types.{StructType, StructField, StringType, DoubleType}\n\nval customSchema = StructType(Array(\n  StructField(\"_id\", StringType, nullable = true),\n  StructField(\"author\", StringType, nullable = true),\n  StructField(\"description\", StringType, nullable = true),\n  StructField(\"genre\", StringType, nullable = true),\n  StructField(\"price\", DoubleType, nullable = true),\n  StructField(\"publish_date\", StringType, nullable = true),\n  StructField(\"title\", StringType, nullable = true)))\nval df = spark.read.option(\"rowTag\", \"book\").schema(customSchema).xml(xmlPath) \/\/ books.xml\n\nval selectedData = df.select(\"author\", \"_id\")\nselectedData.write.option(\"rootTag\", \"books\").option(\"rowTag\", \"book\").xml(\"newbooks.xml\")\n\n```\n\n```\ncustomSchema <- structType(\n  structField(\"_id\", \"string\"),\n  structField(\"author\", \"string\"),\n  structField(\"description\", \"string\"),\n  structField(\"genre\", \"string\"),\n  structField(\"price\", \"double\"),\n  structField(\"publish_date\", \"string\"),\n  structField(\"title\", \"string\"))\n\ndf <- loadDF(\"books.xml\", source = \"xml\", schema = customSchema, rowTag = \"book\")\n# In this case, `rootTag` is set to \"ROWS\" and `rowTag` is set to \"ROW\".\nsaveDF(df, \"newbooks.xml\", \"xml\", \"overwrite\")\n\n```\n\n### SQL API\n\nXML data source can infer data types:\n\n```\nDROP TABLE IF EXISTS books;\nCREATE TABLE books\nUSING XML\nOPTIONS (path \"books.xml\", rowTag \"book\");\nSELECT * FROM books;\n\n```\n\nYou can also specify column names and types in DDL. In this case, the schema is not inferred automatically.\n\n```\nDROP TABLE IF EXISTS books;\n\nCREATE TABLE books (author string, description string, genre string, _id string,\nprice double, publish_date string, title string)\nUSING XML\nOPTIONS (path \"books.xml\", rowTag \"book\");\n\n```\n\n### Load XML using COPY INTO\n\n```\nDROP TABLE IF EXISTS books;\nCREATE TABLE IF NOT EXISTS books;\n\nCOPY INTO books\nFROM \"\/FileStore\/xmltestDir\/input\/books.xml\"\nFILEFORMAT = XML\nFORMAT_OPTIONS ('mergeSchema' = 'true', 'rowTag' = 'book')\nCOPY_OPTIONS ('mergeSchema' = 'true');\n\n```\n\n### Read XML with row validation\n\n```\ndf = (spark.read\n    .format(\"xml\")\n    .option(\"rowTag\", \"book\")\n    .option(\"rowValidationXSDPath\", xsdPath)\n    .load(inputPath))\ndf.printSchema()\n\n```\n\n```\nval df = spark.read\n  .option(\"rowTag\", \"book\")\n  .option(\"rowValidationXSDPath\", xsdPath)\n  .xml(inputPath)\ndf.printSchema\n\n```\n\n### Parse nested XML (from\\_xml and schema\\_of\\_xml)\n\n```\nfrom pyspark.sql.functions import from_xml, schema_of_xml, lit, col\n\nxml_data = \"\"\"\n  <book id=\"bk103\">\n    <author>Corets, Eva<\/author>\n    <title>Maeve Ascendant<\/title>\n    <genre>Fantasy<\/genre>\n    <price>5.95<\/price>\n    <publish_date>2000-11-17<\/publish_date>\n  <\/book>\n\"\"\"\n\ndf = spark.createDataFrame([(8, xml_data)], [\"number\", \"payload\"])\nschema = schema_of_xml(df.select(\"payload\").limit(1).collect()[0][0])\nparsed = df.withColumn(\"parsed\", from_xml(col(\"payload\"), schema))\nparsed.printSchema()\nparsed.show()\n\n```\n\n```\nimport org.apache.spark.sql.functions.{from_xml,schema_of_xml,lit}\n\nval xmlData = \"\"\"\n  <book id=\"bk103\">\n    <author>Corets, Eva<\/author>\n    <title>Maeve Ascendant<\/title>\n    <genre>Fantasy<\/genre>\n    <price>5.95<\/price>\n    <publish_date>2000-11-17<\/publish_date>\n  <\/book>\"\"\".stripMargin\n\nval df = Seq((8, xmlData)).toDF(\"number\", \"payload\")\nval schema = schema_of_xml(xmlData)\nval parsed = df.withColumn(\"parsed\", from_xml($\"payload\", schema))\nparsed.printSchema()\nparsed.show()\n\n```\n\n### from\\_xml and schema\\_of\\_xml with SQL API\n\n```\nSELECT from_xml('\n  <book id=\"bk103\">\n    <author>Corets, Eva<\/author>\n    <title>Maeve Ascendant<\/title>\n    <genre>Fantasy<\/genre>\n    <price>5.95<\/price>\n    <publish_date>2000-11-17<\/publish_date>\n  <\/book>',\n  schema_of_xml('\n  <book id=\"bk103\">\n    <author>Corets, Eva<\/author>\n    <title>Maeve Ascendant<\/title>\n    <genre>Fantasy<\/genre>\n    <price>5.95<\/price>\n    <publish_date>2000-11-17<\/publish_date>\n  <\/book>')\n);\n\n```\n\n### Load XML with Auto Loader\n\n```\nquery = (spark\n  .readStream\n    .format(\"cloudFiles\")\n    .option(\"cloudFiles.format\", \"xml\")\n    .option(\"rowTag\", \"book\")\n    .option(\"cloudFiles.inferColumnTypes\", True)\n    .option(\"cloudFiles.schemaLocation\", schemaPath)\n    .option(\"cloudFiles.schemaEvolutionMode\", \"rescue\")\n    .load(inputPath)\n    .writeStream\n    .format(\"delta\")\n    .option(\"mergeSchema\", \"true\")\n    .option(\"checkpointLocation\", checkPointPath)\n    .trigger(Trigger.AvailableNow()))\n\nquery = query.start(outputPath).awaitTermination()\ndf = spark.read.format(\"delta\").load(outputPath)\ndf.show()\n\n```\n\n```\nval query = spark\n.readStream\n  .format(\"cloudFiles\")\n  .option(\"cloudFiles.format\", \"xml\")\n  .option(\"rowTag\", \"book\")\n  .option(\"cloudFiles.inferColumnTypes\", true)\n  .option(\"cloudFiles.schemaLocation\", schemaPath)\n  .option(\"cloudFiles.schemaEvolutionMode\", \"rescue\")\n  .load(inputPath)\n  .writeStream\n  .format(\"delta\")\n  .option(\"mergeSchema\", \"true\")\n  .option(\"checkpointLocation\", checkPointPath)\n  .trigger(Trigger.AvailableNow())\n\nquery.start(outputPath).awaitTermination()\nval df = spark.read.format(\"delta\").load(outputPath)\ndf.show()\n\n```\n\n## Additional resources\n\n[Read and write XML data using the spark-xml library](..\/..\/archive\/connectors\/spark-xml-library.html)\n\n---\n\n \u00a9 Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the [Apache Software Foundation](http:\/\/www.apache.org\/).\n \n\n[Send us feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n \n | [Privacy Policy](https:\/\/databricks.com\/privacy-policy) | [Terms of Use](https:\/\/databricks.com\/terms-of-use)\n\n { # 's' variable stands for the search page here. TODO: Fix it. #}\n \n [![DATA+AU Summit 2024](..\/..\/_static\/images\/dais\/docs-dais-promo-image.png)](https:\/\/www.databricks.com\/dataaisummit?utm_source=databricks&utm_medium=web&utm_campaign=7018y000001f7cvqai&utm_offer=docs \"DATA+AU Summit 2024\")"},{"doc_uri":"https:\/\/docs.databricks.com\/query\/formats\/xml.html","content":"Read and write XML files | Databricks on AWS\n\n* [![Databricks](..\/..\/_static\/small-scale-lockup-full-color-rgb.svg)](https:\/\/www.databricks.com\/)\n\n* [![Databricks](..\/..\/_static\/small-scale-lockup-full-color-rgb.svg)](https:\/\/www.databricks.com\/)\n* [Help Center](https:\/\/help.databricks.com\/s\/)\n* [Documentation](https:\/\/docs.databricks.com\/en\/index.html)\n* [Knowledge Base](https:\/\/kb.databricks.com\/)\n\n* [Community](https:\/\/community.databricks.com)\n* [Support](https:\/\/help.databricks.com)\n* [Feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n* [Try Databricks](https:\/\/databricks.com\/try-databricks)\n\n[![](..\/..\/_static\/icons\/globe.png)English](javascript:void(0))\n* [English](..\/..\/..\/en\/query\/formats\/xml.html)\n* [\u65e5\u672c\u8a9e](..\/..\/..\/ja\/query\/formats\/xml.html)\n* [Portugu\u00eas](..\/..\/..\/pt\/query\/formats\/xml.html)\n\n[![](..\/..\/_static\/icons\/aws.svg)Amazon\n Web Services](javascript:void(0))\n* [![](..\/..\/_static\/icons\/azure.svg)Microsoft Azure](https:\/\/learn.microsoft.com\/azure\/databricks\/query\/formats\/xml)\n* [![](..\/..\/_static\/icons\/gcp.svg)Google Cloud Platform](https:\/\/docs.gcp.databricks.com\/query\/formats\/xml.html)\n\n[Databricks on AWS](..\/..\/index.html)\nGet started\n\n* [Get started](..\/..\/getting-started\/index.html)\n* [What is Databricks?](..\/..\/introduction\/index.html)\n* [DatabricksIQ](..\/..\/databricksiq\/index.html)\n* [Release notes](..\/..\/release-notes\/index.html)\n\nLoad & manage data\n\n* [Connect to data sources](..\/..\/connect\/index.html)\n* [Connect to compute](..\/..\/compute\/index.html)\n* [Discover data](..\/..\/discover\/index.html)\n* [Query data](..\/index.html)\n\t+ [Data format options](index.html)\n\t\t- [Delta Sharing](deltasharing.html)\n\t\t- [Read Parquet files using Databricks](parquet.html)\n\t\t- [ORC file](orc.html)\n\t\t- [JSON file](json.html)\n\t\t- [Read and write to CSV files](csv.html)\n\t\t- [Avro file](avro.html)\n\t\t- [Text files](text.html)\n\t\t- [Image](image.html)\n\t\t- [Binary file](binary.html)\n\t\t- [Hive table](hive-tables.html)\n\t\t- [XML file](#)\n\t\t- [MLflow experiment](mlflow-experiment.html)\n\t\t- [LZO compressed file](lzo.html)\n\t+ [Query streaming data](..\/streaming.html)\n* [Ingest data](..\/..\/ingestion\/index.html)\n* [Transform data](..\/..\/transform\/index.html)\n* [Monitor data and AI assets](..\/..\/lakehouse-monitoring\/index.html)\n* [Share data (Delta Sharing)](..\/..\/data-sharing\/index.html)\n* [Databricks Marketplace](..\/..\/marketplace\/index.html)\n\nWork with data\n\n* [Data engineering](..\/..\/workspace-index.html)\n* [Generative AI & LLMs](..\/..\/generative-ai\/generative-ai.html)\n* [Machine learning](..\/..\/machine-learning\/index.html)\n* [Model serving](..\/..\/machine-learning\/model-serving\/index.html)\n* [Data warehousing](..\/..\/sql\/index.html)\n* [Delta Lake](..\/..\/delta\/index.html)\n* [Developers](..\/..\/languages\/index.html)\n* [Technology partners](..\/..\/integrations\/index.html)\n\nAdministration\n\n* [Account and workspace administration](..\/..\/admin\/index.html)\n* [Security and compliance](..\/..\/security\/index.html)\n* [Data governance (Unity Catalog)](..\/..\/data-governance\/index.html)\n* [Lakehouse architecture](..\/..\/lakehouse-architecture\/index.html)\n\nReference & resources\n\n* [Reference](..\/..\/reference\/api.html)\n* [Resources](..\/..\/resources\/index.html)\n* [What\u2019s coming?](..\/..\/whats-coming.html)\n* [Documentation archive](..\/..\/archive\/index.html)\n\n Updated May 28, 2024\n \n\n[Send us feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n\n* [Documentation](..\/..\/index.html)\n* [Query data](..\/index.html)\n* [Data format options](index.html)\n* Read and write XML files\n* \n\n# Read and write XML files\n\nPreview\n\nThis feature is in [Public Preview](..\/..\/release-notes\/release-types.html).\n\nThis article describes how to read and write XML files.\n\nExtensible Markup Language (XML) is a markup language for formatting, storing, and sharing data in textual format. It defines a set of rules for serializing data ranging from documents to arbitrary data structures.\n\nNative XML file format support enables ingestion, querying, and parsing of XML data for batch processing or streaming. It can automatically infer and evolve schema and data types, supports SQL expressions like `from_xml`, and can generate XML documents. It doesn\u2019t require external jars and works seamlessly with Auto Loader, `read_files` and `COPY INTO`.\n\n## Requirements\n\nDatabricks Runtime 14.3 and above\n\n## Parse XML records\n\nXML specification mandates a well-formed structure. However, this specification doesn\u2019t immediately map to a tabular format. You must specify the `rowTag` option to indicate the XML element that maps to a `DataFrame` `Row`. The `rowTag` element becomes the top-level `struct`. The child elements of `rowTag` become the fields of the top-level `struct`.\n\nYou can specify the schema for this record or let it be inferred automatically. Because the parser only examines the `rowTag` elements, DTD and external entities are filtered out.\n\nThe following examples illustrate schema inference and parsing of an XML file using different `rowTag` options:\n\n```\nxmlString = \"\"\"\n  <books>\n    <book id=\"bk103\">\n      <author>Corets, Eva<\/author>\n      <title>Maeve Ascendant<\/title>\n    <\/book>\n    <book id=\"bk104\">\n      <author>Corets, Eva<\/author>\n      <title>Oberon's Legacy<\/title>\n    <\/book>\n  <\/books>\"\"\"\n\nxmlPath = \"dbfs:\/tmp\/books.xml\"\ndbutils.fs.put(xmlPath, xmlString, True)\n\n```\n\n```\nval xmlString = \"\"\"\n  <books>\n    <book id=\"bk103\">\n      <author>Corets, Eva<\/author>\n      <title>Maeve Ascendant<\/title>\n    <\/book>\n    <book id=\"bk104\">\n      <author>Corets, Eva<\/author>\n      <title>Oberon's Legacy<\/title>\n    <\/book>\n  <\/books>\"\"\"\nval xmlPath = \"dbfs:\/tmp\/books.xml\"\ndbutils.fs.put(xmlPath, xmlString)\n\n```\n\nRead the XML file with `rowTag` option as \u201cbooks\u201d:\n\n```\ndf = spark.read.option(\"rowTag\", \"books\").format(\"xml\").load(xmlPath)\ndf.printSchema()\ndf.show(truncate=False)\n\n```\n\n```\nval df = spark.read.option(\"rowTag\", \"books\").xml(xmlPath)\ndf.printSchema()\ndf.show(truncate=false)\n\n```\n\nOutput:\n\n```\nroot\n|-- book: array (nullable = true)\n| |-- element: struct (containsNull = true)\n| | |-- _id: string (nullable = true)\n| | |-- author: string (nullable = true)\n| | |-- title: string (nullable = true)\n\n+------------------------------------------------------------------------------+\n|book                                                                          |\n+------------------------------------------------------------------------------+\n|[{bk103, Corets, Eva, Maeve Ascendant}, {bk104, Corets, Eva, Oberon's Legacy}]|\n+------------------------------------------------------------------------------+\n\n```\n\nRead the XML file with `rowTag` as \u201cbook\u201d:\n\n```\ndf = spark.read.option(\"rowTag\", \"book\").format(\"xml\").load(xmlPath)\n# Infers three top-level fields and parses `book` in separate rows:\n\n```\n\n```\nval df = spark.read.option(\"rowTag\", \"book\").xml(xmlPath)\n\/\/ Infers three top-level fields and parses `book` in separate rows:\n\n```\n\nOutput:\n\n```\nroot\n|-- _id: string (nullable = true)\n|-- author: string (nullable = true)\n|-- title: string (nullable = true)\n\n+-----+-----------+---------------+\n|_id  |author     |title          |\n+-----+-----------+---------------+\n|bk103|Corets, Eva|Maeve Ascendant|\n|bk104|Corets, Eva|Oberon's Legacy|\n+-----+-----------+---------------+\n\n```\n\n## Data source options\n\nData source options for XML can be specified the following ways:\n\n* The `.option\/.options` methods of the following:\n\n\t+ DataFrameReader\n\t+ DataFrameWriter\n\t+ DataStreamReader\n\t+ DataStreamWriter\n* The following built-in functions:\n\n\t+ [from\\_xml](..\/..\/sql\/language-manual\/functions\/from_xml.html)\n\t+ [to\\_xml](..\/..\/sql\/language-manual\/functions\/to_xml.html)\n\t+ [schema\\_of\\_xml](..\/..\/sql\/language-manual\/functions\/schema_of_xml.html)\n* The `OPTIONS` clause of [CREATE TABLE USING DATA\\_SOURCE](..\/..\/sql\/language-manual\/sql-ref-syntax-ddl-create-table-using.html)\n\nFor a list of options, see [Auto Loader options](..\/..\/ingestion\/auto-loader\/options.html).\n\n## XSD support\n\nYou can optionally validate each row-level XML record by an XML Schema Definition (XSD). The XSD file is specified in the `rowValidationXSDPath` option. The XSD does not otherwise affect the schema provided or inferred. A record that fails the validation is marked as \u201ccorrupted\u201d and handled based on the corrupt record handling mode option described in the option section.\n\nYou can use `XSDToSchema` to extract a Spark DataFrame schema from a XSD file. It supports only simple, complex, and sequence types, and only supports basic XSD functionality.\n\n```\nimport org.apache.spark.sql.execution.datasources.xml.XSDToSchema\nimport org.apache.hadoop.fs.Path\n\nval xsdPath = \"dbfs:\/tmp\/books.xsd\"\nval xsdString = \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\" ?>\n  <xs:schema xmlns:xs=\"http:\/\/www.w3.org\/2001\/XMLSchema\">\n    <xs:element name=\"book\">\n      <xs:complexType>\n        <xs:sequence>\n          <xs:element name=\"author\" type=\"xs:string\" \/>\n          <xs:element name=\"title\" type=\"xs:string\" \/>\n          <xs:element name=\"genre\" type=\"xs:string\" \/>\n          <xs:element name=\"price\" type=\"xs:decimal\" \/>\n          <xs:element name=\"publish_date\" type=\"xs:date\" \/>\n          <xs:element name=\"description\" type=\"xs:string\" \/>\n        <\/xs:sequence>\n        <xs:attribute name=\"id\" type=\"xs:string\" use=\"required\" \/>\n      <\/xs:complexType>\n    <\/xs:element>\n  <\/xs:schema>\"\"\"\n\ndbutils.fs.put(xsdPath, xsdString, true)\n\nval schema1 = XSDToSchema.read(xsdString)\nval schema2 = XSDToSchema.read(new Path(xsdPath))\n\n```\n\nThe following table shows the conversion of XSD data types to Spark data types:"},{"doc_uri":"https:\/\/docs.databricks.com\/archive\/connectors\/spark-xml-library.html","content":"val df = spark.read\n  .option(\"rowTag\", \"book\")\n  .schema(customSchema)\n  .xml(\"books.xml\")\n\nval selectedData = df.select(\"author\", \"_id\")\nselectedData.write\n  .option(\"rootTag\", \"books\")\n  .option(\"rowTag\", \"book\")\n  .xml(\"dbfs:\/newbooks.xml\")\n\n```\n\n```\n# Infer schema\n\nlibrary(SparkR)\n\nsparkR.session(\"local[4]\", sparkPackages = c(\"com.databricks:spark-xml_2.12:<release>\"))\n\ndf <- read.df(\"dbfs:\/books.xml\", source = \"xml\", rowTag = \"book\")\n\n# Default `rootTag` and `rowTag`\nwrite.df(df, \"dbfs:\/newbooks.xml\", \"xml\")\n\n# Specify schema\n\ncustomSchema <- structType(\n  structField(\"_id\", \"string\"),\n  structField(\"author\", \"string\"),\n  structField(\"description\", \"string\"),\n  structField(\"genre\", \"string\"),\n  structField(\"price\", \"double\"),\n  structField(\"publish_date\", \"string\"),\n  structField(\"title\", \"string\"))\n\ndf <- read.df(\"dbfs:\/books.xml\", source = \"xml\", schema = customSchema, rowTag = \"book\")\n\n# In this case, `rootTag` is set to \"ROWS\" and `rowTag` is set to \"ROW\".\nwrite.df(df, \"dbfs:\/newbooks.xml\", \"xml\", \"overwrite\")\n\n```\n\n## Options\n\n* Read\n\n\t+ `path`: Location of XML files. Accepts standard Hadoop globbing expressions.\n\t+ `rowTag`: The row tag to treat as a row. For example, in this XML `<books><book><book>...<\/books>`, the value would be `book`. Default is `ROW`.\n\t+ `samplingRatio`: Sampling ratio for inferring schema (0.0 ~ 1). Default is 1. Possible types are `StructType`, `ArrayType`, `StringType`, `LongType`, `DoubleType`, `BooleanType`, `TimestampType` and `NullType`, unless you provide a schema.\n\t+ `excludeAttribute`: Whether to exclude attributes in elements. Default is false.\n\t+ `nullValue`: The value to treat as a `null` value. Default is `\"\"`.\n\t+ `mode`: The mode for dealing with corrupt records. Default is `PERMISSIVE`.\n\t\n\t\n\t\t- `PERMISSIVE`:\n\t\t\n\t\t\n\t\t\t* When it encounters a corrupted record, sets all fields to `null` and puts the malformed string into a new field configured by `columnNameOfCorruptRecord`.\n\t\t\t* When it encounters a field of the wrong data type, sets the offending field to `null`.\n\t\t- `DROPMALFORMED`: ignores corrupted records.\n\t\t- `FAILFAST`: throws an exception when it detects corrupted records.\n\t+ `inferSchema`: if `true`, attempts to infer an appropriate type for each resulting DataFrame column, like a boolean, numeric or date type. If `false`, all resulting columns are of string type. Default is `true`.\n\t+ `columnNameOfCorruptRecord`: The name of new field where malformed strings are stored. Default is `_corrupt_record`.\n\t+ `attributePrefix`: The prefix for attributes so that to differentiate attributes and elements. This is the prefix for field names. Default is `_`.\n\t+ `valueTag`: The tag used for the value when there are attributes in an element that has no child elements. Default is `_VALUE`.\n\t+ `charset`: Defaults to `UTF-8` but can be set to other valid charset names.\n\t+ `ignoreSurroundingSpaces`: Whether or not whitespaces surrounding values should be skipped. Default is false.\n\t+ `rowValidationXSDPath`: Path to an XSD file that is used to validate the XML for each row. Rows that fail to validate are treated like parse errors as above. The XSD does not otherwise affect the schema provided or inferred. If the same local path is not already also visible on the executors in the cluster, then the XSD and any others it depends on should be added to the Spark executors with [SparkContext.addFile](https:\/\/api-docs.databricks.com\/scala\/spark\/latest\/org\/apache\/spark\/SparkContext.html#addFile(path:String):Unit). In this case, to use local XSD `\/foo\/bar.xsd`, call `addFile(\"\/foo\/bar.xsd\")` and pass `\"bar.xsd\"` as `rowValidationXSDPath`.\n* Write\n\n\t+ `path`: Location to write files.\n\t+ `rowTag`: The row tag to treat as a row. For example, in this XML `<books><book><book>...<\/books>`, the value would be `book`. Default is `ROW`.\n\t+ `rootTag`: The root tag to treat as the root. For example, in this XML `<books><book><book>...<\/books>`, the value would be `books`. Default is `ROWS`.\n\t+ `nullValue`: The value to write `null` value. Default is the string `\"null\"`. When `\"null\"`, it does not write attributes and elements for fields.\n\t+ `attributePrefix`: The prefix for attributes to differentiate attributes and elements. This is the prefix for field names. Default is `_`.\n\t+ `valueTag`: The tag used for the value when there are attributes in an element that has no child elements. Default is `_VALUE`.\n\t+ `compression`: Compression codec to use when saving to file. Should be the fully qualified name of a class implementing `org.apache.hadoop.io.compress.CompressionCodec` or one of case-insensitive short names (`bzip2`, `gzip`, `lz4`, and `snappy`). Default is no compression.\n\nSupports the shortened name usage; You can use `xml` instead of `com.databricks.spark.xml`.\n\n## XSD support\n\nYou can validate individual rows against an XSD schema using `rowValidationXSDPath`.\n\nYou use the utility `com.databricks.spark.xml.util.XSDToSchema` to extract a Spark DataFrame\nschema from *some* XSD files. It supports only simple, complex and sequence types, only basic XSD functionality,\nand is experimental.\n\n```\nimport com.databricks.spark.xml.util.XSDToSchema\nimport java.nio.file.Paths\n\nval schema = XSDToSchema.read(Paths.get(\"\/path\/to\/your.xsd\"))\nval df = spark.read.schema(schema)....xml(...)\n\n```\n\n## Parse nested XML\n\nAlthough primarily used to convert an XML file into a DataFrame, you can also use the `from_xml` method to parse XML in a string-valued column in an existing DataFrame and add it as a new column with parsed results as a struct with:\n\n```\nimport com.databricks.spark.xml.functions.from_xml\nimport com.databricks.spark.xml.schema_of_xml\nimport spark.implicits._\n\nval df = ... \/\/\/ DataFrame with XML in column 'payload'\nval payloadSchema = schema_of_xml(df.select(\"payload\").as[String])\nval parsed = df.withColumn(\"parsed\", from_xml($\"payload\", payloadSchema))\n\n```\n\nNote\n\n* `mode`:\n\n\t+ If set to `PERMISSIVE`, the default, the parse mode instead defaults to `DROPMALFORMED`. If you include a column in the schema for `from_xml` that matches the `columnNameOfCorruptRecord`, then `PERMISSIVE` mode outputs malformed records to that column in the resulting struct.\n\t+ If set to `DROPMALFORMED`, XML values that do not parse correctly result in a `null` value for the column. No rows are dropped.\n* `from_xml` converts arrays of strings containing XML to arrays of parsed structs. Use `schema_of_xml_array` instead.\n* `from_xml_string` is an alternative for use in UDFs that operates on a String directly instead of a column.\n\n## Conversion rules\n\nDue to structural differences between DataFrames and XML, there are some conversion rules from XML data to DataFrame and from DataFrame to XML data. You can disable handling attributes with the option `excludeAttribute`.\n\n### Convert XML to DataFrame\n\n* Attributes: Attributes are converted as fields with the prefix specified in the `attributePrefix` option. If `attributePrefix` is `_`, the document\n\n```\n<one myOneAttrib=\"AAAA\">\n    <two>two<\/two>\n    <three>three<\/three>\n<\/one>\n\n```\n\nproduces the schema:\n\n```\nroot\n |-- _myOneAttrib: string (nullable = true)\n |-- two: string (nullable = true)\n |-- three: string (nullable = true)\n\n```\n* If an element has attributes but no child elements, the attribute value is put in a separate field specified in the `valueTag` option. If `valueTag` is `_VALUE`, the document\n\n```\n<one>\n    <two myTwoAttrib=\"BBBBB\">two<\/two>\n    <three>three<\/three>\n<\/one>\n\n```\n\nproduces the schema:\n\n```\nroot\n |-- two: struct (nullable = true)\n |    |-- _VALUE: string (nullable = true)\n |    |-- _myTwoAttrib: string (nullable = true)\n |-- three: string (nullable = true)\n\n```\n\n### Convert DataFrame to XML\n\nWriting an XML file from DataFrame having a field `ArrayType` with its element as `ArrayType` would have an additional nested field for the element. This would not happen in reading and writing XML data but in writing a DataFrame read from other sources. Therefore, roundtrip in reading and writing XML files has the same structure but writing a DataFrame read from other sources is possible to have a different structure.\n\nA DataFrame with the schema:\n\n```\n |-- a: array (nullable = true)\n |    |-- element: array (containsNull = true)\n |    |    |-- element: string (containsNull = true)\n\n```\n\nand data:\n\n```\n+------------------------------------+\n|                                   a|\n+------------------------------------+\n|[WrappedArray(aa), WrappedArray(bb)]|\n+------------------------------------+\n\n```\n\nproduces the XML file:\n\n```\n<a>\n  <item>aa<\/item>\n<\/a>\n<a>\n  <item>bb<\/item>\n<\/a>\n\n```\n\n---\n\n \u00a9 Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the [Apache Software Foundation](http:\/\/www.apache.org\/).\n \n\n[Send us feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n \n | [Privacy Policy](https:\/\/databricks.com\/privacy-policy) | [Terms of Use](https:\/\/databricks.com\/terms-of-use)\n\n { # 's' variable stands for the search page here. TODO: Fix it. #}\n \n [![DATA+AU Summit 2024](..\/..\/_static\/images\/dais\/docs-dais-promo-image.png)](https:\/\/www.databricks.com\/dataaisummit?utm_source=databricks&utm_medium=web&utm_campaign=7018y000001f7cvqai&utm_offer=docs \"DATA+AU Summit 2024\")"},{"doc_uri":"https:\/\/docs.databricks.com\/sql\/language-manual\/functions\/read_files.html","content":"Updated May 28, 2024\n \n\n[Send us feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n\n* [Documentation](..\/..\/..\/index.html)\n* [Develop on Databricks](..\/..\/..\/languages\/index.html)\n* [SQL language reference](..\/index.html)\n* [Functions](..\/sql-ref-functions.html)\n* [Built-in functions](..\/sql-ref-functions-builtin.html)\n* [Alphabetical list of built-in functions](..\/sql-ref-functions-builtin-alpha.html)\n* `read_files` table-valued function\n* \n\n# `read_files` table-valued function\n\n**Applies to:** ![check marked yes](..\/..\/..\/_images\/check.png) Databricks SQL ![check marked yes](..\/..\/..\/_images\/check.png) Databricks Runtime 13.3 LTS and above\n\nReads files under a provided location and returns the data in tabular form.\n\nSupports reading `JSON`, `CSV`, `XML`, `TEXT`, `BINARYFILE`, `PARQUET`, `AVRO`, and `ORC` file formats.\nCan detect the file format automatically and infer a unified schema across all files.\n\n## Syntax\n\n```\nread_files(path [, option_key => option_value ] [...])\n\n```\n\n## Arguments\n\nThis function requires [named parameter invocation](..\/sql-ref-function-invocation.html#named-parameter-invocation) for the option keys.\n\n* `path`: A `STRING` with the URI of the location of the data. Supports reading from Azure Data Lake Storage Gen2 (`'abfss:\/\/'`), S3 (`s3:\/\/`) and Google Cloud Storage (`'gs:\/\/'`). Can contain globs. See [File discovery](#file-discovery) for more details.\n* `option_key`: The name of the option to configure. You need to use backticks (`) for options that contain dots (`.`).\n* `option_value`: A constant expression to set the option to. Accepts literals and scalar functions.\n\n## Returns\n\nA table comprised of the data from files read under the given `path`.\n\n## File discovery\n\n`read_files` can read an individual file or read files under a provided directory. `read_files` discovers all files under the provided directory recursively unless a [glob](#globbing) is provided, which instructs `read_files` to recurse into a specific directory pattern.\n\n### Filtering directories or files using glob patterns\n\nGlob patterns can be used for filtering directories and files when provided in the path.\n\n| Pattern | Description |\n| --- | --- |\n| `?` | Matches any single character |\n| `*` | Matches zero or more characters |\n| `[abc]` | Matches a single character from character set {a,b,c}. |\n| `[a-z]` | Matches a single character from the character range {a\u2026z}. |\n| `[^a]` | Matches a single character that is not from character set or range {a}. Note that the `^` character must occur immediately to the right of the opening bracket. |\n| `{ab,cd}` | Matches a string from the string set {ab, cd}. |\n| `{ab,c{de, fh}}` | Matches a string from the string set {ab, cde, cfh}. |\n\n`read_files` uses Auto Loader\u2019s strict globber when discovering files with globs. This is configured by the `useStrictGlobber` option. When the strict globber is disabled, trailing slashes (`\/`) are dropped and a star pattern such as `\/*\/` can expand into discovering multiple directories. See the examples below to see the difference in behavior.\n\n| Pattern | File path | Strict globber disabled | Strict globber enabled |\n| --- | --- | --- | --- |\n| `\/a\/b` | `\/a\/b\/c\/file.txt` | *Yes* | *Yes* |\n| `\/a\/b` | `\/a\/b_dir\/c\/file.txt` | *No* | *No* |\n| `\/a\/b` | `\/a\/b.txt` | *No* | *No* |\n| `\/a\/b\/` | `\/a\/b.txt` | *No* | *No* |\n| `\/a\/*\/c\/` | `\/a\/b\/c\/file.txt` | *Yes* | *Yes* |\n| `\/a\/*\/c\/` | `\/a\/b\/c\/d\/file.txt` | *Yes* | *Yes* |\n| `\/a\/*\/d\/` | `\/a\/b\/c\/d\/file.txt` | *Yes* | *No* |\n| `\/a\/*\/c\/` | `\/a\/b\/x\/y\/c\/file.txt` | *Yes* | *No* |\n| `\/a\/*\/c` | `\/a\/b\/c_file.txt` | *Yes* | *No* |\n| `\/a\/*\/c\/` | `\/a\/b\/c_file.txt` | *Yes* | *No* |\n| `\/a\/*\/c` | `\/a\/b\/cookie\/file.txt` | *Yes* | *No* |\n| `\/a\/b*` | `\/a\/b.txt` | *Yes* | *Yes* |\n| `\/a\/b*` | `\/a\/b\/file.txt` | *Yes* | *Yes* |\n| `\/a\/{0.txt,1.txt}` | `\/a\/0.txt` | *Yes* | *Yes* |\n| `\/a\/*\/{0.txt,1.txt}` | `\/a\/0.txt` | *No* | *No* |\n| `\/a\/b\/[cde-h]\/i\/` | `\/a\/b\/c\/i\/file.txt` | *Yes* | *Yes* |\n\n## Schema inference\n\nThe schema of the files can be explicitly provided to `read_files` with the `schema` option. When the schema is not provided, `read_files` attempts to infer a unified schema across the discovered files, which requires reading all the files unless a `LIMIT` statement is used. Even when using a `LIMIT` query, a larger set of files than required might be read to return a more representative schema of the data. Databricks automatically adds a `LIMIT` statement for `SELECT` queries in notebooks and the SQL editor if a user hasn\u2019t provided one.\n\nThe `schemaHints` option can be used to fix subsets of the inferred schema. See [Override schema inference with schema hints](..\/..\/..\/ingestion\/auto-loader\/schema.html#schema-hints) for more details.\n\nA `rescuedDataColumn` is provided by default to rescue any data that doesn\u2019t match the schema. See [What is the rescued data column?](..\/..\/..\/ingestion\/auto-loader\/schema.html#rescue) for more details. You can drop the `rescuedDataColumn` by setting the option `schemaEvolutionMode => 'none'`.\n\n### Partition schema inference\n\n`read_files` can also infer *partitioning columns* if files are stored under *Hive-style partitioned* directories, that is `\/column_name=column_value\/`. If a `schema` is provided, the discovered partition columns use the types provided in the `schema`. If the partition columns are not part of the provided `schema`, then the inferred partition columns are ignored.\n\nIf a column exists in both the partition schema and in the data columns, the value that is read from the partition value is used instead of the data value. If you would like to ignore the values coming from the directory and use the data column, you can provide the list of partition columns in a comma-separated list with the `partitionColumns` option.\n\nThe `partitionColumns` option can also be used to instruct `read_files` on which discovered columns to include in the final inferred schema. Providing an empty string ignores all partition columns.\n\nThe `schemaHints` option can also be provided to override the inferred schema for a partition column.\n\nThe `TEXT` and `BINARYFILE` formats have a fixed schema, but `read_files` also attempts to infer partitioning for these formats when possible.\n\n## Usage in streaming tables\n\n`read_files` can be used in streaming tables to ingest files into Delta Lake. `read_files` leverages Auto Loader when used in a streaming table query. You must use the `STREAM` keyword with `read_files`. See [What is Auto Loader?](..\/..\/..\/ingestion\/auto-loader\/index.html) for more details.\n\nWhen used in a streaming query, `read_files` uses a sample of the data to infer the schema, and can evolve the schema as it processes more data. See [Configure schema inference and evolution in Auto Loader](..\/..\/..\/ingestion\/auto-loader\/schema.html) for more details.\n\n## Options\n\n* [Basic Options](#basic-options)\n* [Generic options](#generic-options)\n* [`JSON` options](#json-options)\n* [`CSV` options](#csv-options)\n* [`XML` options](#xml-options)\n* [`PARQUET` options](#parquet-options)\n* [`AVRO` options](#avro-options)\n* [`BINARYFILE` options](#binaryfile-options)\n* [`TEXT` options](#text-options)\n* [`ORC` options](#orc-options)\n* [Streaming options](#streaming-options)\n\n### [Basic Options](#id1)\n\n| Option |\n| --- |\n| **`format`** Type: `String` The [data file format](..\/..\/..\/query\/formats\/index.html) in the source path. Auto-inferred if not provided. Allowed values include:* `avro`: [Avro file](..\/..\/..\/query\/formats\/avro.html) * `binaryFile`: [Binary file](..\/..\/..\/query\/formats\/binary.html) * `csv`: [Read and write to CSV files](..\/..\/..\/query\/formats\/csv.html) * `json`: [JSON file](..\/..\/..\/query\/formats\/json.html) * `orc`: [ORC file](..\/..\/..\/query\/formats\/orc.html) * `parquet`: [Read Parquet files using Databricks](..\/..\/..\/query\/formats\/parquet.html) * `text`: [Text files](..\/..\/..\/query\/formats\/text.html) * `xml`: [Read and write XML files](..\/..\/..\/query\/formats\/xml.html)  Default value: None |\n| **`inferColumnTypes`** Type: `Boolean` Whether to infer exact column types when leveraging schema inference. By default, columns are inferred when inferring JSON and CSV datasets. See [schema inference](..\/..\/..\/ingestion\/auto-loader\/schema.html) for more details. Note that this is the opposite of the default of Auto Loader. Default value: `true` |\n| **`partitionColumns`** Type: `String` A comma-separated list of Hive style partition columns that you would like inferred from the directory structure of the files. Hive style partition columns are key-value pairs combined by an equality sign such as `<base-path>\/a=x\/b=1\/c=y\/file.format`. In this example, the partition columns are `a`, `b`, and `c`. By default these columns will be automatically added to your schema if you are using schema inference and provide the `<base-path>` to load data from. If you provide a schema, Auto Loader expects these columns to be included in the schema. If you do not want these columns as part of your schema, you can specify `\"\"` to ignore these columns. In addition, you can use this option when you want columns to be inferred the file path in complex directory structures, like the example below: `<base-path>\/year=2022\/week=1\/file1.csv` `<base-path>\/year=2022\/month=2\/day=3\/file2.csv` `<base-path>\/year=2022\/month=2\/day=4\/file3.csv` Specifying `cloudFiles.partitionColumns` as `year,month,day` will return `year=2022` for `file1.csv`, but the `month` and `day` columns will be `null`. `month` and `day` will be parsed correctly for `file2.csv` and `file3.csv`. Default value: None |\n| **`schemaHints`** Type: `String` Schema information that you provide to Auto Loader during schema inference. See [schema hints](..\/..\/..\/ingestion\/auto-loader\/schema.html) for more details. Default value: None |\n| **`useStrictGlobber`** Type: `Boolean` Whether to use a strict globber that matches the default globbing behavior of other file sources in Apache Spark. See [Common data loading patterns](..\/..\/..\/ingestion\/auto-loader\/patterns.html) for more details. Available in Databricks Runtime 12.2 LTS and above. Note that this is the opposite of the default for Auto Loader. Default value: `true` |\n\n### [Generic options](#id2)\n\nThe following options apply to all file formats."},{"doc_uri":"https:\/\/docs.databricks.com\/archive\/connectors\/spark-xml-library.html","content":"Read and write XML data using the spark-xml library | Databricks on AWS\n\n* [![Databricks](..\/..\/_static\/small-scale-lockup-full-color-rgb.svg)](https:\/\/www.databricks.com\/)\n\n* [![Databricks](..\/..\/_static\/small-scale-lockup-full-color-rgb.svg)](https:\/\/www.databricks.com\/)\n* [Help Center](https:\/\/help.databricks.com\/s\/)\n* [Documentation](https:\/\/docs.databricks.com\/en\/index.html)\n* [Knowledge Base](https:\/\/kb.databricks.com\/)\n\n* [Community](https:\/\/community.databricks.com)\n* [Support](https:\/\/help.databricks.com)\n* [Feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n* [Try Databricks](https:\/\/databricks.com\/try-databricks)\n\n[![](..\/..\/_static\/icons\/globe.png)English](javascript:void(0))\n* [English](..\/..\/..\/en\/archive\/connectors\/spark-xml-library.html)\n* [\u65e5\u672c\u8a9e](..\/..\/..\/ja\/archive\/connectors\/spark-xml-library.html)\n* [Portugu\u00eas](..\/..\/..\/pt\/archive\/connectors\/spark-xml-library.html)\n\n[![](..\/..\/_static\/icons\/aws.svg)Amazon\n Web Services](javascript:void(0))\n* [![](..\/..\/_static\/icons\/azure.svg)Microsoft Azure](https:\/\/learn.microsoft.com\/azure\/databricks\/archive\/connectors\/spark-xml-library)\n* [![](..\/..\/_static\/icons\/gcp.svg)Google Cloud Platform](https:\/\/docs.gcp.databricks.com\/archive\/connectors\/spark-xml-library.html)\n\n[Databricks on AWS](..\/..\/index.html)\nGet started\n\n* [Get started](..\/..\/getting-started\/index.html)\n* [What is Databricks?](..\/..\/introduction\/index.html)\n* [DatabricksIQ](..\/..\/databricksiq\/index.html)\n* [Release notes](..\/..\/release-notes\/index.html)\n\nLoad & manage data\n\n* [Connect to data sources](..\/..\/connect\/index.html)\n* [Connect to compute](..\/..\/compute\/index.html)\n* [Discover data](..\/..\/discover\/index.html)\n* [Query data](..\/..\/query\/index.html)\n* [Ingest data](..\/..\/ingestion\/index.html)\n* [Transform data](..\/..\/transform\/index.html)\n* [Monitor data and AI assets](..\/..\/lakehouse-monitoring\/index.html)\n* [Share data (Delta Sharing)](..\/..\/data-sharing\/index.html)\n* [Databricks Marketplace](..\/..\/marketplace\/index.html)\n\nWork with data\n\n* [Data engineering](..\/..\/workspace-index.html)\n* [Generative AI & LLMs](..\/..\/generative-ai\/generative-ai.html)\n* [Machine learning](..\/..\/machine-learning\/index.html)\n* [Model serving](..\/..\/machine-learning\/model-serving\/index.html)\n* [Data warehousing](..\/..\/sql\/index.html)\n* [Delta Lake](..\/..\/delta\/index.html)\n* [Developers](..\/..\/languages\/index.html)\n* [Technology partners](..\/..\/integrations\/index.html)\n\nAdministration\n\n* [Account and workspace administration](..\/..\/admin\/index.html)\n* [Security and compliance](..\/..\/security\/index.html)\n* [Data governance (Unity Catalog)](..\/..\/data-governance\/index.html)\n* [Lakehouse architecture](..\/..\/lakehouse-architecture\/index.html)\n\nReference & resources\n\n* [Reference](..\/..\/reference\/api.html)\n* [Resources](..\/..\/resources\/index.html)\n* [What\u2019s coming?](..\/..\/whats-coming.html)\n* [Documentation archive](..\/index.html)\n\t+ [End-of-life for legacy workspaces](..\/aws\/end-of-life-legacy-workspaces.html)\n\t+ [Administration guide (legacy)](..\/admin-guide\/index.html)\n\t+ [Create cluster UI (legacy)](..\/compute\/configure.html)\n\t+ [Cluster UI preview](..\/compute\/cluster-ui-preview.html)\n\t+ [Install a library with an init script (legacy)](..\/compute\/libraries-init-scripts.html)\n\t+ [Cluster-named init scripts (legacy)](..\/init-scripts\/legacy-cluster-named.html)\n\t+ [Global init scripts (legacy)](..\/init-scripts\/legacy-global.html)\n\t+ [Compute policies best practices](..\/compute\/policies-best-practices.html)\n\t+ [Manage libraries with `%conda` commands (legacy)](..\/legacy\/conda.html)\n\t+ [Explore and create tables in DBFS](..\/legacy\/data-tab.html)\n\t+ [Transactional writes to cloud storage with DBIO](..\/legacy\/dbio-commit.html)\n\t+ [Browse files in DBFS](..\/legacy\/file-browser.html)\n\t+ [FileStore](..\/legacy\/filestore.html)\n\t+ [Koalas](..\/legacy\/koalas.html)\n\t+ [Skew join optimization using skew hints](..\/legacy\/skew-join.html)\n\t+ [Legacy UniForm IcebergCompatV1](..\/legacy\/uniform.html)\n\t+ [Workspace libraries (legacy)](..\/legacy\/workspace-libraries.html)\n\t+ [Databricks CLI (legacy)](..\/dev-tools\/cli\/index.html)\n\t+ [What is dbx by Databricks Labs?](..\/dev-tools\/dbx\/index.html)\n\t+ [dbutils.library](..\/dev-tools\/dbutils-library.html)\n\t+ [Migrate to Spark 3.x](..\/spark-3.x-migration\/index.html)\n\t+ [VScode with Git folders](..\/dev-tools\/vscode-repos.html)\n\t+ [External metastores (legacy)](..\/external-metastores\/index.html)\n\t+ [Credential passthrough (legacy)](..\/credential-passthrough\/index.html)\n\t+ [Share feature tables across workspaces (legacy)](..\/machine-learning\/feature-store\/multiple-workspaces.html)\n\t+ [MLeap ML model export](..\/model-export\/mleap-model-export.html)\n\t+ [Train a PySpark model and save in MLeap format](..\/model-export\/tracking-ex-pyspark.html)\n\t+ [Deploy MLeap model on SageMaker](..\/model-export\/mleap-model-deployment-on-sagemaker.html)\n\t+ [Model serving (legacy)](..\/legacy-model-serving\/model-serving.html)\n\t+ [Serverless Real-Time Inference (preview)](..\/serverless-inference-preview\/serverless-real-time-inference.html)\n\t+ [Optimized LLM serving](..\/machine-learning\/llm-optimized-model-serving.html)\n\t+ [Set up and considerations for `ai_generate_text()`](..\/machine-learning\/ai-onboard.html)\n\t+ [Analyze customer reviews with `ai_generate_text()` and OpenAI](..\/machine-learning\/ai-generate-text-example.html)\n\t+ [Databricks light](..\/runtime\/light.html)\n\t+ [Databricks runtime release notes (unsupported)](..\/runtime-release-notes\/index.html)\n\t+ [Unity Catalog GA release note](..\/unity-catalog\/20220825.html)\n\t+ [Audit log schemas for security monitoring](..\/security\/monitor-log-schemas.html)\n\t+ [Create and verify a cluster for legacy HIPAA support](..\/security\/hipaa-legacy-cluster.html)\n\t+ [End-of-life for legacy workspaces](..\/aws\/end-of-life-legacy-workspaces.html)\n\t+ [Amazon S3 source with Amazon SQS (legacy)](..\/aws\/sqs.html)\n\t+ [Azure Blob storage file source with Azure Queue Storage (legacy)](..\/azure\/aqs.html)\n\t+ [Azure Cosmos DB](..\/azure\/cosmosdb.html)\n\t+ [Connecting Databricks and Azure Synapse with PolyBase (legacy)](..\/azure\/synapse-polybase.html)\n\t+ [Neo4j](neo4j.html)\n\t+ [Read and write XML data using the `spark-xml` library](#)\n\t+ [Accessing Azure Data Lake Storage Gen1 from Databricks](..\/storage\/azure-datalake.html)\n\t+ [Configure Delta storage credentials](..\/storage\/delta-storage-credentials.html)\n\t+ [Connect to Azure Blob Storage with WASB (legacy)](..\/storage\/wasb-blob.html)\n\n Updated May 28, 2024\n \n\n[Send us feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n\n* [Documentation](..\/..\/index.html)\n* [Databricks documentation archive](..\/index.html)\n* Read and write XML data using the `spark-xml` library\n* \n\n# Read and write XML data using the `spark-xml` library\n\nImportant\n\nThis documentation has been retired and might not be updated. The products, services, or technologies mentioned int his content are not officially endorsed or tested by Databricks.\n\nNative XML file format support is available as a Public Preview. See [Read and write XML files](..\/..\/query\/formats\/xml.html).\n\nThis article describes how to read and write an XML file as an Apache Spark data source.\n\n## Requirements\n\n1. Create the `spark-xml` library as a [Maven library](..\/..\/libraries\/package-repositories.html#maven-libraries). For the Maven coordinate, specify:\n\n\t* Databricks Runtime 7.x and above: `com.databricks:spark-xml_2.12:<release>`See `spark-xml` [Releases](https:\/\/github.com\/databricks\/spark-xml\/releases) for the latest version of `<release>`.\n2. [Install the library](..\/..\/libraries\/cluster-libraries.html#install-libraries) on a cluster.\n\n## Example\n\nThe example in this section uses the [books](https:\/\/github.com\/databricks\/spark-xml\/raw\/master\/src\/test\/resources\/books.xml) XML file.\n\n1. Retrieve the books XML file:\n\n```\n$ wget https:\/\/github.com\/databricks\/spark-xml\/raw\/master\/src\/test\/resources\/books.xml\n\n```\n2. Upload the file to [DBFS](..\/legacy\/data-tab.html).\n\n### Read and write XML data\n\n```\n\/*Infer schema*\/\n\nCREATE TABLE books\nUSING xml\nOPTIONS (path \"dbfs:\/books.xml\", rowTag \"book\")\n\n\/*Specify column names and types*\/\n\nCREATE TABLE books (author string, description string, genre string, _id string, price double, publish_date string, title string)\nUSING xml\nOPTIONS (path \"dbfs:\/books.xml\", rowTag \"book\")\n\n```\n\n```\n\/\/ Infer schema\n\nimport com.databricks.spark.xml._ \/\/ Add the DataFrame.read.xml() method\n\nval df = spark.read\n  .option(\"rowTag\", \"book\")\n  .xml(\"dbfs:\/books.xml\")\n\nval selectedData = df.select(\"author\", \"_id\")\nselectedData.write\n  .option(\"rootTag\", \"books\")\n  .option(\"rowTag\", \"book\")\n  .xml(\"dbfs:\/newbooks.xml\")\n\n\/\/ Specify schema\n\nimport org.apache.spark.sql.types.{StructType, StructField, StringType, DoubleType}\n\nval customSchema = StructType(Array(\n  StructField(\"_id\", StringType, nullable = true),\n  StructField(\"author\", StringType, nullable = true),\n  StructField(\"description\", StringType, nullable = true),\n  StructField(\"genre\", StringType, nullable = true),\n  StructField(\"price\", DoubleType, nullable = true),\n  StructField(\"publish_date\", StringType, nullable = true),\n  StructField(\"title\", StringType, nullable = true)))\n\nval df = spark.read\n  .option(\"rowTag\", \"book\")\n  .schema(customSchema)\n  .xml(\"books.xml\")\n\nval selectedData = df.select(\"author\", \"_id\")\nselectedData.write\n  .option(\"rootTag\", \"books\")\n  .option(\"rowTag\", \"book\")\n  .xml(\"dbfs:\/newbooks.xml\")\n\n```\n\n```\n# Infer schema\n\nlibrary(SparkR)\n\nsparkR.session(\"local[4]\", sparkPackages = c(\"com.databricks:spark-xml_2.12:<release>\"))\n\ndf <- read.df(\"dbfs:\/books.xml\", source = \"xml\", rowTag = \"book\")\n\n# Default `rootTag` and `rowTag`\nwrite.df(df, \"dbfs:\/newbooks.xml\", \"xml\")\n\n# Specify schema\n\ncustomSchema <- structType(\n  structField(\"_id\", \"string\"),\n  structField(\"author\", \"string\"),\n  structField(\"description\", \"string\"),\n  structField(\"genre\", \"string\"),\n  structField(\"price\", \"double\"),\n  structField(\"publish_date\", \"string\"),\n  structField(\"title\", \"string\"))\n\ndf <- read.df(\"dbfs:\/books.xml\", source = \"xml\", schema = customSchema, rowTag = \"book\")\n\n# In this case, `rootTag` is set to \"ROWS\" and `rowTag` is set to \"ROW\".\nwrite.df(df, \"dbfs:\/newbooks.xml\", \"xml\", \"overwrite\")\n\n```\n\n## Options\n\n* Read"}]}
{"request_id":"id-6134","request":"\"What is the difference between a Databricks user and a service principal, and when should I use each one?\"","response":"To manage service principals in Databricks, you can use the Databricks Terraform provider to create and manage service principals for your Databricks account and workspaces. The Databricks Terraform provider allows you to create, update, and delete service principals, as well as grant and revoke roles and permissions for service principals.\n\nTo get started with the Databricks Terraform provider, you will need to install the provider and configure it with your Databricks authentication credentials. Once you have done this, you can use the `databricks_service_principal` resource to create and manage service principals.\n\nHere is an example of how to create a service principal using the Databricks Terraform provider:\n\n```\nprovider \"databricks\" {\n  host = \"https:\/\/<your-databricks-workspace-url>\"\n  token = \"<your-databricks-token>\"\n}\n\nresource \"databricks_service_principal\" \"example\" {\n  display_name = \"example-service-principal\"\n}\n```\n\nThis will create a new service principal with the display name \"example-service-principal\" in your Databricks workspace.\n\nYou can also grant roles and permissions to service principals using the `databricks_permissions` resource. For example, to grant the \"read\" permission on a Databricks workspace to a service principal, you can use the following code:\n\n```\nresource \"databricks_permissions\" \"example\" {\n  provider = databricks.example\n\n  authorization = \"tokens\"\n\n  resource_id = \"<your-databricks-workspace-id>\"\n\n  access_control {\n    service_principal_name = databricks_service_principal.example.display_name\n    permission_level      = \"read\"\n  }\n}\n```\n\nThis will grant the \"read\" permission on the specified Databricks workspace to the service principal created in the previous example.\n\nFor more information on using the Databricks Terraform provider to manage service principals, please see the [Databricks Terraform provider documentation](https:\/\/registry.terraform.io\/providers\/databrickslabs\/databricks\/latest\/docs\/resources\/service_principal).","expected_retrieved_context":[{"doc_uri":"https:\/\/docs.databricks.com\/getting-started\/concepts.html","content":"Databricks concepts  \nThis article introduces the set of fundamental concepts you need to understand in order to use Databricks effectively.  \nAccounts and workspaces\nAccounts and workspaces\nIn Databricks, a workspace is a Databricks deployment in the cloud that functions as an environment for your team to access Databricks assets. Your organization can choose to have either multiple workspaces or just one, depending on its needs.  \nA Databricks account represents a single entity that can include multiple workspaces. Accounts enabled for Unity Catalog can be used to manage users and their access to data centrally across all of the workspaces in the account. Billing and support are also handled at the account level.\n\nBilling: Databricks units (DBUs)\nBilling: Databricks units (DBUs)\nDatabricks bills based on Databricks units (DBUs), units of processing capability per hour based on VM instance type.  \nSee the Databricks on AWS pricing estimator.\n\nAuthentication and authorization\nAuthentication and authorization\nThis section describes concepts that you need to know when you manage Databricks identities and their access to Databricks assets.  \nUser  \nA unique individual who has access to the system. User identities are represented by email addresses. See Manage users.  \nService principal  \nA service identity for use with jobs, automated tools, and systems such as scripts, apps, and CI\/CD platforms. Service principals are represented by an application ID. See Manage service principals.  \nGroup  \nA collection of identities. Groups simplify identity management, making it easier to assign access to workspaces, data, and other securable objects. All Databricks identities can be assigned as members of groups. See Manage groups  \nAccess control list (ACL)  \nA list of permissions attached to the workspace, cluster, job, table, or experiment. An ACL specifies which users or system processes are granted access to the objects, as well as what operations are allowed on the assets. Each entry in a typical ACL specifies a subject and an operation. See Access control lists  \nPersonal access token  \nAn opaque string is used to authenticate to the REST API and by tools in the Technology partners to connect to SQL warehouses. See Databricks personal access token authentication.  \nUI  \nThe Databricks UI is a graphical interface for interacting with features, such as workspace folders and their contained objects, data objects, and computational resources.\n\nData science & engineering\nData science & engineering\nData science & engineering tools aid collaboration among data scientists, data engineers, and data analysts. This section describes the fundamental concepts.  \nWorkspace  \nA workspace is an environment for accessing all of your Databricks assets. A workspace organizes objects (notebooks, libraries, dashboards, and experiments) into folders and provides access to data objects and computational resources.  \nNotebook  \nA web-based interface for creating data science and machine learning workflows that can contain runnable commands, visualizations, and narrative text. See Introduction to Databricks notebooks.  \nDashboard  \nAn interface that provides organized access to visualizations. See Dashboards in notebooks.  \nLibrary  \nA package of code available to the notebook or job running on your cluster. Databricks runtimes include many libraries and you can add your own.  \nGit folder (formerly Repos)  \nA folder whose contents are co-versioned together by syncing them to a remote Git repository. Databricks Git folders integrate with Git to provide source and version control for your projects.  \nExperiment  \nA collection of MLflow runs for training a machine learning model. See Organize training runs with MLflow experiments.\n\nDatabricks interfaces\nDatabricks interfaces\nThis section describes the interfaces that Databricks supports, in addition to the UI, for accessing your assets: API and command-line (CLI).  \nREST API  \nThe Databricks REST API provides endpoints for modifying or requesting information about Databricks account and workspace objects. See account reference and workspace reference.  \nCLI  \nThe Databricks CLI is hosted on GitHub. The CLI is built on top of the Databricks REST API.\n\nData management"}],"expected_response":"A Databricks user is a unique individual who has access to the system, represented by an email address. Users can interact with the Databricks workspace, notebooks, and other assets. They are typically used for data scientists, engineers, and analysts who need to access and work with data directly.\n\nA service principal, on the other hand, is a service identity for use with jobs, automated tools, and systems such as scripts, apps, and CI\/CD platforms. Service principals are represented by an application ID. They are typically used for programmatic access to the Databricks workspace, such as running jobs, managing clusters, and accessing data objects. Service principals are useful when you need to automate tasks or integrate with other systems, as they provide a way to authenticate and access Databricks assets without requiring a user account.\n\nIn summary, you should use a Databricks user for individual users who need to interact with the Databricks workspace, and a service principal for programmatic access and automation tasks.","retrieved_context":[{"doc_uri":"https:\/\/docs.databricks.com\/admin\/users-groups\/service-principals.html","content":"Manage service principals | Databricks on AWS\n\n* [![Databricks](..\/..\/_static\/small-scale-lockup-full-color-rgb.svg)](https:\/\/www.databricks.com\/)\n\n* [![Databricks](..\/..\/_static\/small-scale-lockup-full-color-rgb.svg)](https:\/\/www.databricks.com\/)\n* [Help Center](https:\/\/help.databricks.com\/s\/)\n* [Documentation](https:\/\/docs.databricks.com\/en\/index.html)\n* [Knowledge Base](https:\/\/kb.databricks.com\/)\n\n* [Community](https:\/\/community.databricks.com)\n* [Support](https:\/\/help.databricks.com)\n* [Feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n* [Try Databricks](https:\/\/databricks.com\/try-databricks)\n\n[![](..\/..\/_static\/icons\/globe.png)English](javascript:void(0))\n* [English](..\/..\/..\/en\/admin\/users-groups\/service-principals.html)\n* [\u65e5\u672c\u8a9e](..\/..\/..\/ja\/admin\/users-groups\/service-principals.html)\n* [Portugu\u00eas](..\/..\/..\/pt\/admin\/users-groups\/service-principals.html)\n\n[![](..\/..\/_static\/icons\/aws.svg)Amazon\n Web Services](javascript:void(0))\n* [![](..\/..\/_static\/icons\/azure.svg)Microsoft Azure](https:\/\/learn.microsoft.com\/azure\/databricks\/admin\/users-groups\/service-principals)\n* [![](..\/..\/_static\/icons\/gcp.svg)Google Cloud Platform](https:\/\/docs.gcp.databricks.com\/admin\/users-groups\/service-principals.html)\n\n[Databricks on AWS](..\/..\/index.html)\nGet started\n\n* [Get started](..\/..\/getting-started\/index.html)\n* [What is Databricks?](..\/..\/introduction\/index.html)\n* [DatabricksIQ](..\/..\/databricksiq\/index.html)\n* [Release notes](..\/..\/release-notes\/index.html)\n\nLoad & manage data\n\n* [Connect to data sources](..\/..\/connect\/index.html)\n* [Connect to compute](..\/..\/compute\/index.html)\n* [Discover data](..\/..\/discover\/index.html)\n* [Query data](..\/..\/query\/index.html)\n* [Ingest data](..\/..\/ingestion\/index.html)\n* [Transform data](..\/..\/transform\/index.html)\n* [Monitor data and AI assets](..\/..\/lakehouse-monitoring\/index.html)\n* [Share data (Delta Sharing)](..\/..\/data-sharing\/index.html)\n* [Databricks Marketplace](..\/..\/marketplace\/index.html)\n\nWork with data\n\n* [Data engineering](..\/..\/workspace-index.html)\n* [Generative AI & LLMs](..\/..\/generative-ai\/generative-ai.html)\n* [Machine learning](..\/..\/machine-learning\/index.html)\n* [Model serving](..\/..\/machine-learning\/model-serving\/index.html)\n* [Data warehousing](..\/..\/sql\/index.html)\n* [Delta Lake](..\/..\/delta\/index.html)\n* [Developers](..\/..\/languages\/index.html)\n* [Technology partners](..\/..\/integrations\/index.html)\n\nAdministration\n\n* [Account and workspace administration](..\/index.html)\n\t+ [Get started with Databricks administration](..\/..\/getting-started\/admin-get-started.html)\n\t+ [Account administration](..\/account-settings\/index.html)\n\t+ [Workspace deployment](..\/workspace\/index.html)\n\t+ [Manage a workspace](..\/workspace-settings\/index.html)\n\t+ [Identity management](index.html)\n\t\t- [Identity best practices](best-practices.html)\n\t\t- [Manage users](users.html)\n\t\t- [Manage service principals](#)\n\t\t\t* [Roles for managing service principals](..\/..\/security\/auth-authz\/access-control\/service-principal-acl.html)\n\t\t- [Manage groups](groups.html)\n\t\t- [Sync users and groups from your identity provider](scim\/index.html)\n\t+ [Compute policies](..\/clusters\/policies.html)\n\t+ [Audit logs](..\/account-settings\/audit-logs.html)\n\t+ [System tables](..\/system-tables\/index.html)\n* [Security and compliance](..\/..\/security\/index.html)\n* [Data governance (Unity Catalog)](..\/..\/data-governance\/index.html)\n* [Lakehouse architecture](..\/..\/lakehouse-architecture\/index.html)\n\nReference & resources\n\n* [Reference](..\/..\/reference\/api.html)\n* [Resources](..\/..\/resources\/index.html)\n* [What\u2019s coming?](..\/..\/whats-coming.html)\n* [Documentation archive](..\/..\/archive\/index.html)\n\n Updated May 28, 2024\n \n\n[Send us feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n\n* [Documentation](..\/..\/index.html)\n* [Databricks administration introduction](..\/index.html)\n* [Manage users, service principals, and groups](index.html)\n* Manage service principals\n* \n\n# Manage service principals\n\nThis article explains how to create and manage service principals for your Databricks account and workspaces.\n\nFor an overview of the Databricks identity model, see [Databricks identities](index.html#identity-model).\n\nTo manage access for service principals, see [Authentication and access control](..\/..\/security\/auth-authz\/index.html).\n\n## What is a service principal?\n\nA service principal is an identity that you create in Databricks for use with automated tools, jobs, and applications. Service principals give automated tools and scripts API-only access to Databricks resources, providing greater security than using users or groups.\n\nYou can grant and restrict a service principal\u2019s access to resources in the same way as you can a Databricks user. For example, you can do the following:\n\n* Give a service principal account admin and workspace admin roles.\n* Give a service principal access to data, either at the account level using Unity Catalog, or at the workspace level.\n* [Add a service principal to a group](groups.html) at both the account and workspace level, including the workspace `admins` group.\n\nYou can also grant Databricks users, service principals, and groups permissions to use a service principal. This allows users to run jobs as the service principal, instead of as their identity. This prevents jobs from failing if a user leaves your organization or a group is modified.\n\nUnlike a Databricks user, a service principal is an API-only identity; it cannot be used to access the Databricks UI.\n\nDatabricks recommends that you [enable your workspaces for identity federation](index.html#enable-identity-federation). Identity federation enables you to configure service principals in the account console, and then assign them access to specific workspaces. This simplifies Databricks administration and data governance.\n\nImportant\n\nIf your account was created after November 8, 2023, identity federation is enabled on all new workspaces by default, and it cannot be disabled.\n\n## Who can manage and use service principals?\n\nTo manage service principals in Databricks, you must have one of the following: the account admin role, the workspace admin role, or the manager or user role on a service principal.\n\n* **Account admins** can add service principals to the account and assign them admin roles. They can also assign service principals to workspaces, as long as those workspaces use identity federation.\n* **Workspace admins** can add service principals to a Databricks workspace, assign them the workspace admin role, and manage access to objects and functionality in the workspace, such as the ability to create clusters or access specified persona-based environments.\n* **Service Principal Managers** can manage roles on a service principal. The creator of a service principal becomes the service principal manager. Account admins are service principal managers on all service principals in an account.\n\nNote\n\nIf a service principal was created before June 13, 2023, then the creator of the service principal does not have the service principal manager role by default. Ask an account admin to grant you the service principal manager role.\n\n* **Service Principal Users** can run jobs as the service principal. The job runs using the identity of the service principal, instead of the identity of the job owner. For more information, see [Run a job as a service principal](..\/..\/workflows\/jobs\/create-run-jobs.html#run-as-sp).\n\n **Service principal Users** that are workspace admins can also create tokens on behalf of the service principal.\n\nNote\n\nWhen the `RestrictWorkspaceAdmins` setting on a workspace is set to `ALLOW ALL`, workspace admins can create a personal access token on behalf of any service principal in their workspace. See [Restrict workspace admins](..\/workspace-settings\/restrict-workspace-admins.html).\n\nFor information on how to grant the service principal manager and user roles, see [Roles for managing service principals](..\/..\/security\/auth-authz\/access-control\/service-principal-acl.html).\n\n## Manage service principals in your account\n\nAccount admins can add service principals to your Databricks account using the account console.\n\n### Add service principals to your account using the account console\n\n1. As an account admin, log in to the [account console](https:\/\/accounts.cloud.databricks.com).\n2. In the sidebar, click **User management**.\n3. On the **Service principals** tab, click **Add service principal**.\n4. Enter a name for the service principal.\n5. Click **Add**.\n\n### Assign account admin roles to a service principal\n\n1. As an account admin, log in to the [account console](https:\/\/accounts.cloud.databricks.com).\n2. In the sidebar, click **User management**.\n3. On the **Service principals** tab, find and click the username.\n4. On the **Roles** tab, turn on **Account admin** or **Marketplace admin**.\n\n### Assign a service principal to a workspace using the account console\n\nTo add users to a workspace using the [account console](https:\/\/accounts.cloud.databricks.com), the workspace must be enabled for identity federation. Workspace admins can also assign service principals to workspaces using the workspace admin settings page. For details, see [Add a service principal to a workspace using the workspace admin settings](#add-sp-admin-console).\n\n1. As an account admin, log in to the [account console](https:\/\/accounts.cloud.databricks.com).\n2. In the sidebar, click **Workspaces**.\n3. Click your workspace name.\n4. On the **Permissions** tab, click **Add permissions**.\n5. Search for and select the service principal, assign the permission level (workspace **User** or **Admin**), and click **Save**.\n\n### Remove a service principal from a workspace using the account console\n\nTo remove service principals from a workspace using the account console, the workspace must be enabled for identity federation. When a service principal is removed from a workspace, the service principal can no longer access the workspace, however permissions are maintained on the service principal. If the service principal is later added back to the workspace, it regains its previous permissions.\n\n1. As an account admin, log in to the [account console](https:\/\/accounts.cloud.databricks.com)\n2. In the sidebar, click **Workspaces**.\n3. Click your workspace name.\n4. On the **Permissions** tab, find the service principal.\n5. Click the ![Kebab menu](..\/..\/_images\/kebab-menu.png) kebab menu at the far right of the service principal row and select **Remove**.\n6. On the confirmation dialog, click **Remove**.\n\n### Deactivate a service principal in your Databricks account\n\nAccount admins can deactivate service principals across a Databricks account. A deactivated service principal cannot authenticate to the Databricks account or workspaces. However, all of the service principal\u2019s permissions and workspace objects remain unchanged. When a service principal is deactivated the following is true:\n\n* The service principal cannot authenticate to the account or any of their workspaces from any method.\n* Applications or scripts that use the tokens generated by the service principal are no longer able to access the Databricks API. The tokens remain but cannot be used to authenticate while a service principal is deactivated.\n* Clusters owned by the service principal remain running.\n* Scheduled jobs created by the service principal fail unless they are assigned to a new owner.\n\nWhen a service principal is reactivated, it can login to Databricks with the same permissions. Databricks recommends deactivating service principals from the account instead of removing them because removing a service principal is a destructive action. See [Remove service principals from your Databricks account](#remove-sp). When you deactivate a service principal from the account, that service principal is also deactivated from their identity federated workspaces.\n\nYou cannot deactivate a user using the account console. Instead, use the Account Service Principals API. See [Deactivate a service principal using the API](#deactivate-api).\n\n### Remove service principals from your Databricks account\n\nAccount admins can delete service principals from a Databricks account. Workspace admins cannot. When you delete a service principal from the account, that principal is also removed from their workspaces.\n\nImportant\n\nWhen you remove a service principal from the account, that service principal is also removed from their workspaces, regardless of whether or not identity federation has been enabled. We recommend that you refrain from deleting account-level service principals unless you want them to lose access to all workspaces in the account. Be aware of the following consequences of deleting service principals:\n\n* Applications or scripts that use the tokens generated by the service principal can no longer access Databricks APIs\n* Jobs owned by the service principal fail\n* Clusters owned by the service principal stop\n* Queries or dashboards created by the service principal and shared using the Run as Owner credential have to be assigned to a new owner to prevent sharing from failing\n\nTo remove a service principal using the [account console](https:\/\/accounts.cloud.databricks.com), do the following:\n\n1. As an account admin, log in to the account console.\n2. In the sidebar, click **User management**.\n3. On the **Service principals** tab, find and click the username.\n4. On the **Principal Information** tab, click the ![Kebab menu](..\/..\/_images\/kebab-menu.png) kebab menu in the upper-right corner and select **Delete**.\n5. In the confirmation dialog box, click **Confirm delete**."},{"doc_uri":"https:\/\/docs.databricks.com\/dev-tools\/service-principals-tools-apis.html","content":"Provision a service principal by using Terraform | Databricks on AWS\n\n* [![Databricks](..\/_static\/small-scale-lockup-full-color-rgb.svg)](https:\/\/www.databricks.com\/)\n\n* [![Databricks](..\/_static\/small-scale-lockup-full-color-rgb.svg)](https:\/\/www.databricks.com\/)\n* [Help Center](https:\/\/help.databricks.com\/s\/)\n* [Documentation](https:\/\/docs.databricks.com\/en\/index.html)\n* [Knowledge Base](https:\/\/kb.databricks.com\/)\n\n* [Community](https:\/\/community.databricks.com)\n* [Support](https:\/\/help.databricks.com)\n* [Feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n* [Try Databricks](https:\/\/databricks.com\/try-databricks)\n\n[![](..\/_static\/icons\/globe.png)English](javascript:void(0))\n* [English](..\/..\/en\/dev-tools\/service-principals-tools-apis.html)\n* [\u65e5\u672c\u8a9e](..\/..\/ja\/dev-tools\/service-principals-tools-apis.html)\n* [Portugu\u00eas](..\/..\/pt\/dev-tools\/service-principals-tools-apis.html)\n\n[![](..\/_static\/icons\/aws.svg)Amazon\n Web Services](javascript:void(0))\n* [![](..\/_static\/icons\/azure.svg)Microsoft Azure](https:\/\/learn.microsoft.com\/azure\/databricks\/dev-tools\/service-principals-tools-apis)\n* [![](..\/_static\/icons\/gcp.svg)Google Cloud Platform](https:\/\/docs.gcp.databricks.com\/dev-tools\/service-principals-tools-apis.html)\n\n[Databricks on AWS](..\/index.html)\nGet started\n\n* [Get started](..\/getting-started\/index.html)\n* [What is Databricks?](..\/introduction\/index.html)\n* [DatabricksIQ](..\/databricksiq\/index.html)\n* [Release notes](..\/release-notes\/index.html)\n\nLoad & manage data\n\n* [Connect to data sources](..\/connect\/index.html)\n* [Connect to compute](..\/compute\/index.html)\n* [Discover data](..\/discover\/index.html)\n* [Query data](..\/query\/index.html)\n* [Ingest data](..\/ingestion\/index.html)\n* [Transform data](..\/transform\/index.html)\n* [Monitor data and AI assets](..\/lakehouse-monitoring\/index.html)\n* [Share data (Delta Sharing)](..\/data-sharing\/index.html)\n* [Databricks Marketplace](..\/marketplace\/index.html)\n\nWork with data\n\n* [Data engineering](..\/workspace-index.html)\n* [Generative AI & LLMs](..\/generative-ai\/generative-ai.html)\n* [Machine learning](..\/machine-learning\/index.html)\n* [Model serving](..\/machine-learning\/model-serving\/index.html)\n* [Data warehousing](..\/sql\/index.html)\n* [Delta Lake](..\/delta\/index.html)\n* [Developers](..\/languages\/index.html)\n\t+ [Python](..\/languages\/python.html)\n\t+ [R](..\/sparkr\/index.html)\n\t+ [Scala](..\/languages\/scala.html)\n\t+ [SQL](..\/sql\/language-manual\/index.html)\n\t+ [UDFs](..\/udf\/index.html)\n\t+ [Tools](index.html)\n\t\t- [Authentication](auth\/index.html)\n\t\t\t* [Databricks personal access token authentication](auth\/pat.html)\n\t\t\t* [Basic (legacy) authentication](auth\/basic.html)\n\t\t\t* [OAuth M2M authentication](auth\/oauth-m2m.html)\n\t\t\t* [OAuth U2M authentication](auth\/oauth-u2m.html)\n\t\t\t* [Create a service principal with Terraform](#)\n\t\t- [Databricks Connect](databricks-connect\/index.html)\n\t\t- [Visual Studio Code](visual-studio-code.html)\n\t\t- [Databricks extension for Visual Studio Code](vscode-ext\/index.html)\n\t\t- [PyCharm overview](pycharm.html)\n\t\t- [IntelliJ IDEA](intellij-idea.html)\n\t\t- [Eclipse](eclipse.html)\n\t\t- [RStudio Desktop](rstudio.html)\n\t\t- [SDKs](index-sdk.html)\n\t\t- [SQL connectors and drivers](index-driver.html)\n\t\t- [SQL tools](index-sql.html)\n\t\t- [Databricks CLI](cli\/index.html)\n\t\t- [Utilities](databricks-utils.html)\n\t\t- [IaC](index-iac.html)\n\t\t- [CI\/CD](index-ci-cd.html)\n* [Technology partners](..\/integrations\/index.html)\n\nAdministration\n\n* [Account and workspace administration](..\/admin\/index.html)\n* [Security and compliance](..\/security\/index.html)\n* [Data governance (Unity Catalog)](..\/data-governance\/index.html)\n* [Lakehouse architecture](..\/lakehouse-architecture\/index.html)\n\nReference & resources\n\n* [Reference](..\/reference\/api.html)\n* [Resources](..\/resources\/index.html)\n* [What\u2019s coming?](..\/whats-coming.html)\n* [Documentation archive](..\/archive\/index.html)\n\n Updated May 28, 2024\n \n\n[Send us feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n\n* [Documentation](..\/index.html)\n* [Develop on Databricks](..\/languages\/index.html)\n* [Developer tools and guidance](index.html)\n* [Authentication for Databricks automation - overview](auth\/index.html)\n* Provision a service principal by using Terraform\n* \n\n# Provision a service principal by using Terraform\n\nNote\n\nTo provision a Databricks service principal by using the Databricks user interface instead, see [Manage service principals](..\/admin\/users-groups\/service-principals.html)..\n\nA [service principal](..\/admin\/users-groups\/service-principals.html) is an identity for automated tools and systems like scripts, apps, and CI\/CD platforms. Databricks recommends using a service principal and its OAuth token or personal access token instead of your Databricks user account and personal access token. Benefits include:\n\n* Granting and restricting access to resources independently of a user.\n* Enabling users to better protect their own access tokens.\n* Disabling or deleting a service principal without affecting other users.\n* Removing a user when they leave the organization without impacting any service principal.\n\nFollow these instructions to use the [Databricks Terraform provider](terraform\/index.html) to create a Databricks service principal in your Databricks workspace and then create a Databricks access token for the Databricks service principal.\n\n## Requirements\n\n* A Databricks personal access token to allow the Databricks Terraform provider to call the Databricks APIs on behalf of your Databricks user account within the Databricks workspace. To create a personal access token, do the following:\n\n\t1. In your Databricks workspace, click your Databricks username in the top bar, and then select **Settings** from the drop down.\n\t2. Click **Developer**.\n\t3. Next to **Access tokens**, click **Manage**.\n\t4. Click **Generate new token**.\n\t5. (Optional) Enter a comment that helps you to identify this token in the future, and change the token\u2019s default lifetime of 90 days. To create a token with no lifetime (not recommended), leave the **Lifetime (days)** box empty (blank).\n\t6. Click **Generate**.\n\t7. Copy the displayed token to a secure location, and then click **Done**.\nNote\n\nBe sure to save the copied token in a secure location. Do not share your copied token with others. If you lose the copied token, you cannot regenerate that exact same token. Instead, you must repeat this procedure to create a new token. If you lose the copied token, or you believe that the token has been compromised, Databricks strongly recommends that you immediately delete that token from your workspace by clicking the trash can (**Revoke**) icon next to the token on the **Access tokens** page.\n\nIf you are not able to create or use tokens in your workspace, this might be because your workspace administrator has disabled tokens or has not given you permission to create or use tokens. See your workspace administrator or the following:\n\n\t+ [Enable or disable personal access token authentication for the workspace](..\/admin\/access-control\/tokens.html#enable-tokens)\n\t+ [Personal access token permissions](..\/security\/auth-authz\/api-access-permissions.html#pat)\n* Databricks CLI version 0.205 or above, configured with a Databricks authentication configuration profile that references the corresponding Databricks personal access token. To create this configuration profile, do the following:\n\nNote\n\nThe following procedure uses the [Databricks CLI](cli\/install.html) to create a Databricks [configuration profile](auth\/index.html#config-profiles) with the name `DEFAULT`. If you already have a `DEFAULT` configuration profile, this procedure overwrites your existing `DEFAULT` configuration profile.\n\nTo check whether you already have a `DEFAULT` configuration profile, and to view this profile\u2019s settings if it exists, use the Databricks CLI to run the command `databricks auth env --profile DEFAULT`.\n\nTo create a configuration profile with a name other than `DEFAULT`, replace the `DEFAULT` part of `--profile DEFAULT` in the following `databricks configure` command with a different name for the configuration profile.\n\n\t1. Use the [Databricks CLI](cli\/install.html) to create a Databricks [configuration profile](auth\/index.html#config-profiles) named `DEFAULT` that uses Databricks personal access token authentication. To do this, run the following command:\n\t\n\t\n\t\n\t```\n\tdatabricks configure --profile DEFAULT\n\t\n\t```\n\t2. For the prompt **Databricks Host**, enter your Databricks [workspace instance URL](..\/workspace\/workspace-details.html#workspace-url), for example `https:\/\/dbc-a1b2345c-d6e7.cloud.databricks.com`.\n\t3. For the prompt **Personal Access Token**, enter the Databricks personal access token for your workspace.\n* The Terraform CLI. See [Download Terraform](https:\/\/www.terraform.io\/downloads.html).\n\n## Create the Databricks service principal and Databricks access token\n\n1. In your terminal, create an empty directory and then switch to it. Each separate set of Terraform configuration files must be in its own directory. For example: `mkdir terraform_service_principal_demo && cd terraform_service_principal_demo`.\n\n```\nmkdir terraform_service_principal_demo && cd terraform_service_principal_demo\n\n```\n2. In this empty directory, create a file named `main.tf`. Add the following content to this file, and then save the file.\n\nWarning\n\nThe following content contains the statement `authorization = \"tokens\"`. There can be only one `authorization = \"tokens\"` permissions resource per Databricks workspace. After applying the following changes, users who previously had either `CAN_USE` or `CAN_MANAGE` permission will have their access to token-based authentication revoked. Their active tokens are also immediately deleted (revoked). Because of the potentially disruptive nature of this operation, the related configuration is commented out in the `main.tf` file.\n\nAlternatively, you can use the Databricks user interface to enable the Databricks service principal to use a Databricks access token. See [Manage access to Databricks automation](..\/security\/auth-authz\/api-access-permissions.html). You can also use the Databricks user interface to generate a Databricks access token for the Databricks service principal. See [Manage tokens for a service principal](..\/admin\/users-groups\/service-principals.html#personal-access-tokens).\n\nNote\n\nThe following content creates a service principal at the Databricks workspace level. If your Databricks workspace is enabled for [identity federation](..\/admin\/users-groups\/index.html#enable-identity-federation), then the following content also automatically synchronizes the service principal to the related Databricks account (see [How do admins assign users to workspaces?](..\/admin\/users-groups\/index.html#assign-users-to-workspaces)). To create a service principal at the Databricks account level only instead of at the workspace level, see the \u201cCreating service principal in AWS Databricks account\u201d section of [databricks\\_service\\_principal Resource](https:\/\/registry.terraform.io\/providers\/databrickslabs\/databricks\/latest\/docs\/resources\/service_principal) in the Databricks Terraform provider documentation.\n\nIf you choose to uncomment the following resources and output, a personal access token is also generated. This personal access token can be used by the service principal for automation only within the specified Databricks workspace.\n\nYou cannot use personal access tokens with service principals for Databricks account-level automation. If you attempt to generate a personal access token for a service principal at the Databricks account level, the attempt will fail.\n\n```\nvariable \"databricks_connection_profile\" {\n  description = \"The name of the Databricks authentication configuration profile to use.\"\n  type        = string\n}\n\nvariable \"service_principal_display_name\" {\n  description = \"The display name for the service principal.\"\n  type        = string\n}\n\nvariable \"service_principal_access_token_lifetime\" {\n  description = \"The lifetime of the service principal's access token, in seconds.\"\n  type        = number\n  default     = 3600\n}\n\nterraform {\n  required_providers {\n    databricks = {\n      source = \"databricks\/databricks\"\n    }\n  }\n}\n\nprovider \"databricks\" {\n  profile = var.databricks_connection_profile\n}\n\nresource \"databricks_service_principal\" \"sp\" {\n  provider     = databricks\n  display_name = var.service_principal_display_name\n}"},{"doc_uri":"https:\/\/docs.databricks.com\/security\/auth-authz\/access-control\/service-principal-acl.html","content":"Roles for managing service principals | Databricks on AWS\n\n* [![Databricks](..\/..\/..\/_static\/small-scale-lockup-full-color-rgb.svg)](https:\/\/www.databricks.com\/)\n\n* [![Databricks](..\/..\/..\/_static\/small-scale-lockup-full-color-rgb.svg)](https:\/\/www.databricks.com\/)\n* [Help Center](https:\/\/help.databricks.com\/s\/)\n* [Documentation](https:\/\/docs.databricks.com\/en\/index.html)\n* [Knowledge Base](https:\/\/kb.databricks.com\/)\n\n* [Community](https:\/\/community.databricks.com)\n* [Support](https:\/\/help.databricks.com)\n* [Feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n* [Try Databricks](https:\/\/databricks.com\/try-databricks)\n\n[![](..\/..\/..\/_static\/icons\/globe.png)English](javascript:void(0))\n* [English](..\/..\/..\/..\/en\/security\/auth-authz\/access-control\/service-principal-acl.html)\n* [\u65e5\u672c\u8a9e](..\/..\/..\/..\/ja\/security\/auth-authz\/access-control\/service-principal-acl.html)\n* [Portugu\u00eas](..\/..\/..\/..\/pt\/security\/auth-authz\/access-control\/service-principal-acl.html)\n\n[![](..\/..\/..\/_static\/icons\/aws.svg)Amazon\n Web Services](javascript:void(0))\n* [![](..\/..\/..\/_static\/icons\/azure.svg)Microsoft Azure](https:\/\/learn.microsoft.com\/azure\/databricks\/security\/auth-authz\/access-control\/service-principal-acl)\n* [![](..\/..\/..\/_static\/icons\/gcp.svg)Google Cloud Platform](https:\/\/docs.gcp.databricks.com\/security\/auth-authz\/access-control\/service-principal-acl.html)\n\n[Databricks on AWS](..\/..\/..\/index.html)\nGet started\n\n* [Get started](..\/..\/..\/getting-started\/index.html)\n* [What is Databricks?](..\/..\/..\/introduction\/index.html)\n* [DatabricksIQ](..\/..\/..\/databricksiq\/index.html)\n* [Release notes](..\/..\/..\/release-notes\/index.html)\n\nLoad & manage data\n\n* [Connect to data sources](..\/..\/..\/connect\/index.html)\n* [Connect to compute](..\/..\/..\/compute\/index.html)\n* [Discover data](..\/..\/..\/discover\/index.html)\n* [Query data](..\/..\/..\/query\/index.html)\n* [Ingest data](..\/..\/..\/ingestion\/index.html)\n* [Transform data](..\/..\/..\/transform\/index.html)\n* [Monitor data and AI assets](..\/..\/..\/lakehouse-monitoring\/index.html)\n* [Share data (Delta Sharing)](..\/..\/..\/data-sharing\/index.html)\n* [Databricks Marketplace](..\/..\/..\/marketplace\/index.html)\n\nWork with data\n\n* [Data engineering](..\/..\/..\/workspace-index.html)\n* [Generative AI & LLMs](..\/..\/..\/generative-ai\/generative-ai.html)\n* [Machine learning](..\/..\/..\/machine-learning\/index.html)\n* [Model serving](..\/..\/..\/machine-learning\/model-serving\/index.html)\n* [Data warehousing](..\/..\/..\/sql\/index.html)\n* [Delta Lake](..\/..\/..\/delta\/index.html)\n* [Developers](..\/..\/..\/languages\/index.html)\n* [Technology partners](..\/..\/..\/integrations\/index.html)\n\nAdministration\n\n* [Account and workspace administration](..\/..\/..\/admin\/index.html)\n\t+ [Get started with Databricks administration](..\/..\/..\/getting-started\/admin-get-started.html)\n\t+ [Account administration](..\/..\/..\/admin\/account-settings\/index.html)\n\t+ [Workspace deployment](..\/..\/..\/admin\/workspace\/index.html)\n\t+ [Manage a workspace](..\/..\/..\/admin\/workspace-settings\/index.html)\n\t+ [Identity management](..\/..\/..\/admin\/users-groups\/index.html)\n\t\t- [Identity best practices](..\/..\/..\/admin\/users-groups\/best-practices.html)\n\t\t- [Manage users](..\/..\/..\/admin\/users-groups\/users.html)\n\t\t- [Manage service principals](..\/..\/..\/admin\/users-groups\/service-principals.html)\n\t\t\t* [Roles for managing service principals](#)\n\t\t- [Manage groups](..\/..\/..\/admin\/users-groups\/groups.html)\n\t\t- [Sync users and groups from your identity provider](..\/..\/..\/admin\/users-groups\/scim\/index.html)\n\t+ [Compute policies](..\/..\/..\/admin\/clusters\/policies.html)\n\t+ [Audit logs](..\/..\/..\/admin\/account-settings\/audit-logs.html)\n\t+ [System tables](..\/..\/..\/admin\/system-tables\/index.html)\n* [Security and compliance](..\/..\/index.html)\n* [Data governance (Unity Catalog)](..\/..\/..\/data-governance\/index.html)\n* [Lakehouse architecture](..\/..\/..\/lakehouse-architecture\/index.html)\n\nReference & resources\n\n* [Reference](..\/..\/..\/reference\/api.html)\n* [Resources](..\/..\/..\/resources\/index.html)\n* [What\u2019s coming?](..\/..\/..\/whats-coming.html)\n* [Documentation archive](..\/..\/..\/archive\/index.html)\n\n Updated May 28, 2024\n \n\n[Send us feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n\n* [Documentation](..\/..\/..\/index.html)\n* [Databricks administration introduction](..\/..\/..\/admin\/index.html)\n* [Manage users, service principals, and groups](..\/..\/..\/admin\/users-groups\/index.html)\n* [Manage service principals](..\/..\/..\/admin\/users-groups\/service-principals.html)\n* Roles for managing service principals\n* \n\n# Roles for managing service principals\n\nThis article describes how to manage roles on service principals in your Databricks account.\n\nA service principal is an identity that you create in Databricks for use with automated tools, jobs, and applications. Service principals give automated tools and scripts API-only access to Databricks resources, providing greater security than using users or groups.\n\nYou can grant Databricks users, service principals, and account groups access to use a service principal. This allows users to run jobs as the service principal, instead of as their identity. This prevents jobs from failing if a user leaves your organization or a group is modified.\n\nFor an overview of service principals see [Manage service principals](..\/..\/..\/admin\/users-groups\/service-principals.html).\n\n## Service principal roles\n\nService principal roles are account-level roles. This means that they only need to be defined once, in your account, and apply across all workspaces. There are two roles that you can grant on a service principal: **Service Principal Manager** and **Service Principal User**.\n\n* **Service Principal Manager** allows you to manage roles on a service principal. The creator of a service principal has the **Service Principal Manager** role on the service principal. Account admins have the **Service Principal Manager** role on all service principals in an account.\n\nNote\n\nIf a service principal was created before June 13, 2023, the creator of the service principal does not have the **Service Principal Manager** role by default. If you need to be a manager, ask an account admin to grant you the **Service Principal Manager** role.\n\n* **Service Principal User** allows workspace users to run jobs as the service principal. The job will run with the identity of the service principal, instead of the identity of the job owner. The **Service Principal User** role also allows workspace admins to create tokens on behalf of the service principal.\n\nNote\n\nWhen the `RestrictWorkspaceAdmins` setting on a workspace is set to `ALLOW ALL`, workspace admins can create a personal access token on behalf of any service principal in their workspace. To enforce the **Service Principal User** role for workspace admins to create a personal access token for a service principal, see [Restrict workspace admins](..\/..\/..\/admin\/workspace-settings\/restrict-workspace-admins.html).\n\nUsers with the **Service Principal Manager** role do not inherit the **Service Principal User** role. If you want to use the service principal to execute jobs, you need to explicitly assign yourself the service principal user role, even after creating the service principal.\n\n## Manage service principal roles using the account console\n\nAccount admins can manage service principals roles using the account console.\n\n### View roles on a service principal\n\n1. As an account admin, log in to the [account console](https:\/\/accounts.cloud.databricks.com).\n2. In the sidebar, click **User management**.\n3. On the **Service principals** tab, find and click the name.\n4. Click the **Permissions** tab.\n\nYou can see the list of principals and the roles that they are granted on the service principal. You can also use the search bar to search for a specific principal or role.\n\n### Grant roles on a service principal\n\n1. As an account admin, log in to the [account console](https:\/\/accounts.cloud.databricks.com).\n2. In the sidebar, click **User management**.\n3. On the **Service principals** tab, find and click the name.\n4. Click the **Permissions** tab.\n5. Click **Grant access**.\n6. Search for and select the user, service principal, or group and choose the role or roles (**Service principal: Manager** or **Service principal: User**) to assign.\n\nNote\n\nUsers with the **Service Principal Manager** role do not inherit the **Service Principal User** role. If you want the user to use the service principal to execute jobs, you will need to explicitly assign the **Service Principal User** role.\n7. Click **Save**.\n\n### Revoke roles on a service principal\n\n1. As an account admin, log in to the [account console](https:\/\/accounts.cloud.databricks.com).\n2. In the sidebar, click **User management**.\n3. On the **Service principals** tab, find and click the name.\n4. Click the **Permissions** tab.\n5. Search for the user, service principal, or group to edit their roles.\n6. On the row with the principal, click the kebab menu ![Vertical Ellipsis](..\/..\/..\/_images\/vertical-ellipsis.png) and then select **Edit**. Alternatively, select **Delete** to revoke all of the roles for the principal.\n7. Click **Edit**.\n8. Click the **X** next to the roles that you want to revoke.\n9. Click **Save**.\n\n## Manage service principal roles using the workspace admin settings page\n\nWorkspace admins can manage service principals roles for service principals that they have the **Service Principal Manager** role on using the admin settings page.\n\n### View roles on a service principal\n\n1. As a workspace admin, log in to the Databricks workspace.\n2. Click your username in the top bar of the Databricks workspace and select **Settings**.\n3. Click on the **Identity and access** tab.\n4. Next to **Service principals**, click **Manage**.\n5. Find and click the name.\n6. Click the **Permissions** tab.\n\nYou can see the list of principals and the roles that they are granted on the service principal. You can also use the search bar to search for a specific principal or role.\n\n### Grant roles on a service principal\n\nYou must have the **Service Principal Manager** role on a service principal in order to grant roles.\n\n1. As a workspace admin, log in to the Databricks workspace.\n2. Click your username in the top bar of the Databricks workspace and select **Settings**.\n3. Click on the **Identity and access** tab.\n4. Next to **Service principals**, click **Manage**.\n5. Find and click the name.\n6. Click the **Permissions** tab.\n7. Click **Grant access**.\n8. Search for and select the user, service principal, or group and choose the role or roles (**Service principal: Manager** or **Service principal: User**) to assign.\n\nNote\n\nRoles can be granted to any account-level user, service principal, or group, even if they are not a member of the workspace. Roles cannot be granted to workspace-local groups.\n\nUsers with the **Service Principal Manager** role do not inherit the **Service Principal User** role. If you want the user to use the service principal to execute jobs, you will need to explicitly assign the **Service Principal User** role.\n9. Click **Save**.\n\n### Revoke roles on a service principal\n\nYou must have the **Service Principal Manager** role on a service principal in order to revoke roles.\n\n1. As a workspace admin, log in to the Databricks workspace.\n2. Click your username in the top bar of the Databricks workspace and select **Settings**.\n3. Click on the **Identity and access** tab.\n4. Next to **Service principals**, click **Manage**.\n5. Find and click the name.\n6. Click the **Permissions** tab.\n7. Search for the user, service principal, or group to edit their roles.\n8. On the row with the principal, click the kebab menu ![Vertical Ellipsis](..\/..\/..\/_images\/vertical-ellipsis.png) and then select **Edit**. Alternatively, select **Delete** to revoke all of the roles for the principal.\n9. Click **Edit**.\n10. Click the **X** next to the roles that you want to revoke.\n11. Click **Save**.\n\n## Manage service principal roles using the Databricks CLI"},{"doc_uri":"https:\/\/docs.databricks.com\/sql\/language-manual\/sql-ref-principal.html","content":"Updated May 28, 2024\n \n\n[Send us feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n\n* [Documentation](..\/..\/index.html)\n* [Develop on Databricks](..\/..\/languages\/index.html)\n* [SQL language reference](index.html)\n* Principal\n* \n\n# Principal\n\n**Applies to:** ![check marked yes](..\/..\/_images\/check.png) Databricks SQL ![check marked yes](..\/..\/_images\/check.png) Databricks Runtime\n\nA principal is a user, service principal, or group known to the metastore.\nPrincipals can be granted [privileges](sql-ref-privileges.html#privilege-types) and may own [securable objects](sql-ref-privileges.html#securable-objects).\n\n## Syntax\n\n```\n{ `<user>@<domain-name>` |\n  `<sp-application-id>` |\n  group_name |\n  users |\n  `account users` }\n\n```\n\n## Parameters\n\n* **`<user>@<domain-name>`**\n\nAn individual user. You must quote the identifier with back-ticks (`) due to the @ character.\n* **`<sp-application-id>`**\n\nA service principal, specified by its `applicationId` value. You must quote the identifier with back-ticks (`) because of the dash characters in the ID.\n* **group\\_name**\n\nAn [identifier](sql-ref-identifiers.html) specifying a group of users or groups.\n* **`users`**\n\nThe root group to which all users in the workspace belong. You cannot grant `users` privileges to securable objects in the Unity Catalog because it is a [workspace-local group](#workspace-level-principal).\n* **`account users`**\n\nThe root group to which all users in the account belong. You must quote the identifier with back-ticks (`) because of the blank character.\n\n## Workspace-local and account groups\n\nDatabricks has the concept of *account groups* and *workspace-local groups*, with special behaviors:\n\n* **Account groups** Account groups can be created by account admins and workspace admins of identity-federated workspaces. They can be granted access to identity-federated workspaces and privileges to securable objects in the Unity Catalog.\n* **Workspace-local groups** can be created only by workspace admins. These groups are identified as *workspace-local* in the workspace admin settings page and on the workspace Permissions tab in the account console. Workspace-local groups cannot be assigned to additional workspaces or granted privileges to securable objects in the Unity Catalog. The system groups `users` and `admins` are a workspace-local groups.\n\n## Examples\n\n```\n-- Granting a privilege to the user alf@melmak.et\n> GRANT SELECT ON TABLE t TO `alf@melmak.et`;\n\n-- Granting a privilege to the service principal fab9e00e-ca35-11ec-9d64-0242ac120002\n> GRANT SELECT ON TABLE t TO `fab9e00e-ca35-11ec-9d64-0242ac120002`;\n\n-- Revoking a privilege from the general public group.\n> REVOKE SELECT ON TABLE t FROM `account users`;\n\n-- Transferring ownership of an object to `some_group`\n> ALTER SCHEMA some_schema OWNER TO some_group;\n\n```\n\n## Related\n\n* [ALTER GROUP](security-alter-group.html)\n* [CREATE GROUP](security-create-group.html)\n* [GRANT](security-grant.html)\n* [REVOKE](security-revoke.html)\n\n---\n\n \u00a9 Databricks 2024. All rights reserved. Apache, Apache Spark, Spark, and the Spark logo are trademarks of the [Apache Software Foundation](http:\/\/www.apache.org\/).\n \n\n[Send us feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n \n | [Privacy Policy](https:\/\/databricks.com\/privacy-policy) | [Terms of Use](https:\/\/databricks.com\/terms-of-use)\n\n { # 's' variable stands for the search page here. TODO: Fix it. #}\n \n [![DATA+AU Summit 2024](..\/..\/_static\/images\/dais\/docs-dais-promo-image.png)](https:\/\/www.databricks.com\/dataaisummit?utm_source=databricks&utm_medium=web&utm_campaign=7018y000001f7cvqai&utm_offer=docs \"DATA+AU Summit 2024\")"},{"doc_uri":"https:\/\/docs.databricks.com\/admin\/users-groups\/index.html","content":"Manage users, service principals, and groups | Databricks on AWS\n\n* [![Databricks](..\/..\/_static\/small-scale-lockup-full-color-rgb.svg)](https:\/\/www.databricks.com\/)\n\n* [![Databricks](..\/..\/_static\/small-scale-lockup-full-color-rgb.svg)](https:\/\/www.databricks.com\/)\n* [Help Center](https:\/\/help.databricks.com\/s\/)\n* [Documentation](https:\/\/docs.databricks.com\/en\/index.html)\n* [Knowledge Base](https:\/\/kb.databricks.com\/)\n\n* [Community](https:\/\/community.databricks.com)\n* [Support](https:\/\/help.databricks.com)\n* [Feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n* [Try Databricks](https:\/\/databricks.com\/try-databricks)\n\n[![](..\/..\/_static\/icons\/globe.png)English](javascript:void(0))\n* [English](..\/..\/..\/en\/admin\/users-groups\/index.html)\n* [\u65e5\u672c\u8a9e](..\/..\/..\/ja\/admin\/users-groups\/index.html)\n* [Portugu\u00eas](..\/..\/..\/pt\/admin\/users-groups\/index.html)\n\n[![](..\/..\/_static\/icons\/aws.svg)Amazon\n Web Services](javascript:void(0))\n* [![](..\/..\/_static\/icons\/azure.svg)Microsoft Azure](https:\/\/learn.microsoft.com\/azure\/databricks\/admin\/users-groups\/)\n* [![](..\/..\/_static\/icons\/gcp.svg)Google Cloud Platform](https:\/\/docs.gcp.databricks.com\/admin\/users-groups\/index.html)\n\n[Databricks on AWS](..\/..\/index.html)\nGet started\n\n* [Get started](..\/..\/getting-started\/index.html)\n* [What is Databricks?](..\/..\/introduction\/index.html)\n* [DatabricksIQ](..\/..\/databricksiq\/index.html)\n* [Release notes](..\/..\/release-notes\/index.html)\n\nLoad & manage data\n\n* [Connect to data sources](..\/..\/connect\/index.html)\n* [Connect to compute](..\/..\/compute\/index.html)\n* [Discover data](..\/..\/discover\/index.html)\n* [Query data](..\/..\/query\/index.html)\n* [Ingest data](..\/..\/ingestion\/index.html)\n* [Transform data](..\/..\/transform\/index.html)\n* [Monitor data and AI assets](..\/..\/lakehouse-monitoring\/index.html)\n* [Share data (Delta Sharing)](..\/..\/data-sharing\/index.html)\n* [Databricks Marketplace](..\/..\/marketplace\/index.html)\n\nWork with data\n\n* [Data engineering](..\/..\/workspace-index.html)\n* [Generative AI & LLMs](..\/..\/generative-ai\/generative-ai.html)\n* [Machine learning](..\/..\/machine-learning\/index.html)\n* [Model serving](..\/..\/machine-learning\/model-serving\/index.html)\n* [Data warehousing](..\/..\/sql\/index.html)\n* [Delta Lake](..\/..\/delta\/index.html)\n* [Developers](..\/..\/languages\/index.html)\n* [Technology partners](..\/..\/integrations\/index.html)\n\nAdministration\n\n* [Account and workspace administration](..\/index.html)\n\t+ [Get started with Databricks administration](..\/..\/getting-started\/admin-get-started.html)\n\t+ [Account administration](..\/account-settings\/index.html)\n\t+ [Workspace deployment](..\/workspace\/index.html)\n\t+ [Manage a workspace](..\/workspace-settings\/index.html)\n\t+ [Identity management](#)\n\t\t- [Identity best practices](best-practices.html)\n\t\t- [Manage users](users.html)\n\t\t- [Manage service principals](service-principals.html)\n\t\t- [Manage groups](groups.html)\n\t\t- [Sync users and groups from your identity provider](scim\/index.html)\n\t+ [Compute policies](..\/clusters\/policies.html)\n\t+ [Audit logs](..\/account-settings\/audit-logs.html)\n\t+ [System tables](..\/system-tables\/index.html)\n* [Security and compliance](..\/..\/security\/index.html)\n* [Data governance (Unity Catalog)](..\/..\/data-governance\/index.html)\n* [Lakehouse architecture](..\/..\/lakehouse-architecture\/index.html)\n\nReference & resources\n\n* [Reference](..\/..\/reference\/api.html)\n* [Resources](..\/..\/resources\/index.html)\n* [What\u2019s coming?](..\/..\/whats-coming.html)\n* [Documentation archive](..\/..\/archive\/index.html)\n\n Updated May 28, 2024\n \n\n[Send us feedback](mailto:doc-feedback@databricks.com?subject=Documentation Feedback)\n\n* [Documentation](..\/..\/index.html)\n* [Databricks administration introduction](..\/index.html)\n* Manage users, service principals, and groups\n* \n\n# Manage users, service principals, and groups\n\nThis article introduces the Databricks identity management model and provides an overview of how to manage users, groups, and service principals in Databricks.\n\nFor an opinionated perspective on how to best configure identity in Databricks, see [Identity best practices](best-practices.html).\n\nTo manage access for users, service principals, and groups, see [Authentication and access control](..\/..\/security\/auth-authz\/index.html).\n\n## Databricks identities\n\nThere are three types of Databricks identity:\n\n* **Users**: User identities recognized by Databricks and represented by email addresses.\n* **Service principals**: Identities for use with jobs, automated tools, and systems such as scripts, apps, and CI\/CD platforms.\n* **Groups**: A collection of identities used by admins to manage group access to workspaces, data, and other securable objects. All Databricks identities can be assigned as members of groups.\nThere are two types of groups in Databricks: account groups and workspace-local groups. For more information, see [Difference between account groups and workspace-local groups](groups.html#account-vs-workspace-group).\n\nYou can have a maximum of 10,000 combined users and service principals and 5,000 groups in an account. Each workspace can have a maximum of 10,000 combined users and service principals and 5,000 groups.\n\nFor detailed instructions, see:\n\n* [Manage users](users.html)\n* [Manage service principals](service-principals.html)\n* [Manage groups](groups.html)\n* [Sync users and groups from your identity provider](scim\/index.html)\n\n## Who can manage identities in Databricks?\n\nTo manage identities in Databricks, you must have one of the following: the account admin role, the workspace admin role, or the manager role on a service principal or group.\n\n* **Account admins** can add users, service principals, and groups to the account and assign them admin roles. Account admins can update and delete users, service principals, and groups in the account. They can give users access to workspaces, as long as those workspaces use [identity federation](#assign-users-to-workspaces).\n* **Workspace admins** can add users and service principals to the Databricks account. They can also add groups to the Databricks account if their workspaces are enabled for identity federation. Workspace admins can grant users, service principals, and groups access to their workspaces. They cannot delete users and service principals from the account.\n\nWorkspace admins can also manage workspace-local groups. For more information, see [Manage workspace-local groups (legacy)](workspace-local-groups.html).\n* **Group managers** can manage group membership and delete the group. They can also assign other users the group manager role. Account admins have the group manager role on all groups in the account. Workspace admins have the group manager role on account groups that they create. See [Who can manage account groups?](groups.html#who-can-manage-groups).\n* **Service principal managers** can manage roles on a service principal. Account admins have the service principal manager role on all service principals in the account. Workspace admins have the service principal manager role on service principals that they create. For more information, see [Roles for managing service principals](..\/..\/security\/auth-authz\/access-control\/service-principal-acl.html).\n\n## How do admins assign users to the account?\n\nDatabricks recommends using SCIM provisioning to sync all users and groups automatically from your identity provider to your Databricks account. Users in a Databricks account do not have any default access to a workspace, data, or compute resources. Account admins and workspace admins can assign account users to workspaces. Workspace admins can also add a new user directly to a workspace, which both automatically adds the user to the account and assigns them to that workspace.\n\nUsing dashboard share to account, users can share published dashboards with other users in the Databricks account, even if those users are not members of their workspace. For more information, see [What is share to account?](..\/..\/dashboards\/index.html#share-to-account).\n\nFor detailed instructions on adding users to the account, see:\n\n* [Sync users and groups from your identity provider](scim\/index.html)\n* [Add users to your account](users.html#add-user-account)\n* [Add service principals to your account](service-principals.html#add-sp-account)\n* [Add groups to your account](groups.html#add-group-account-console)\n\n## How do admins assign users to workspaces?\n\nTo enable a user, service principal, or group to work in a Databricks workspace, an account admin or workspace admin needs to assign them to a workspace. You can assign workspace access to users, service principals, and groups that exist in the account as long as the workspace is enabled for [identity federation](#enable-identity-federation).\n\nWorkspace admins can also add a new user, service principal, or account group directly to a workspace. This action automatically adds the chosen user, service principal, or account group to the account and assigns them to that particular workspace.\n\n![Account-level identity diagram](..\/..\/_images\/account-level-identity-diagram.png)\n\nNote\n\nWorkspace admins can also create legacy workspace-local groups in workspaces using the Workspace Groups API. Workspace-local groups are not automatically added to the account. Workspace-local groups cannot be assigned to additional workspaces, or granted access to data in a [Unity Catalog](..\/..\/data-governance\/unity-catalog\/index.html) metastore.\n\nFor those workspaces that aren\u2019t enabled for identity federation, workspace admins manage their workspace users, service principals, and groups entirely within the scope of the workspace. Users and service principals added to non-identity federated workspaces are automatically added to the account. Groups added to non-identity federated workspaces are legacy workspace-local groups that are not added to the account.\n\nFor detailed instructions, see:\n\n* [Add users to a workspace](users.html#add-user-workspace)\n* [Add service principals to a workspace](service-principals.html#add-sp-workspace)\n* [Add groups to a workspace](groups.html#add-groups-workspace)\n\n## How do admins enable identity federation on a workspace?\n\nIf your account was created after November 8, 2023, identity federation is enabled on all new workspaces by default, and it cannot be disabled.\n\nTo enable identity federation in a workspace, an account admin needs to enable the workspace for Unity Catalog by assigning a Unity Catalog metastore. See [Enable a workspace for Unity Catalog](..\/..\/data-governance\/unity-catalog\/enable-workspaces.html).\n\nWhen the assignment is complete, identity federation is marked as **Enabled** on the workspace\u2019s Configuration tab in the account console.\n\nWorkspace admins can tell if a workspace has identity federation enabled from the workspace admin settings page. In an identity federated workspace, when you choose to add a user, service principal, or group in workspace admin settings, you have the option to select a user, service principal, or group from your account to add to the workspace.\n\n ![Add user identity federation](..\/..\/_images\/identity-federation-add-user-from-workspace.png)\n\nIn a non-identity federated workspace, you do not have the option to add users, service principals, or groups from your account.\n\n## Assigning admin roles\n\nAccount admins can assign other users as account admins. They can also become Unity Catalog metastore admins by virtue of creating a metastore, and they can transfer the metastore admin role to another user or group.\n\nBoth account admins and workspace admins can assign other users as workspace admins. The workspace admin role is determined by membership in the workspace *admins* group, which is a default group in Databricks and cannot be deleted.\n\nAccount admins can also assign other users as Marketplace admins.\n\nSee:\n\n* [Assign account admin roles to a user](users.html#account-admin)\n* [Assign the workspace admin role to a user using the workspace admin settings page](users.html#workspace-admin)\n* [Assign a metastore admin](..\/..\/data-governance\/unity-catalog\/manage-privileges\/admin-privileges.html#assign-metastore-admin)\n* [Assign the Marketplace admin role](..\/..\/marketplace\/get-started-provider.html#marketplace-admin)\n\n## Setting up single sign-on (SSO)\n\nSingle sign-on (SSO) enables you to authenticate your users using a third-party identity provider like Okta. If your identity provider supports the SAML 2.0 protocol (or, in the case of account-level SSO, the OIDC protocol), you can use Databricks SSO to integrate with your identity provider.\n\nPreview\n\nUnified login is currently in [Public Preview](..\/..\/release-notes\/release-types.html) for accounts created before June 21, 2023. Unified login is [generally available](..\/..\/release-notes\/release-types.html) for accounts created after June 21, 2023."}]}
