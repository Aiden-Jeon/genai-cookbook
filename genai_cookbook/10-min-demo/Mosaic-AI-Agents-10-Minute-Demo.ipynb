{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b21aec0-bc65-4bbe-934c-9d1913c5f6b7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Run this DBX notebook\n",
    "\n",
    "#### TLDR; this notebook will:\n",
    "1. Deploy a RAG application built with [Agent Framework](https://docs.databricks.com/generative-ai/retrieval-augmented-generation.html) to the [Agent Evaluation](https://docs.databricks.com/generative-ai/agent-evaluation/index.html) review application\n",
    "    - The review application is used by your business stakeholders to provide feedback on your app\n",
    "2. Evaluate the quality of the application with [Agent Evaluation](https://docs.databricks.com/generative-ai/agent-evaluation/index.html) and MLflow\n",
    "    - These AI-assisted evaluations are used by developers to improve the application's quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6832cb07-a593-462e-8865-2656e02e6c89",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U -qqqq databricks-agents mlflow mlflow-skinny databricks-vectorsearch databricks-sdk langchain==0.2.1 langchain_core==0.2.5 langchain_community==0.2.4 \n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "432c7c8e-1ee5-43c2-abf2-be6102f31712",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Setup: Load the necessary data and code from the Databricks Cookbook repo\n",
    "\n",
    "The following cell clones the Generative AI cookbook repo from `https://github.com/databricks/genai-cookbook` into a folder `genai-cookbook` in the same folder as this notebook using a [Git Folder](https://docs.databricks.com/en/repos/index.html).  \n",
    "\n",
    "Alternatively, you can manually clone the Git repo `https://github.com/databricks/genai-cookbook` to a folder `genai-cookbook`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "809cd750-7b54-49e1-a629-f68426f98afd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from databricks.sdk.core import DatabricksError\n",
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "CURRENT_FOLDER = os.getcwd()\n",
    "QUICK_START_REPO_URL = \"https://github.com/epec254/staging-cookbook.git\"\n",
    "QUICK_START_REPO_URL = \"https://github.com/databricks/genai-cookbook.git\"\n",
    "QUICK_START_REPO_SAVE_FOLDER = \"genai-cookbook\"\n",
    "\n",
    "if os.path.isdir(QUICK_START_REPO_SAVE_FOLDER):\n",
    "    raise Exception(\n",
    "        f\"{QUICK_START_REPO_SAVE_FOLDER} folder already exists, please change the variable QUICK_START_REPO_SAVE_FOLDER to be a non-existant path.\"\n",
    "    )\n",
    "\n",
    "# Clone the repo\n",
    "w = WorkspaceClient()\n",
    "try:\n",
    "    w.repos.create(\n",
    "        url=QUICK_START_REPO_URL, provider=\"github\", path=f\"{CURRENT_FOLDER}/{QUICK_START_REPO_SAVE_FOLDER}\"\n",
    "    )\n",
    "    print(f\"Cloned sample code repo to: {QUICK_START_REPO_SAVE_FOLDER}\")\n",
    "except DatabricksError as e:\n",
    "    if e.error_code == \"RESOURCE_ALREADY_EXISTS\":\n",
    "        print(\"Repo already exists. Skipping creation\")\n",
    "    else:\n",
    "        raise Exception(\n",
    "            f\"Failed to clone the quick start code.  You can manually import this by creating a Git folder from the contents of {QUICK_START_REPO_URL} in the {QUICK_START_REPO_SAVE_FOLDER} folder in your workspace and then re-running this Notebook.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9109dc91-6363-4f28-b84a-d7268832f143",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Application configuration\n",
    "\n",
    "We've selected defaults for the following parameters based on your user name, but inspect and change if you prefer to use existing resources.  Any missing resources will be created in the next step.\n",
    "\n",
    "1. `UC_CATALOG` & `UC_SCHEMA`: [Unity Catalog](https://docs.databricks.com/en/data-governance/unity-catalog/create-catalogs.html#create-a-catalog) and a Schema where the output Delta Tables with the parsed/chunked documents and Vector Search indexes are stored\n",
    "2. `UC_MODEL_NAME`: Unity Catalog location to log and store the chain's model\n",
    "3. `VECTOR_SEARCH_ENDPOINT`: [Vector Search Endpoint](https://docs.databricks.com/en/generative-ai/create-query-vector-search.html#create-a-vector-search-endpoint) to host the resulting vector index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "371c9bd8-9355-41be-9c24-13b031673b76",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Use the current user name to create any necesary resources\n",
    "w = WorkspaceClient()\n",
    "user_name = w.current_user.me().user_name.split(\"@\")[0].replace(\".\", \"\")\n",
    "\n",
    "# UC Catalog & Schema where outputs tables/indexs are saved\n",
    "# If this catalog/schema does not exist, you need create catalog/schema permissions.\n",
    "UC_CATALOG = f'{user_name}_catalog'\n",
    "UC_CATALOG = \"rag_m1_release\"\n",
    "UC_SCHEMA = f'rag_{user_name}'\n",
    "UC_SCHEMA = \"10_mins\"\n",
    "\n",
    "# UC Model name where the POC chain is logged\n",
    "UC_MODEL_NAME = f\"{UC_CATALOG}.{UC_SCHEMA}.{user_name}_agent_quick_start\"\n",
    "\n",
    "# Vector Search endpoint where index is loaded\n",
    "# If this does not exist, it will be created\n",
    "VECTOR_SEARCH_ENDPOINT = f'{user_name}_vector_search'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f8b69e0-7fbc-4af6-8b1b-7bb958a9ed97",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Check if the UC Catalog & Vector Search endpoint exist; create otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c132c95-d19d-4694-a465-fe758e62d983",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.vectorsearch import EndpointStatusState, EndpointType\n",
    "from databricks.sdk.service.serving import EndpointCoreConfigInput, EndpointStateReady\n",
    "from databricks.sdk.errors import ResourceDoesNotExist\n",
    "import os\n",
    "\n",
    "w = WorkspaceClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16764462-53a8-49de-a34e-193f65c6b695",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Validate UC Catalog & Schema, create if not exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac03797b-1980-4637-b65a-566d4811f796",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.errors import NotFound, PermissionDenied\n",
    "w = WorkspaceClient()\n",
    "\n",
    "# Create UC Catalog if it does not exist, otherwise, raise an exception\n",
    "try:\n",
    "    _ = w.catalogs.get(UC_CATALOG)\n",
    "    print(f\"PASS: UC catalog `{UC_CATALOG}` exists\")\n",
    "except NotFound as e:\n",
    "    print(f\"`{UC_CATALOG}` does not exist, trying to create...\")\n",
    "    try:\n",
    "        _ = w.catalogs.create(name=UC_CATALOG)\n",
    "    except PermissionDenied as e:\n",
    "        print(f\"FAIL: `{UC_CATALOG}` does not exist, and no permissions to create.  Please provide an existing UC Catalog.\")\n",
    "        raise ValueError(f\"Unity Catalog `{UC_CATALOG}` does not exist.\")\n",
    "        \n",
    "# Create UC Schema if it does not exist, otherwise, raise an exception\n",
    "try:\n",
    "    _ = w.schemas.get(full_name=f\"{UC_CATALOG}.{UC_SCHEMA}\")\n",
    "    print(f\"PASS: UC schema `{UC_CATALOG}.{UC_SCHEMA}` exists\")\n",
    "except NotFound as e:\n",
    "    print(f\"`{UC_CATALOG}.{UC_SCHEMA}` does not exist, trying to create...\")\n",
    "    try:\n",
    "        _ = w.schemas.create(name=UC_SCHEMA, catalog_name=UC_CATALOG)\n",
    "        print(f\"PASS: UC schema `{UC_CATALOG}.{UC_SCHEMA}` created\")\n",
    "    except PermissionDenied as e:\n",
    "        print(f\"FAIL: `{UC_CATALOG}.{UC_SCHEMA}` does not exist, and no permissions to create.  Please provide an existing UC Schema.\")\n",
    "        raise ValueError(\"Unity Catalog Schema `{UC_CATALOG}.{UC_SCHEMA}` does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04acf93c-62b2-492e-b070-700ffb0107d0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Validate Vector Search endpoint, create if not exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e819a8b-1389-4579-8fa8-f3b3c4243e40",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create the Vector Search endpoint if it does not exist\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.vectorsearch import EndpointType\n",
    "w = WorkspaceClient()\n",
    "vector_search_endpoints = w.vector_search_endpoints.list_endpoints()\n",
    "if sum([VECTOR_SEARCH_ENDPOINT == ve.name for ve in vector_search_endpoints]) == 0:\n",
    "    print(f\"Please wait, creating Vector Search endpoint `{VECTOR_SEARCH_ENDPOINT}`.  This can take up to 20 minutes...\")\n",
    "    w.vector_search_endpoints.create_endpoint_and_wait(VECTOR_SEARCH_ENDPOINT, endpoint_type=EndpointType.STANDARD)\n",
    "\n",
    "# Make sure vector search endpoint is online and ready.\n",
    "w.vector_search_endpoints.wait_get_endpoint_vector_search_endpoint_online(VECTOR_SEARCH_ENDPOINT)\n",
    "\n",
    "print(f\"PASS: Vector Search endpoint `{VECTOR_SEARCH_ENDPOINT}` exists\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ccd4ff4-ec49-446f-bbdf-f22f6c9920e8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Build & deploy the application\n",
    "\n",
    "Below is a high-level overview of the architecture we will deploy:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61fce883-f5ae-4e72-8948-217ea0e6cef4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## 1/ Create the Vector Search Index\n",
    "\n",
    "First, we copy the sample data to a Delta Table and sync to a Vector Search index.  Here, we use the [gte-large-en-v1.5](https://huggingface.co/Alibaba-NLP/gte-large-en-v1.5) embedding model hosted on [Databricks Foundational Model APIs](https://docs.databricks.com/en/machine-learning/foundation-models/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc89d554-2428-4887-9991-df21d162cf08",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# UC locations to store the chunked documents & index\n",
    "CHUNKS_DELTA_TABLE = f\"{UC_CATALOG}.{UC_SCHEMA}.databricks_docs_chunked2\"\n",
    "CHUNKS_VECTOR_INDEX = f\"{UC_CATALOG}.{UC_SCHEMA}.databricks_docs_chunked_index2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "158ac4c4-0a44-4814-8dc1-43a78e2f2838",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from databricks.vector_search.client import VectorSearchClient\n",
    "\n",
    "# Workspace URL for printing links to the delta table/vector index\n",
    "workspace_url = SparkSession.getActiveSession().conf.get(\n",
    "    \"spark.databricks.workspaceUrl\", None\n",
    ")\n",
    "\n",
    "# Vector Search client\n",
    "vsc = VectorSearchClient(disable_notice=True)\n",
    "\n",
    "# Load the chunked data to Delta Table & enable change-data capture to allow the table to sync to Vector Search\n",
    "chunked_docs_df = spark.read.parquet(\n",
    "    f\"file:{QUICK_START_REPO_SAVE_FOLDER}/quick_start_demo/chunked_databricks_docs.snappy.parquet\"\n",
    ")\n",
    "chunked_docs_df.write.format(\"delta\").saveAsTable(CHUNKS_DELTA_TABLE)\n",
    "spark.sql(\n",
    "    f\"ALTER TABLE {CHUNKS_DELTA_TABLE} SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"View Delta Table at: https://{workspace_url}/explore/data/{UC_CATALOG}/{UC_SCHEMA}/{CHUNKS_DELTA_TABLE.split('.')[-1]}\"\n",
    ")\n",
    "\n",
    "# Embed and sync chunks to a vector index\n",
    "print(\n",
    "    f\"Embedding docs & creating Vector Search Index, this will take ~5 - 10 minutes.\\nView Index Status at: https://{workspace_url}/explore/data/{UC_CATALOG}/{UC_SCHEMA}/{CHUNKS_VECTOR_INDEX.split('.')[-1]}\"\n",
    ")\n",
    "\n",
    "index = vsc.create_delta_sync_index_and_wait(\n",
    "    endpoint_name=VECTOR_SEARCH_ENDPOINT,\n",
    "    index_name=CHUNKS_VECTOR_INDEX,\n",
    "    primary_key=\"chunk_id\",\n",
    "    source_table_name=CHUNKS_DELTA_TABLE,\n",
    "    pipeline_type=\"TRIGGERED\",\n",
    "    embedding_source_column=\"chunked_text\",\n",
    "    embedding_model_endpoint_name=\"databricks-gte-large-en\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8f0f1c5-ed6e-429f-9696-feed74b5fbcc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 2/ Deploy to the review application\n",
    "\n",
    "Now that our Vector Search index is ready, let's prepare the RAG chain and deploy it to the review application backed by a scalable-production ready REST API on Model serving.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33547725-88aa-4c80-bad2-63813f6d0683",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 2.1/ Configuring our Chain parameters\n",
    "\n",
    "Databricks makes it easy to parameterize your chain with MLflow Model Configurations. Later, you can tune application quality by adjusting these parameters, such as the system prompt or retrieval settings.  Most applications will include many more parameters, but for this demo, we'll keep the configuration to a minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6397831d-3398-432f-9c0d-2f64ea9c20d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "chain_config = {\n",
    "    \"llm_model_serving_endpoint_name\": \"databricks-dbrx-instruct\",  # the foundation model we want to use\n",
    "    \"vector_search_endpoint_name\": VECTOR_SEARCH_ENDPOINT,  # Endoint for vector search\n",
    "    \"vector_search_index\": f\"{UC_CATALOG}.{UC_SCHEMA}.{CHUNKS_VECTOR_INDEX}\",\n",
    "    \"llm_prompt_template\": \"\"\"You are an assistant that answers questions. Use the following pieces of retrieved context to answer the question. Some pieces of context may be irrelevant, in which case you should not use them to form the answer.\\n\\nContext: {context}\"\"\", # LLM Prompt template\n",
    "}\n",
    "\n",
    "# Here, we define an input example in the schema required by Agent Framework\n",
    "input_example = {\"messages\": [ {\"role\": \"user\", \"content\": \"What is Retrieval-augmented Generation?\"}]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6fdb358-fe9a-4126-b511-59024117ccbd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 2.1/ Log the application & view trace\n",
    "\n",
    "We first register the chain as an MLflow model and inspect the MLflow Trace to understand what is happening inside the chain.\n",
    "\n",
    "#### MLflow trace\n",
    "<br/>\n",
    "<img src=\"https://ai-cookbook.io/_images/mlflow_trace2.gif\" width=\"80%\" style=\"margin-left: 10px\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7a43ca1-4807-4e03-8752-8b36a481b515",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# Log the model to MLflow\n",
    "with mlflow.start_run(run_name=\"databricks-docs-bot\"):\n",
    "    logged_chain_info = mlflow.langchain.log_model(\n",
    "        lc_model=os.path.join(\n",
    "            os.getcwd(),\n",
    "            f\"{QUICK_START_REPO_SAVE_FOLDER}/quick_start_demo/sample_rag_chain\",\n",
    "        ),  # Chain code file from the quick start repo\n",
    "        model_config=chain_config,  # Chain configuration set above\n",
    "        artifact_path=\"chain\",  # Required by MLflow\n",
    "        input_example=input_example,  # Save the chain's input schema.  MLflow will execute the chain before logging & capture it's output schema.\n",
    "    )\n",
    "\n",
    "# Test the chain locally to see the MLflow Trace\n",
    "chain = mlflow.langchain.load_model(logged_chain_info.model_uri)\n",
    "chain.invoke(input_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef6bc543-5eee-4bf4-87c7-dbfa82506f23",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 2.1/ Deploy the application\n",
    "\n",
    "Now, we:\n",
    "1. Register the application in Unity Catalog\n",
    "2. Use Agent Framework to deploy to the Quality Lab review application\n",
    "\n",
    "Along side the review ap, a scalable, production-ready Model Serving endpoint is also deployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7510513-9c5c-465e-85c3-6afe5cca41a3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks import agents\n",
    "import time\n",
    "from databricks.sdk.service.serving import EndpointStateReady, EndpointStateConfigUpdate\n",
    "\n",
    "# Use Unity Catalog to log the chain\n",
    "mlflow.set_registry_uri('databricks-uc')\n",
    "\n",
    "# Register the chain to UC\n",
    "uc_registered_model_info = mlflow.register_model(model_uri=logged_chain_info.model_uri, name=UC_MODEL_NAME)\n",
    "\n",
    "# Deploy to enable the Review APP and create an API endpoint\n",
    "deployment_info = agents.deploy(model_name=UC_MODEL_NAME, model_version=uc_registered_model_info.version)\n",
    "\n",
    "# Wait for the Review App to be ready\n",
    "print(\"\\nWaiting for endpoint to deploy.  This can take 15 - 20 minutes.\", end=\"\")\n",
    "while w.serving_endpoints.get(deployment_info.endpoint_name).state.ready == EndpointStateReady.NOT_READY or w.serving_endpoints.get(deployment_info.endpoint_name).state.config_update == EndpointStateConfigUpdate.IN_PROGRESS:\n",
    "    print(\".\", end=\"\")\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e25e8e90-1571-417c-8fd2-b3cb61e11542",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 3/ Use Agent Evaluation to evaluate your application\n",
    "\n",
    "## 3.1/ Have stakeholders chat your bot to build your evaluation dataset\n",
    "\n",
    "Normally, you would now give access to internal domain experts and have them test and review the bot.  **Your domain experts do NOT need to have Databricks Workspace access** - you can assign permissions to any user in your SSO if you have enabled [SCIM](https://docs.databricks.com/en/admin/users-groups/scim/index.html)\n",
    "\n",
    "This is a critical step to build or improve your evaluation dataset: have users ask questions to your bot, and provide the bot with output answer when they don't answer properly.\n",
    "\n",
    "Your applicaation is automatically capturing all stakeholder questions and bot responses, including the MLflow Trace for each, into Delta Tables in your Lakehouse. On top of that, Databricks makes it easy to track feedback from your end user: if the chatbot doesn't give a good answer and the user gives a thumbdown, their feedback is included in the Delta Tables.\n",
    "\n",
    "Your evaluation dataset forms the basis of your development workflow to improve quality: identifying the root causes of quality issues and then objectively measuring the impact of your fixes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fabd72c8-9836-4d80-9c73-9b5a539005f1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 3.2/ Run Evaluation of your Chain\n",
    "\n",
    "Now, let's use everage Agent Evaluation's specialized AI evaluators to evaluate our model performance.  Agent Evaluation is integrated into `mlflow.evaluate(...)`, all you need to do is pass `model_type=\"databricks-agent\"`.\n",
    "\n",
    "For this demo, we use a toy 10 question evaluation dataset.  Read more about our [best practices](https://ai-cookbook.io/nbs/4-evaluation-eval-sets.html) on the size of your evaluation dataset.\n",
    "\n",
    "<img src=\"https://ai-cookbook.io/_images/mlflow-eval-agent.gif\" width=\"80%\">\n",
    "<br/><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f5aac41-47b4-47ae-a672-d3c0d66c922c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sample_eval_set = [\n",
    "    {\n",
    "        \"request_id\": \"5482\",\n",
    "        \"request\": \"What happens if I try to access an index that is out of bounds in an array using the [ ] operator in Databricks SQL when spark.sql.ansi.enabled is set to false?\",\n",
    "        \"response\": \"If you try to access an index that is out of bounds in an array using the [ ] operator in Databricks SQL when `spark.sql.ansi.enabled` is set to false, Databricks will return `NULL` instead of raising an `INVALID_ARRAY_INDEX` error. However, if `spark.sql.ansi.enabled` is set to true, an `INVALID_ARRAY_INDEX` error will be raised.\",\n",
    "        \"expected_retrieved_context\": [\n",
    "            {\n",
    "                \"content\": \"[ ] (bracket sign) operator  \\nApplies to: Databricks SQL Databricks Runtime  \\nReturns an array element or map value given an index or key.  \\nSyntax\\nSyntax\\nexpr [ keyExpr ]  \\nNote that the use of brackets here is as literal text and not indicating optional syntax.\\n\\nArguments\\nArguments\\nexpr: An ARRAY or MAP expression.  \\nkeyExpr: If expr is an ARRAY an integral numeric. Otherwise an expression matching the type of the key of the MAP.\\n\\nReturns\\nReturns\\nThe result type is the element type of the ARRAY or the value type of the MAP.  \\nThe first element of an ARRAY is at index 0.  \\nIf the keyExpr is not a valid key for the MAP expr Databricks returns null.  \\nIf the keyExpr is out of bound for the ARRAY expr Databricks raises a INVALID_ARRAY_INDEXerror.  \\nNote  \\nIn Databricks Runtime, if spark.sql.ansi.enabled is false, the operator returns NULL instead of an out of bounds error.\\n\\nExamples\\nExamples\\n> SELECT a[2] FROM VALUES(array(10, 20, 30)) AS T(a); 30 > SELECT m[1] FROM VALUES(map(1, 'Hello', 2, 'World')) AS T(m); Hello\\n\\nRelated functions\\nRelated functions\\narray function  \\narray_contains function  \\narray_position function  \\nmap function  \\nmap_keys function  \\nmap_values function\",\n",
    "                \"doc_uri\": \"https://docs.databricks.com/sql/language-manual/functions/bracketsign.html\",\n",
    "            }\n",
    "        ],\n",
    "        \"expected_response\": \"If you try to access an index that is out of bounds in an array using the [ ] operator in Databricks SQL when spark.sql.ansi.enabled is set to false, Databricks will return NULL instead of raising an error.\",\n",
    "    },\n",
    "    {\n",
    "        \"request_id\": \"2112\",\n",
    "        \"request\": \"Why is a long-running stage in my Spark job only showing one task, and how can I resolve this issue?\",\n",
    "        \"response\": \"A long-running stage in a Spark job with only one task could be caused by expensive UDFs on small data, window functions without a `PARTITION BY` statement, reading from an unsplittable file type, setting the `multiLine` option when reading a JSON or CSV file, schema inference of a large file, or using `repartition(1)` or `coalesce(1)`. To diagnose this issue, you can check the Spark UI to identify the longest stage of the job and look at the Input, Output, Shuffle Read, and Shuffle Write columns. If there is only one task, it could be a sign of a problem. To resolve this issue, you can try optimizing your Spark job by avoiding the aforementioned causes. For example, you can avoid using `repartition(1)` or `coalesce(1)` unless necessary, use a splittable file type, and optimize your UDFs and window functions.\",\n",
    "        \"expected_retrieved_context\": [\n",
    "            {\n",
    "                \"content\": \"One Spark task  \\nIf you see a long-running stage with just one task, that’s likely a sign of a problem. While this one task is running only one CPU is utilized and the rest of the cluster may be idle. This happens most frequently in the following situations:  \\nExpensive UDF on small data  \\nWindow function without PARTITION BY statement  \\nReading from an unsplittable file type. This means the file cannot be read in multiple parts, so you end up with one big task. Gzip is an example of an unsplittable file type.  \\nSetting the multiLine option when reading a JSON or CSV file  \\nSchema inference of a large file  \\nUse of repartition(1) or coalesce(1)\",\n",
    "                \"doc_uri\": \"https://docs.databricks.com/optimizations/spark-ui-guide/one-spark-task.html\",\n",
    "            }\n",
    "        ],\n",
    "        \"expected_response\": \"A long-running stage with one task in a Spark job could be due to several reasons such as:\\n\\n1. Expensive User Defined Functions (UDFs) on small data\\n2. Window function without a PARTITION BY statement\\n3. Reading from an unsplittable file type like gzip\\n4. Setting the multiLine option when reading a JSON or CSV file\\n5. Schema inference of a large file\\n6. Use of repartition(1) or coalesce(1)\\n\\nTo resolve this issue, you can:\\n\\n1. Optimize your UDFs or replace them with built-in functions if possible.\\n2. Ensure that you have a proper PARTITION BY statement in your window functions.\\n3. Avoid using unsplittable file types like gzip. Instead, use splittable file types like snappy or lz4.\\n4. Avoid setting the multiLine option when reading JSON or CSV files.\\n5. Perform schema inference on a small sample of your data and then apply it to the entire dataset.\\n6. Avoid using repartition(1) or coalesce(1) unless necessary.\\n\\nBy implementing these changes, you should be able to resolve the issue of a long-running stage with only one task in your Spark job.\",\n",
    "    },\n",
    "    {\n",
    "        \"request_id\": \"5054\",\n",
    "        \"request\": \"How can I represent 4-byte single-precision floating point numbers in Databricks SQL and what are their limits?\",\n",
    "        \"response\": \"4-byte single-precision floating point numbers in Databricks SQL are represented using the `FLOAT` data type. The range of numbers that can be represented is from -3.402E+38 to +3.402E+38, including negative infinity, positive infinity, and NaN (not a number). The literals for `FLOAT` type can be expressed in base-10 or base-2 format, with the `F` postfix and `E` exponent being case insensitive. However, due to the base-2 representation, the literal may not be exact. For accurate representation of fractional or large base-10 numbers, it is recommended to use the `DECIMAL` data type.\",\n",
    "        \"expected_retrieved_context\": [\n",
    "            {\n",
    "                \"content\": \"FLOAT type  \\nApplies to: Databricks SQL Databricks Runtime  \\nRepresents 4-byte single-precision floating point numbers.  \\nSyntax\\nSyntax\\n{ FLOAT | REAL }\\n\\nLimits\\nLimits\\nThe range of numbers is:  \\n-∞ (negative infinity)  \\n-3.402E+38 to -1.175E-37  \\n0  \\n+1.175E-37 to +3.402E+38  \\n+∞ (positive infinity)  \\nNaN (not a number)\\n\\nLiterals\\nLiterals\\ndecimal_digits [ exponent ] F | [ + | - ] digit [ ... ] [ exponent ] F decimal_digits: [ + | - ] { digit [ ... ] . [ digit [ ... ] ] | . digit [ ... ] } exponent: E [ + | - ] digit [ ... ]  \\ndigit: Any numeral from 0 to 9.  \\nThe F postfix and E exponent are case insensitive.\\n\\nNotes\\nNotes\\nFLOAT is a base-2 numeric type. When given a literal which is base-10 the representation may not be exact. Use DECIMAL type to accurately represent fractional or large base-10 numbers.\\n\\nExamples\\nExamples\\n> SELECT +1F; 1.0 > SELECT 5E10F; 5E10 > SELECT 5.3E10F; 5.3E10 > SELECT -.1F; -0.1 > SELECT 2.F; 2.0 > SELECT -5555555555555555.1F -5.5555558E15 > SELECT CAST(6.1 AS FLOAT) 6.1\\n\\nRelated\\nRelated\\nTINYINT type  \\nSMALLINT type  \\nINT type  \\nBIGINT type  \\nDECIMAL type  \\nDOUBLE type  \\ncast function  \\nSpecial floating point values\",\n",
    "                \"doc_uri\": \"https://docs.databricks.com/sql/language-manual/data-types/float-type.html\",\n",
    "            }\n",
    "        ],\n",
    "        \"expected_response\": \"4-byte single-precision floating point numbers can be represented in Databricks SQL using the `FLOAT` or `REAL` syntax. The range of numbers that can be represented is from -3.402E+38 to +3.402E+38, including negative infinity, positive infinity, and NaN (not a number). Here are some examples of how to represent these numbers:\\n\\n* `+1F` represents 1.0\\n* `5E10F` represents 5E10\\n* `5.3E10F` represents 5.3E10\\n* `-.1F` represents -0.1\\n* `2.F` represents 2.0\\n* `-5555555555555555.1F` represents -5.5555558E15\\n* `CAST(6.1 AS FLOAT)` represents 6.1\\n\\nNote that `FLOAT` is a base-2 numeric type, so the representation of base-10 literals may not be exact. If you need to accurately represent fractional or large base-10 numbers, consider using the `DECIMAL` type instead.\",\n",
    "    },\n",
    "    {\n",
    "        \"request_id\": \"2003\",\n",
    "        \"request\": \"How can I identify the reason for failing executors in my Databricks workspace, and what steps can I take to resolve memory issues?\",\n",
    "        \"response\": \"To identify the reason for failing executors in your Databricks workspace, you should first check the compute's Event log in the Spark UI to see if there's any explanation for why the executors failed. If you don't find any information in the event log, navigate to the Executors tab in the Spark UI to get the logs from the failed executors.\\n\\nThe most common reasons for executors being removed are autoscaling, spot instance losses, and executors running out of memory. If you see any failing jobs, click on them to get to their pages, scroll down to see the failed stage and a failure reason, and check the failed tasks to identify the issue.\\n\\nIf you suspect a memory issue, you can verify it by doubling the memory per core to see if it impacts your problem. If it takes longer to fail with the extra memory or doesn't fail at all, that's a good sign that you're on the right track. If you can fix your issue by increasing the memory, great! If it doesn't fix the issue, or you can't bear the extra cost, you should dig deeper into memory issues.\",\n",
    "        \"expected_retrieved_context\": [\n",
    "            {\n",
    "                \"content\": \"Failing jobs or executors removed  \\nSo you’re seeing failed jobs or removed executors:  \\nThe most common reasons for executors being removed are:  \\nAutoscaling: In this case it’s expected and not an error. See Enable autoscaling.  \\nSpot instance losses: The cloud provider is reclaiming your VMs. You can learn more about Spot instances here.  \\nExecutors running out of memory  \\nFailing jobs\\nFailing jobs\\nIf you see any failing jobs click on them to get to their pages. Then scroll down to see the failed stage and a failure reason:  \\nYou may get a generic error. Click on the link in the description to see if you can get more info:  \\nIf you scroll down in this page, you will be able to see why each task failed. In this case it’s becoming clear there’s a memory issue:\\n\\nFailing executors\\nFailing executors\\nTo find out why your executors are failing, you’ll first want to check the compute’s Event log to see if there’s any explanation for why the executors failed. For example, it’s possible you’re using spot instances and the cloud provider is taking them back.  \\nSee if there are any events explaining the loss of executors. For example you may see messages indicating that the cluster is resizing or spot instances are being lost.  \\nIf you are using spot instances, see Losing spot instances.  \\nIf your compute was resized with autoscaling, it’s expected and not an error. See Learn more about cluster resizing.  \\nIf you don’t see any information in the event log, navigate back to the Spark UI then click the Executors tab:  \\nHere you can get the logs from the failed executors:\\n\\nNext step\\nNext step\\nIf you’ve gotten this far, the likeliest explanation is a memory issue. The next step is to dig into memory issues. See Spark memory issues.\",\n",
    "                \"doc_uri\": \"https://docs.databricks.com/optimizations/spark-ui-guide/failing-spark-jobs.html\",\n",
    "            }\n",
    "        ],\n",
    "        \"expected_response\": \"1. Identify failing executors: In your Databricks workspace, navigate to the compute's Event log to check for any explanations regarding executor failures. Look for messages indicating spot instance losses or cluster resizing due to autoscaling. If using spot instances, refer to 'Losing spot instances' documentation. For autoscaling, refer to 'Learn more about cluster resizing' documentation.\\n\\n2. Check executor logs: If no information is found in the event log, go to the Spark UI and click the Executors tab. Here, you can access logs from failed executors to investigate further.\\n\\n3. Identify memory issues: If the above steps do not provide a clear reason for failing executors, it is likely a memory issue. To dig into memory issues, refer to the 'Spark memory issues' documentation.\\n\\n4. Resolve memory issues: To resolve memory issues, consider the following steps:\\n\\n   a. Increase executor memory: Allocate more memory to executors by adjusting the 'spark.executor.memory' property in your Spark configuration.\\n\\n   b. Increase driver memory: Allocate more memory to the driver by adjusting the 'spark.driver.memory' property in your Spark configuration.\\n\\n   c. Use off-heap memory: Enable off-heap memory by setting the 'spark.memory.offHeap.enabled' property to 'true' and allocating off-heap memory using the 'spark.memory.offHeap.size' property.\\n\\n   d. Optimize data processing: Review your data processing workflows and optimize them for memory efficiency. This may include reducing data shuffling, using broadcast variables, or caching data strategically.\\n\\n   e. Monitor memory usage: Monitor memory usage in your Databricks workspace to identify potential memory leaks or inefficient memory utilization. Use tools like the Spark UI, Ganglia, or Grafana to monitor memory usage.\",\n",
    "    },\n",
    "]\n",
    "\n",
    "eval_df = pd.DataFrame(sample_eval_set)\n",
    "display(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "727c86fb-cc4d-4ffc-9b60-a09d2ade9b41",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_id=logged_chain_info.run_id):\n",
    "    # Evaluate\n",
    "    eval_results = mlflow.evaluate(\n",
    "        data=eval_df, # Your evaluation set\n",
    "        model=logged_chain_info.model_uri, # previously logged model\n",
    "        model_type=\"databricks-agent\", # activate Mosaic AI Agent Evaluation\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e00d9d5-68ed-4ec1-9df7-522562ea2087",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# What's next?\n",
    "\n",
    "\n",
    "## Code-based quick starts\n",
    "\n",
    "| Time required | Outcome | Link |\n",
    "|------ | ---- | ---- |\n",
    "| 🕧 <br/> 10 minutes | Sample RAG app deployed to web-based chat app that collects feedback | ✅ |\n",
    "| 🕧🕧🕧 <br/>60 minutes | POC RAG app with *your data* deployed to a chat UI that can collect feedback from your business stakeholders | [Deploy POC w/ your data](https://ai-cookbook.io/nbs/5-hands-on-build-poc.html)|\n",
    "| 🕧🕧 <br/>30 minutes | Comprehensive quality/cost/latency evaluation of your POC app | - [Evaluate your POC](https://ai-cookbook.io/nbs/5-hands-on-evaluate-poc.html) <br/> - [Identify the root causes of quality issues](https://ai-cookbook.io/nbs/5-hands-on-improve-quality-step-1.html) |"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Mosaic-AI-Agents-10-Minute-Demo",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
